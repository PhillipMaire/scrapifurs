{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cae7f1fe",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbdc7a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\npip install scipy\\npip install h5py\\npip install matplotlib\\npip install opencv-python\\npip install seaborn\\npip install matplotlib\\npip install ipywidgets\\npip install matplotlib install ipywidgets\\npip install opencv-pythonjupyter nbextension enable --py widgetsnbextension\\npip install ipython\\npip install WordCloud\\npip install geopy\\npip install scipy\\npip install plotly\\npip install nbformat --upgrade\\npip install folium\\npip install geopandas\\npip install scikit-learn   \\npip install cairosvg\\npip install lxm\\n\\n\\n\\npip install selenium\\npip install pandas\\npip install python-dotenv\\npip install openai\\npip install beautifulsoup4\\npip install numpy\\npip install tqdm\\npip install seaborn\\npip install matplotlib\\npip install scipy\\npip install geopy\\npip install plotly\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "pip install scipy\n",
    "pip install h5py\n",
    "pip install matplotlib\n",
    "pip install opencv-python\n",
    "pip install seaborn\n",
    "pip install matplotlib\n",
    "pip install ipywidgets\n",
    "pip install matplotlib install ipywidgets\n",
    "pip install opencv-pythonjupyter nbextension enable --py widgetsnbextension\n",
    "pip install ipython\n",
    "pip install WordCloud\n",
    "pip install geopy\n",
    "pip install scipy\n",
    "pip install plotly\n",
    "pip install nbformat --upgrade\n",
    "pip install folium\n",
    "pip install geopandas\n",
    "pip install scikit-learn   \n",
    "pip install cairosvg\n",
    "pip install lxm\n",
    "\n",
    "\n",
    "\n",
    "pip install selenium\n",
    "pip install pandas\n",
    "pip install python-dotenv\n",
    "pip install openai\n",
    "pip install beautifulsoup4\n",
    "pip install numpy\n",
    "pip install tqdm\n",
    "pip install seaborn\n",
    "pip install matplotlib\n",
    "pip install scipy\n",
    "pip install geopy\n",
    "pip install plotly\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "863e81a0-751d-4372-9e32-ab78defb0722",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_instructions = \"\"\"\n",
    "for the below text using using your LLM ability (not python code) extract data from the text. get the following and save all as strings, if they don't exist for a specific job entry, use an empty string (i.e., \"\"). \n",
    "there will be exactly 20 of each. \n",
    "\n",
    "job_title \n",
    "company_name \n",
    "location\n",
    "\n",
    "format the output as a python single dictionary for all the jobs. for example all the job titles will use key \"job_title\" and be a list of the jobs\n",
    "\n",
    "I need to have have a standard output and use it directly for my analysis. process every single job \n",
    "\n",
    "Construct the dictionary: Organize all the extracted information into a dictionary format where the key represents the type of information and the value is a list of all the extracted information of that type in the order of appearance.\n",
    "\n",
    "Formatting the instructions: Always make sure to start the code block with a comment '##BEGIN##', fill in the data accordingly, and end it with '##END##'. All columns of data should align with each other.\n",
    "\n",
    "    For example:\n",
    "\n",
    "\n",
    "    ##BEGIN##\n",
    "job_data = {\n",
    "    \"job_title\": [...],\n",
    "    \"company_name\": [...],\n",
    "    \"location\": [...]\n",
    "}\n",
    "    ##END##\n",
    "\n",
    "    Please make sure to follow these instructions to achieve consistent results.\n",
    "\n",
    "    here is the text:\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a851fda6-9129-492d-9e6f-42280b87413c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56d6382-b492-4b2e-80ca-365cec50c23e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebb4a66-b13d-4547-89a4-4f3253ca7c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a11ab93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import openai\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "from scrapifurs import utils\n",
    "from scrapifurs.GPTinstructions import GPTinstructions\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from scipy.cluster.hierarchy import linkage, leaves_list\n",
    "\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8c0bb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lat_lon(locations):\n",
    "    geolocator = Nominatim(user_agent=\"example.lastname@gmail.com\")\n",
    "    lat_lon_data = {}\n",
    "    for location in tqdm(locations):\n",
    "        location_data = geolocator.geocode(location)\n",
    "        if location_data is not None:\n",
    "            lat_lon_data[location] = (location_data.latitude, location_data.longitude)\n",
    "        else:\n",
    "            lat_lon_data[location] = (None, None)\n",
    "    return lat_lon_data\n",
    "\n",
    "def calculate_distances(lat_lon_data, target_loc_str):\n",
    "    target_location = get_lat_lon([target_loc_str])[target_loc_str]\n",
    "    if target_location[0] is None:\n",
    "        raise ValueError(\"No latitude and longitude found for the provided location.\")\n",
    "    distances = {}\n",
    "    for location, loc_lat_lon in lat_lon_data.items():\n",
    "        if loc_lat_lon[0] is not None:\n",
    "            distance = geodesic(target_location, loc_lat_lon).miles\n",
    "            distances[location] = distance\n",
    "        else:\n",
    "            distances[location] = None\n",
    "    return distances\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def jaccard_similarity(str1, str2):\n",
    "    set1 = set(str1.split())\n",
    "    set2 = set(str2.split())\n",
    "    union_length = len(set1.union(set2))\n",
    "    if union_length == 0:\n",
    "        return np.nan\n",
    "    return len(set1.intersection(set2)) / union_length\n",
    "\n",
    "\n",
    "class GPT_StringDataCleaner:\n",
    "    def __init__(self, original_df, key_name, instructions, overwrite_cleaned_key = True, model=\"gpt-4\"):\n",
    "        self.original_df = original_df\n",
    "        self.key_name = key_name\n",
    "        self.instructions = instructions\n",
    "        self.model = model\n",
    "        self.clean_key = key_name + '_CLEANED_BY_GPT'\n",
    "        self.overwrite_cleaned_key = overwrite_cleaned_key\n",
    "\n",
    "    def send_to_gpt(self):\n",
    "        self.unique_names = sorted(self.original_df[self.key_name].unique())\n",
    "\n",
    "        messages = [{\"role\": \"system\", \"content\": self.instructions}]\n",
    "        messages.append({\"role\": \"user\", \"content\": str(self.unique_names)})\n",
    "        chat = openai.ChatCompletion.create(model=self.model, messages=messages)\n",
    "        self.reply = chat.choices[0].message.content\n",
    "    def test_lengths(self):\n",
    "        print(f\" len input is {len(self.unique_names)}, but len output is {len(cleaned_data)}\")\n",
    "    def process_data(self):\n",
    "        cleaned_data =  eval(self.reply)\n",
    "        \n",
    "        if len(self.unique_names) != len(cleaned_data):\n",
    "            print(f\" len input is {len(self.unique_names)}, but len output is {len(cleaned_data)}\")\n",
    "            raise ValueError(\"Input and output lists must have the same length\")\n",
    "\n",
    "        temp_df = pd.DataFrame({self.key_name: self.unique_names, self.clean_key: cleaned_data})\n",
    "        # delete cleaned key data if we want to overwrite\n",
    "        if self.clean_key in self.original_df.columns and self.overwrite_cleaned_key:\n",
    "            self.original_df = self.original_df.drop(self.clean_key, axis=1)\n",
    "        if self.clean_key not in self.original_df.columns:\n",
    "            merged_df = pd.merge(self.original_df, temp_df[[self.clean_key, self.key_name]],\n",
    "                                 on=self.key_name, how='left')\n",
    "            self.original_df[self.clean_key] = merged_df[self.clean_key]\n",
    "            merged_df = None\n",
    "\n",
    "        return self.original_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbf4af56-ba53-49e0-9bf6-84fd28ead7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def update_master_df(job_data_all, job_data):\n",
    "    # Reverse the order of job_data to check oldest entries first\n",
    "    job_data_reversed = job_data.iloc[::-1]\n",
    "    \n",
    "    # List to hold new entries\n",
    "    new_entries = []\n",
    "    \n",
    "    for i, row in job_data_reversed.iterrows():\n",
    "        # Check if row exists in job_data_all\n",
    "        is_exist = job_data_all[(job_data_all['job_title'] == row['job_title']) & \n",
    "                                (job_data_all['company_name'] == row['company_name']) &\n",
    "                                (job_data_all['location'] == row['location'])].shape[0]\n",
    "        \n",
    "        # If row does not exist in job_data_all, add it to new_entries\n",
    "        if is_exist == 0:\n",
    "            print('NEWWWWWW')\n",
    "            print(row)\n",
    "            new_entries.append(row)\n",
    "    \n",
    "    # Convert new_entries to DataFrame and concatenate it with job_data_all\n",
    "    if new_entries:\n",
    "        new_entries_df = pd.DataFrame(new_entries)\n",
    "        job_data_all = pd.concat([new_entries_df, job_data_all], ignore_index=True)\n",
    "    \n",
    "    return job_data_all\n",
    "\n",
    "\n",
    "def get_job_blocks(gpt_text, job_data):\n",
    "    job_blocks = []\n",
    "    combined_titles = \"|\".join([re.escape(title) for title in job_data['job_title']])\n",
    "    combined_companies = \"|\".join([re.escape(company) for company in job_data['company_name']])\n",
    "    pattern = f\"({combined_titles}).*?({combined_companies}).*?(?=({combined_titles}).*?({combined_companies})|$)\"\n",
    "    \n",
    "    for match in re.finditer(pattern, gpt_text, re.DOTALL):\n",
    "        job_blocks.append(match.group(0))\n",
    "\n",
    "    return job_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a482f315-cd3f-465d-b35e-867b73505133",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "def go_to_next_page(driver):\n",
    "    try:\n",
    "        # Step 1: Find the pagination element\n",
    "        pagination = driver.find_element(By.CSS_SELECTOR, 'ul.artdeco-pagination__pages')\n",
    "    except NoSuchElementException:\n",
    "        print(\"Pagination not found\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Step 2: Find the current active page\n",
    "        current_page_elem = pagination.find_element(By.CSS_SELECTOR, 'li.active')\n",
    "        current_page = int(current_page_elem.text)\n",
    "    except NoSuchElementException:\n",
    "        print(\"Current page not found\")\n",
    "        return\n",
    "\n",
    "    # Step 3: Find the next page\n",
    "    next_page = current_page + 1\n",
    "    next_page_selector = f'li[data-test-pagination-page-btn=\"{next_page}\"]'\n",
    "\n",
    "    try:\n",
    "        next_page_elem = pagination.find_element(By.CSS_SELECTOR, next_page_selector)\n",
    "    except NoSuchElementException:\n",
    "        print(\"Next page not found\")\n",
    "        return\n",
    "\n",
    "    # Step 4: Click the next page\n",
    "    next_page_elem.click()\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from datetime import datetime\n",
    "\n",
    "def update_dataframe(df_main, df_new, keys=None):\n",
    "    if keys is None:\n",
    "        keys = ['job_ids', 'job_title', 'company_name', 'Location', 'pay']\n",
    "\n",
    "    # Check if types are the same for each key\n",
    "    mismatched_keys = []\n",
    "    for key in keys:\n",
    "        type_main = df_main[key].dtype\n",
    "        type_new = df_new[key].dtype\n",
    "        if type_main != type_new:\n",
    "            mismatched_keys.append((key, type_main, type_new))\n",
    "\n",
    "    if mismatched_keys:\n",
    "        print(\"Type mismatch for these keys:\")\n",
    "        for key, type_main, type_new in mismatched_keys:\n",
    "            print(f\"Key: {key}, Type in df_main: {type_main}, Type in df_new: {type_new}\")\n",
    "        raise TypeError(\"Type mismatch detected.\")\n",
    "        \n",
    "    for _, new_row in df_new.iterrows():\n",
    "        mask = df_main[keys].eq(new_row[keys]).all(axis=1)\n",
    "        if not any(mask):\n",
    "            df_main = pd.concat([df_main, pd.DataFrame([new_row])], ignore_index=True)\n",
    "            os.system('say \"youve got jobs\"')  # macOS\n",
    "            print('\\nNNNNNEEWWWWWWW')\n",
    "            print(new_row['job_link'])\n",
    "            print(new_row)\n",
    "\n",
    "    return df_main\n",
    "\n",
    "\n",
    "# def update_dataframe(df_main, df_new, keys=None):\n",
    "#     if keys is None:\n",
    "#         # keys = df_main.columns.tolist()\n",
    "#         keys = ['job_ids', 'job_title', 'company_name', 'Location', 'pay']\n",
    "        \n",
    "        \n",
    "#     for i, new_row in df_new.iterrows():\n",
    "#         if not any(df_main[keys].eq(new_row[keys]).all(axis=1)):\n",
    "            \n",
    "#             df_main = pd.concat([df_main, pd.DataFrame([new_row])], ignore_index=True)\n",
    "#             print('\\nNNNNNEEWWWWWWW')\n",
    "#             print('\\n')\n",
    "#             print(new_row['job_link'])\n",
    "#             print(new_row)\n",
    "#             asdfasdfasdf\n",
    "#     return df_main\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "def get_job_details(driver):\n",
    "    job_dict = {\n",
    "        \"job_ids\": [],\n",
    "        \"is_promoted\": [],\n",
    "        \"job_title\": [],\n",
    "        \"company_name\": [],\n",
    "        \"Location\": [],\n",
    "        \"pay\": [],\n",
    "        \"job_link\": [],\n",
    "        \"time_added\": [],\n",
    "    }\n",
    "    \n",
    "    jobs = driver.find_elements(By.CSS_SELECTOR, '[data-occludable-job-id]')\n",
    "    \n",
    "    for job in jobs:\n",
    "        try:\n",
    "            job_id = job.get_attribute(\"data-occludable-job-id\")\n",
    "        except NoSuchElementException:\n",
    "            job_id = -1\n",
    "        \n",
    "        try:\n",
    "            promoted_elements = job.find_elements(By.CSS_SELECTOR, '.job-card-container__footer-item')\n",
    "            is_promoted = \"Promoted\" in [e.text for e in promoted_elements]\n",
    "        except NoSuchElementException:\n",
    "            is_promoted = False\n",
    "\n",
    "        try:\n",
    "            job_title = job.find_element(By.CSS_SELECTOR, '.job-card-list__title').text\n",
    "        except NoSuchElementException:\n",
    "            job_title = \"\"\n",
    "\n",
    "        try:\n",
    "            company_name = job.find_element(By.CSS_SELECTOR, '.job-card-container__primary-description').text\n",
    "        except NoSuchElementException:\n",
    "            company_name = \"\"\n",
    "\n",
    "        try:\n",
    "            Location = job.find_element(By.CSS_SELECTOR, '.job-card-container__metadata-wrapper li').text\n",
    "        except NoSuchElementException:\n",
    "            Location = \"\"\n",
    "\n",
    "        try:\n",
    "            pay = job.find_element(By.CSS_SELECTOR, '.mt1 .job-card-container__metadata-wrapper li').text\n",
    "        except NoSuchElementException:\n",
    "            pay = \"\"\n",
    "            \n",
    "        try:\n",
    "            job_link = job.find_element(By.CSS_SELECTOR, \"a.job-card-container__link\").get_attribute(\"href\")\n",
    "        except NoSuchElementException:\n",
    "            job_link = \"\"\n",
    "            \n",
    "        \n",
    "\n",
    "        if job_title != \"\" and company_name != \"\":\n",
    "            job_dict[\"job_ids\"].append(job_id)\n",
    "            job_dict[\"is_promoted\"].append(is_promoted)\n",
    "            job_dict[\"job_title\"].append(job_title)\n",
    "            job_dict[\"company_name\"].append(company_name)\n",
    "            job_dict[\"Location\"].append(Location)\n",
    "            job_dict[\"pay\"].append(pay)\n",
    "            job_dict[\"job_link\"].append(job_link)\n",
    "            \n",
    "            current_time = datetime.now()\n",
    "            formatted_time = current_time.strftime('%Y%m%d%H%M%S')\n",
    "            formatted_time_as_int = int(formatted_time)\n",
    "            job_dict[\"time_added\"].append(formatted_time_as_int)\n",
    "            \n",
    "        \n",
    "    return job_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d676d81c",
   "metadata": {},
   "source": [
    "### INIT API key,  intrucitons for GPT which can be saved as text files in the correct data/instrucitons dir of the package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75257fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # setup API key for chatGPT \n",
    "# load_dotenv()  # take environment variables from .env.\n",
    "# os.environ[\"OPENAI_API_KEY\"]  = os.getenv(\"OPENAI_API_KEY\")\n",
    "# openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cdd976",
   "metadata": {},
   "source": [
    "# Info_dict to set up all used variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "379f097d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup basic variable as dict \n",
    "info_dict = {'init_url':'https://www.linkedin.com/',\n",
    "             'save_password_dir':'/Users/phil/Dropbox/GITHUB/DATA/scrapifurs/saved_cookies/',\n",
    "             'start_url':'https://www.linkedin.com/search/results/people/?keywords=data%20scientist&origin=CLUSTER_EXPANSION&sid=fRq'}\n",
    "info_dict['full_cookies_save_path'] = info_dict['save_password_dir']+os.sep+\"linkedin_cookies.pkl\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2b99e4",
   "metadata": {},
   "source": [
    "# Scraping step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6b98c4",
   "metadata": {},
   "source": [
    "### create rules to trim text based on key workds and search characteristics\n",
    "this willl help reduce the amount of data we pass to GPT API and reduce costs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c03b73d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_finder = utils.StringSectionExtractor()\n",
    "text_finder.add_start_rule('\\d+ results', True)\n",
    "text_finder.add_start_rule('All filters', False)\n",
    "text_finder.add_start_rule('Jump to active job details', False)\n",
    "\n",
    "\n",
    "text_finder.add_end_rule('Page \\d+ of \\d+', True)\n",
    "text_finder.add_end_rule(\"Are you finding\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d61225",
   "metadata": {},
   "source": [
    "### INIT: chrome browser, login, save cookies (future login)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d1aa5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#init chrome \n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--disable-extensions\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "driver.get(info_dict['init_url'])\n",
    "time.sleep(1)\n",
    "driver.get(info_dict['init_url'])\n",
    "time.sleep(2)\n",
    "\n",
    "\n",
    "# Load cookies if they exist\n",
    "try:\n",
    "    cookies = pickle.load(open(info_dict['full_cookies_save_path'], \"rb\"))\n",
    "    for cookie in cookies:\n",
    "        driver.add_cookie(cookie)\n",
    "    driver.refresh()\n",
    "    assert(not not cookies)# if empty try a different method\n",
    "except:\n",
    "    print(\"No cookies found. Manual login required.\")\n",
    "    # If not logged in\n",
    "    input('Please login and press Enter to continue...')\n",
    "    pickle.dump(driver.get_cookies(), open(info_dict['full_cookies_save_path'], \"wb\")) # save cookies after login\n",
    "    \n",
    "\n",
    "time.sleep(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac45c456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2700525-eb14-4581-8a73-e0b219c6bef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c8ab79-50dc-46a0-a062-7bd91d5e2b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test scraping full job page for saving data for all jobs I apllied for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "caa7cd4b-e266-4b19-8d19-07d1e4d11c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 'https://www.linkedin.com/jobs/view/3743737484/?eBP=JOB_SEARCH_ORGANIC&refId=wIC2skgivSr27kABYstWjg%3D%3D&trackingId=XVrPkXaydKf4xGrJyLPUOw%3D%3D'\n",
    "info_dict['start_url'] = x\n",
    "driver.get(info_dict['start_url'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ce1ece3-21b4-4be0-a6d2-a72fa70c8a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'job_title': 'Data Scientist', 'company_name': ''}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "def get_job_detail(driver):\n",
    "    job_detail_dict = {\n",
    "        \"job_title\": \"\",\n",
    "        \"company_name\": \"\",\n",
    "        # ... add other fields as necessary\n",
    "    }\n",
    "\n",
    "    # Scrape the job title\n",
    "    try:\n",
    "        job_detail_dict[\"job_title\"] = driver.find_element(By.CSS_SELECTOR, 'h1.t-24.t-bold.job-details-jobs-unified-top-card__job-title').text.strip()\n",
    "    except NoSuchElementException:\n",
    "        job_detail_dict[\"job_title\"] = \"\"\n",
    "\n",
    "    # Scrape the company name\n",
    "    try:\n",
    "        job_detail_dict[\"company_name\"] = driver.find_element(By.CSS_SELECTOR, 'a.app-aware-link').text.strip()\n",
    "    except NoSuchElementException:\n",
    "        job_detail_dict[\"company_name\"] = \"\"\n",
    "\n",
    "    # ... similarly add extraction logic for any other information you need\n",
    "\n",
    "    return job_detail_dict\n",
    "\n",
    "get_job_detail(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cc01bc76-6f65-4f4a-a7f2-ed1153e87714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Scientist\n"
     ]
    }
   ],
   "source": [
    "# Function to safely fetch elements\n",
    "def safe_extract(selector):\n",
    "    try:\n",
    "        return driver.find_element(By.CSS_SELECTOR, selector).text\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Extract contents with updated selectors\n",
    "job_title = safe_extract('h1')  # This one works\n",
    "company_info = safe_extract('.job-details-jobs-unified-top-card__subtitle span:nth-child(1)')  # Adjusting selector\n",
    "job_description = safe_extract('.description')  # Using a more common class name for job descriptions\n",
    "\n",
    "print(job_title)\n",
    "# print(company_info)\n",
    "# print(job_description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "63166851-7fc5-4970-8963-7bab357b07a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Title: Data Scientist\n",
      "Company Name: Expa\n",
      "Company Logo URL: https://media.licdn.com/dms/image/C4E0BAQGYjwhcNh5EVg/company-logo_100_100/0/1613080862386/expa_logo?e=1707350400&v=beta&t=-CR39e7rJogy2WMQ3q9z4H8BCPU7GUMjaJYG0ixayoM\n",
      "Job Location:  \n",
      "Time Since Posted: 1 week ago\n",
      "Number of Applicants: Over 100 applicants\n",
      "Salary Range: $100,000/yr - $120,000/yr\n",
      "Job Type: Hybrid\n",
      "Job Level: Full-time\n",
      "Matches your job preferences, job type is Full-time.\n",
      "Company Info: 11-50 employees · Technology, Information and Internet\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define a function to safely extract elements\n",
    "def safe_extract(element, css_selector, attribute=None):\n",
    "    try:\n",
    "        if attribute:\n",
    "            return element.find_element(By.CSS_SELECTOR, css_selector).get_attribute(attribute)\n",
    "        return element.find_element(By.CSS_SELECTOR, css_selector).text\n",
    "    except NoSuchElementException:\n",
    "        return None\n",
    "\n",
    "# Extracting data from the provided HTML\n",
    "job_title = safe_extract(driver, '.job-details-jobs-unified-top-card__job-title')\n",
    "company_name = safe_extract(driver, '.job-details-jobs-unified-top-card__primary-description a')\n",
    "company_logo_url = safe_extract(driver, '.ivm-view-attr__img--centered', 'src')\n",
    "job_location = safe_extract(driver, '.job-details-jobs-unified-top-card__primary-description span:nth-child(2)')\n",
    "time_since_posted = safe_extract(driver, '.tvm__text--neutral span')\n",
    "num_applicants = safe_extract(driver, '.tvm__text--neutral:last-child')\n",
    "salary_range = safe_extract(driver, '.job-details-jobs-unified-top-card__job-insight span span:first-child')\n",
    "job_type = safe_extract(driver, '.ui-label.ui-label--accent-3:nth-child(1) span:first-child')\n",
    "job_level = safe_extract(driver, '.job-details-jobs-unified-top-card__job-insight-view-model-secondary:nth-child(3)')\n",
    "company_info = safe_extract(driver, '.job-details-jobs-unified-top-card__job-insight:nth-child(2) span')\n",
    "\n",
    "job_description_element = \".jobs-description-content__text\"\n",
    "description = safe_extract(driver, job_description_element)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Job Title: {job_title}\")\n",
    "print(f\"Company Name: {company_name}\")\n",
    "print(f\"Company Logo URL: {company_logo_url}\")\n",
    "print(f\"Job Location: {job_location}\")\n",
    "print(f\"Time Since Posted: {time_since_posted}\")\n",
    "print(f\"Number of Applicants: {num_applicants}\")\n",
    "print(f\"Salary Range: {salary_range}\")\n",
    "print(f\"Job Type: {job_type}\")\n",
    "print(f\"Job Level: {job_level}\")\n",
    "print(f\"Company Info: {company_info}\")\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5d140a8c-1ae8-4545-b125-64dd5b6be855",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_description_element = \".jobs-description-content__text\"\n",
    "description = safe_extract(driver, job_description_element)\n",
    "\n",
    "# print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9135abe0-40da-49d1-b953-ae3eb30b6eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los Angeles, CA  1 week ago\n"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Use the find_element method with By.CSS_SELECTOR to get the location\n",
    "location_div = driver.find_element(By.CSS_SELECTOR, \".job-details-jobs-unified-top-card__primary-description div\")\n",
    "location_text = location_div.text\n",
    "\n",
    "# Since the location is preceded by a `·`, we'll split by it and take the second item\n",
    "job_location = location_text.split('·')[1].strip()\n",
    "print(job_location)  # Outputs: Los Angeles, CA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5f06a429-1f5e-4f97-88ec-f1e575193169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los Angeles,\n"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Use the find_element method with By.CSS_SELECTOR to get the location\n",
    "location_div = driver.find_element(By.CSS_SELECTOR, \".job-details-jobs-unified-top-card__primary-description div\")\n",
    "location_parts = location_div.text.split('·')\n",
    "\n",
    "# Assuming the location is always the second item after splitting by `·`, but stripping any additional info after city and state/country\n",
    "potential_location = location_parts[1].strip().split()\n",
    "\n",
    "# Checking for common separators to extract the primary location (like \",\", \"-\")\n",
    "if \",\" in potential_location[1]:\n",
    "    job_location = ' '.join(potential_location[:2])\n",
    "else:\n",
    "    job_location = ' '.join(potential_location[:1])\n",
    "\n",
    "print(job_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e989ed07-409f-4a1f-9971-389f37ea80ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2d1e62-42aa-4411-bc77-4ef6246305c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f068539c-9d54-41f9-94d8-8fa169cde455",
   "metadata": {},
   "outputs": [
    {
     "ename": "WebDriverException",
     "evalue": "Message: disconnected: not connected to DevTools\n  (failed to check if window was closed: disconnected: not connected to DevTools)\n  (Session info: chrome=118.0.5993.117)\nStacktrace:\n0   chromedriver                        0x0000000101278510 chromedriver + 4310288\n1   chromedriver                        0x00000001012704bc chromedriver + 4277436\n2   chromedriver                        0x0000000100ea3b6c chromedriver + 293740\n3   chromedriver                        0x0000000100e8b39c chromedriver + 193436\n4   chromedriver                        0x0000000100e8b2a4 chromedriver + 193188\n5   chromedriver                        0x0000000100f2370c chromedriver + 816908\n6   chromedriver                        0x0000000100edcfd0 chromedriver + 528336\n7   chromedriver                        0x0000000100edde7c chromedriver + 532092\n8   chromedriver                        0x000000010123e834 chromedriver + 4073524\n9   chromedriver                        0x00000001012427fc chromedriver + 4089852\n10  chromedriver                        0x0000000101242c58 chromedriver + 4090968\n11  chromedriver                        0x00000001012488f8 chromedriver + 4114680\n12  chromedriver                        0x0000000101243234 chromedriver + 4092468\n13  chromedriver                        0x000000010121d604 chromedriver + 3937796\n14  chromedriver                        0x000000010125fee8 chromedriver + 4210408\n15  chromedriver                        0x0000000101260064 chromedriver + 4210788\n16  chromedriver                        0x0000000101270134 chromedriver + 4276532\n17  libsystem_pthread.dylib             0x000000018a71ffa8 _pthread_start + 148\n18  libsystem_pthread.dylib             0x000000018a71ada0 thread_start + 8\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 45\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Extracting data from the provided HTML\u001b[39;00m\n\u001b[1;32m     44\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinked_in_link\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_url\u001b[49m,\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob_title\u001b[39m\u001b[38;5;124m\"\u001b[39m: safe_extract(driver, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.job-details-jobs-unified-top-card__job-title\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompany_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: safe_extract(driver, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.job-details-jobs-unified-top-card__primary-description a\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply_link\u001b[39m\u001b[38;5;124m\"\u001b[39m: get_apply_link(driver),\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob_location\u001b[39m\u001b[38;5;124m\"\u001b[39m: get_location(driver),\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_since_posted\u001b[39m\u001b[38;5;124m\"\u001b[39m: safe_extract(driver, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.tvm__text--neutral span\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_applicants\u001b[39m\u001b[38;5;124m\"\u001b[39m: safe_extract(driver, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.tvm__text--neutral:last-child\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msalary_range\u001b[39m\u001b[38;5;124m\"\u001b[39m: safe_extract(driver, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.job-details-jobs-unified-top-card__job-insight span span:first-child\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: safe_extract(driver, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.ui-label.ui-label--accent-3:nth-child(1) span:first-child\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob_level\u001b[39m\u001b[38;5;124m\"\u001b[39m: safe_extract(driver, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.job-details-jobs-unified-top-card__job-insight-view-model-secondary:nth-child(3)\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompany_info\u001b[39m\u001b[38;5;124m\"\u001b[39m: safe_extract(driver, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.job-details-jobs-unified-top-card__job-insight:nth-child(2) span\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m\"\u001b[39m: safe_extract(driver, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jobs-description-content__text\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m }\n\u001b[1;32m     58\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     59\u001b[0m     key: [value] \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m     60\u001b[0m }\n\u001b[1;32m     62\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data)\n",
      "File \u001b[0;32m~/Library/CloudStorage/Dropbox/GITHUB/scrapifurs/.venv_M1/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py:435\u001b[0m, in \u001b[0;36mWebDriver.current_url\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcurrent_url\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    428\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Gets the URL of the current page.\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \n\u001b[1;32m    430\u001b[0m \u001b[38;5;124;03m    :Usage:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;124;03m            driver.current_url\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 435\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGET_CURRENT_URL\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Library/CloudStorage/Dropbox/GITHUB/scrapifurs/.venv_M1/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py:345\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    343\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m--> 345\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Library/CloudStorage/Dropbox/GITHUB/scrapifurs/.venv_M1/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[0;31mWebDriverException\u001b[0m: Message: disconnected: not connected to DevTools\n  (failed to check if window was closed: disconnected: not connected to DevTools)\n  (Session info: chrome=118.0.5993.117)\nStacktrace:\n0   chromedriver                        0x0000000101278510 chromedriver + 4310288\n1   chromedriver                        0x00000001012704bc chromedriver + 4277436\n2   chromedriver                        0x0000000100ea3b6c chromedriver + 293740\n3   chromedriver                        0x0000000100e8b39c chromedriver + 193436\n4   chromedriver                        0x0000000100e8b2a4 chromedriver + 193188\n5   chromedriver                        0x0000000100f2370c chromedriver + 816908\n6   chromedriver                        0x0000000100edcfd0 chromedriver + 528336\n7   chromedriver                        0x0000000100edde7c chromedriver + 532092\n8   chromedriver                        0x000000010123e834 chromedriver + 4073524\n9   chromedriver                        0x00000001012427fc chromedriver + 4089852\n10  chromedriver                        0x0000000101242c58 chromedriver + 4090968\n11  chromedriver                        0x00000001012488f8 chromedriver + 4114680\n12  chromedriver                        0x0000000101243234 chromedriver + 4092468\n13  chromedriver                        0x000000010121d604 chromedriver + 3937796\n14  chromedriver                        0x000000010125fee8 chromedriver + 4210408\n15  chromedriver                        0x0000000101260064 chromedriver + 4210788\n16  chromedriver                        0x0000000101270134 chromedriver + 4276532\n17  libsystem_pthread.dylib             0x000000018a71ffa8 _pthread_start + 148\n18  libsystem_pthread.dylib             0x000000018a71ada0 thread_start + 8\n"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "def get_apply_link(driver):\n",
    "    try:\n",
    "        # 1. Open the link in a new tab using JavaScript.\n",
    "        apply_button = driver.find_element(By.CSS_SELECTOR, \".jobs-apply-button.artdeco-button\")\n",
    "        driver.execute_script(\"window.open(arguments[0].click(), '_blank');\", apply_button)\n",
    "        # 2. Switch to the new tab.\n",
    "        driver.switch_to.window(driver.window_handles[1])\n",
    "        # 3. Copy the URL.\n",
    "        apply_link = driver.current_url\n",
    "        # 4. Close the new tab.\n",
    "        driver.close()\n",
    "        # 5. Switch back to the original tab.\n",
    "        driver.switch_to.window(driver.window_handles[0])\n",
    "        return apply_link\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_location(element):\n",
    "    location_div = element.find_element(By.CSS_SELECTOR, \".job-details-jobs-unified-top-card__primary-description div\")\n",
    "    location_parts = location_div.text.split('·')\n",
    "    potential_location = location_parts[1].strip().split()\n",
    "    if \",\" in potential_location[1]:\n",
    "        return ' '.join(potential_location[:2])\n",
    "    else:\n",
    "        return ' '.join(potential_location[:1])\n",
    "\n",
    "# Define a function to safely extract elements\n",
    "def safe_extract(element, css_selector, attribute=None):\n",
    "    try:\n",
    "        if attribute:\n",
    "            return element.find_element(By.CSS_SELECTOR, css_selector).get_attribute(attribute)\n",
    "        return element.find_element(By.CSS_SELECTOR, css_selector).text\n",
    "    except NoSuchElementException:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "# Extracting data from the provided HTML\n",
    "data = {\n",
    "    \"linked_in_link\": driver.current_url,\n",
    "    \"job_title\": safe_extract(driver, '.job-details-jobs-unified-top-card__job-title'),\n",
    "    \"company_name\": safe_extract(driver, '.job-details-jobs-unified-top-card__primary-description a'),\n",
    "    \"apply_link\": get_apply_link(driver),\n",
    "    \"job_location\": get_location(driver),\n",
    "    \"time_since_posted\": safe_extract(driver, '.tvm__text--neutral span'),\n",
    "    \"num_applicants\": safe_extract(driver, '.tvm__text--neutral:last-child'),\n",
    "    \"salary_range\": safe_extract(driver, '.job-details-jobs-unified-top-card__job-insight span span:first-child'),\n",
    "    \"job_type\": safe_extract(driver, '.ui-label.ui-label--accent-3:nth-child(1) span:first-child'),\n",
    "    \"job_level\": safe_extract(driver, '.job-details-jobs-unified-top-card__job-insight-view-model-secondary:nth-child(3)'),\n",
    "    \"company_info\": safe_extract(driver, '.job-details-jobs-unified-top-card__job-insight:nth-child(2) span'),\n",
    "    \"description\": safe_extract(driver, \".jobs-description-content__text\")\n",
    "}\n",
    "data = {\n",
    "    key: [value] for key, value in data.items()\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "09660b53-3e44-4f97-a063-f364e5388c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Locate the \"Apply\" button and click it\n",
    "# apply_button = driver.find_element(By.CSS_SELECTOR, \".jobs-apply-button.artdeco-button\")\n",
    "# apply_button.click()\n",
    "# time.sleep(4)\n",
    "# # After clicking, get the current URL\n",
    "# apply_link2 = driver.current_url\n",
    "# apply_link2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9bbc5574-ce92-4d58-8856-25fbb1e56f53",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (2733806538.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[83], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    data[\"linked_in_link\"]\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "job_id_match = re.search(r'/view/(\\d+)/', data[\"linked_in_link\"])\n",
    "job_id = job_id_match.group(1) if job_id_match else None\n",
    "\n",
    " data[\"linked_in_link\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d621e6-4538-453b-bb9a-2cd71fca0600",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac7c849-51a3-426a-b136-5314cf4c6b19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0180ddc-471a-4dac-93c6-54d637aa97e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "get linked in link \n",
    "linked in job number use this as the index \n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "499900fd-9307-4b19-a628-26795813f11e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Use the function\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m job_id \u001b[38;5;241m=\u001b[39m \u001b[43mget_job_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m#datalet-bpr-guid-3188948\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[79], line 7\u001b[0m, in \u001b[0;36mget_job_id\u001b[0;34m(element, css_selector)\u001b[0m\n\u001b[1;32m      4\u001b[0m content \u001b[38;5;241m=\u001b[39m safe_extract(element, css_selector)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Use regex to find the job ID number from the content\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m job_id_match \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/voyager/api/jobs/jobPostings/(\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43md+)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m job_id_match:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m job_id_match\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/re/__init__.py:176\u001b[0m, in \u001b[0;36msearch\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch\u001b[39m(pattern, string, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    174\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Scan through string looking for a match to the pattern, returning\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;124;03m    a Match object, or None if no match was found.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object, got 'NoneType'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def get_job_id(element, css_selector):\n",
    "    content = safe_extract(element, css_selector)\n",
    "    \n",
    "    # Use regex to find the job ID number from the content\n",
    "    job_id_match = re.search(r'/voyager/api/jobs/jobPostings/(\\d+)', content)\n",
    "    if job_id_match:\n",
    "        return job_id_match.group(1)\n",
    "    return None\n",
    "\n",
    "# Use the function\n",
    "job_id = get_job_id(driver, '#datalet-bpr-guid-3188948')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f2b8ad-9f3d-4e20-b064-4e71fafb1b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5276d6-50fb-4e4b-8426-76d4ddc64840",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1be6249-f0fc-4966-a8d0-1589de2b59a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4038f956-4558-4205-bb96-e52d06a26c0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WebElement' object has no attribute 'find_elements_by_xpath'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m all_elements_data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m elements_with_job:\n\u001b[0;32m---> 29\u001b[0m     all_elements_data\u001b[38;5;241m.\u001b[39mappend(\u001b[43mget_element_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(all_elements_data)\n",
      "Cell \u001b[0;32mIn[49], line 10\u001b[0m, in \u001b[0;36mget_element_data\u001b[0;34m(element)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_element_data\u001b[39m(element):\n\u001b[1;32m      2\u001b[0m     data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtag\u001b[39m\u001b[38;5;124m\"\u001b[39m: element\u001b[38;5;241m.\u001b[39mtag_name,\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m: element\u001b[38;5;241m.\u001b[39mget_attribute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchildren\u001b[39m\u001b[38;5;124m\"\u001b[39m: []\n\u001b[1;32m      8\u001b[0m     }\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m \u001b[43melement\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_elements_by_xpath\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./*\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m child\u001b[38;5;241m.\u001b[39mget_attribute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01mor\u001b[39;00m \\\n\u001b[1;32m     12\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m child\u001b[38;5;241m.\u001b[39mget_attribute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01mor\u001b[39;00m \\\n\u001b[1;32m     13\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m child\u001b[38;5;241m.\u001b[39mget_attribute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mlower():\n\u001b[1;32m     14\u001b[0m             child_data \u001b[38;5;241m=\u001b[39m get_element_data(child)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'WebElement' object has no attribute 'find_elements_by_xpath'"
     ]
    }
   ],
   "source": [
    "def get_element_data(element):\n",
    "    data = {\n",
    "        \"tag\": element.tag_name,\n",
    "        \"class\": element.get_attribute('class'),\n",
    "        \"id\": element.get_attribute('id'),\n",
    "        \"text\": element.text.strip(),\n",
    "        \"children\": []\n",
    "    }\n",
    "\n",
    "    for child in element.find_elements_by_xpath(\"./*\"):\n",
    "        if \"job\" in child.get_attribute('class').lower() or \\\n",
    "           \"job\" in child.get_attribute('id').lower() or \\\n",
    "           \"job\" in child.get_attribute('name').lower():\n",
    "            child_data = get_element_data(child)\n",
    "            data[\"children\"].append(child_data)\n",
    "            # Removing child's text from parent to avoid duplication\n",
    "            data[\"text\"] = data[\"text\"].replace(child_data[\"text\"], '').strip()\n",
    "\n",
    "    return data\n",
    "\n",
    "elements_with_job = WebDriverWait(driver, 1).until(\n",
    "    EC.presence_of_all_elements_located((By.CSS_SELECTOR, \n",
    "        \"[class*='job'], [id*='job'], [name*='job']\"))\n",
    ")\n",
    "\n",
    "all_elements_data = []\n",
    "\n",
    "for element in elements_with_job:\n",
    "    all_elements_data.append(get_element_data(element))\n",
    "\n",
    "print(all_elements_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37049dae-a942-40ea-a1cd-5dc7d67a835c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0243dccd-79b8-45db-9ef7-b1bf0b4b13a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66146874-4bae-4684-87f8-52bc485856f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6885c9c5-833a-42fa-b1ae-b6e5856c3e16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9378a219-289c-4bf1-a3a5-226ed5564071",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b92c1e-7e49-4f15-aeb3-85e0f9397cee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e1037c-909d-4921-bd40-910f34086328",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13743168-6cae-48a8-bcf3-62547ff035e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f0cd9c-8d3d-4147-8a6e-9cdbf3f294fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_job_details(driver):\n",
    "    job_dict = {\n",
    "        \"job_ids\": [],\n",
    "        \"is_promoted\": [],\n",
    "        \"job_title\": [],\n",
    "        \"company_name\": [],\n",
    "        \"Location\": [],\n",
    "        \"pay\": [],\n",
    "        \"job_link\": [],\n",
    "        \"time_added\": [],\n",
    "    }\n",
    "    \n",
    "    jobs = driver.find_elements(By.CSS_SELECTOR, '[data-occludable-job-id]')\n",
    "    \n",
    "    for job in jobs:\n",
    "        try:\n",
    "            job_id = job.get_attribute(\"data-occludable-job-id\")\n",
    "        except NoSuchElementException:\n",
    "            job_id = -1\n",
    "        \n",
    "        try:\n",
    "            promoted_elements = job.find_elements(By.CSS_SELECTOR, '.job-card-container__footer-item')\n",
    "            is_promoted = \"Promoted\" in [e.text for e in promoted_elements]\n",
    "        except NoSuchElementException:\n",
    "            is_promoted = False\n",
    "\n",
    "        try:\n",
    "            job_title = job.find_element(By.CSS_SELECTOR, '.job-card-list__title').text\n",
    "        except NoSuchElementException:\n",
    "            job_title = \"\"\n",
    "\n",
    "        try:\n",
    "            company_name = job.find_element(By.CSS_SELECTOR, '.job-card-container__primary-description').text\n",
    "        except NoSuchElementException:\n",
    "            company_name = \"\"\n",
    "\n",
    "        try:\n",
    "            Location = job.find_element(By.CSS_SELECTOR, '.job-card-container__metadata-wrapper li').text\n",
    "        except NoSuchElementException:\n",
    "            Location = \"\"\n",
    "\n",
    "        try:\n",
    "            pay = job.find_element(By.CSS_SELECTOR, '.mt1 .job-card-container__metadata-wrapper li').text\n",
    "        except NoSuchElementException:\n",
    "            pay = \"\"\n",
    "            \n",
    "        try:\n",
    "            job_link = job.find_element(By.CSS_SELECTOR, \"a.job-card-container__link\").get_attribute(\"href\")\n",
    "        except NoSuchElementException:\n",
    "            job_link = \"\"\n",
    "            \n",
    "        \n",
    "\n",
    "        if job_title != \"\" and company_name != \"\":\n",
    "            job_dict[\"job_ids\"].append(job_id)\n",
    "            job_dict[\"is_promoted\"].append(is_promoted)\n",
    "            job_dict[\"job_title\"].append(job_title)\n",
    "            job_dict[\"company_name\"].append(company_name)\n",
    "            job_dict[\"Location\"].append(Location)\n",
    "            job_dict[\"pay\"].append(pay)\n",
    "            job_dict[\"job_link\"].append(job_link)\n",
    "            \n",
    "            current_time = datetime.now()\n",
    "            formatted_time = current_time.strftime('%Y%m%d%H%M%S')\n",
    "            formatted_time_as_int = int(formatted_time)\n",
    "            job_dict[\"time_added\"].append(formatted_time_as_int)\n",
    "            \n",
    "        \n",
    "    return job_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95ee35f",
   "metadata": {},
   "source": [
    "#### navigate to the first page we want to scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d76c2195",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# x = 'https://www.linkedin.com/jobs/search/?currentJobId=3481251151&distance=25&f_PP=102277331%2C106233382%2C102448103%2C103918656%2C102250832%2C103575230%2C100075706&f_T=25206%2C340%2C25190%2C25887%2C30209%2C288%2C2463%2C25584&f_WT=1&geoId=103644278&keywords=data%20scientist&origin=JOB_SEARCH_PAGE_JOB_FILTER&sortBy=DD'\n",
    "# x = 'https://www.linkedin.com/jobs/search/?currentJobId=3682851114&distance=25&f_PP=102277331%2C106233382%2C102448103%2C103918656%2C102250832%2C103575230%2C100075706&f_T=25206%2C340%2C25190%2C25887%2C30209%2C25584%2C2463%2C288%2C30128&f_WT=1%2C2%2C3&geoId=103644278&keywords=data%20scientist&origin=JOB_SEARCH_PAGE_JOB_FILTER&sortBy=DD'\n",
    "# x = 'https://www.linkedin.com/jobs/search/?currefntJobId=3642657953&distance=25&f_PP=102277331%2C106233382%2C102448103%2C103918656%2C102250832%2C103575230%2C100075706&f_T=25206%2C340%2C25190%2C25887%2C30209%2C25584%2C2463%2C288%2C30128&f_TPR=r86400&f_WT=1%2C2%2C3&geoId=103644278&keywords=data%20scientist&origin=JOB_SEARCH_PAGE_JOB_FILTER&sortBy=DD'\n",
    "# x = 'https://www.linkedin.com/jobs/search/?currentJobId=3737611604&distance=25&f_PP=102277331%2C106233382%2C102448103%2C103918656%2C102250832%2C103575230%2C100075706&f_T=25206%2C340%2C25190%2C25887%2C30209%2C25584%2C2463%2C288%2C30128&f_TPR=r604800&f_WT=1%2C2%2C3&geoId=103644278&keywords=data%20scientist&origin=JOB_SEARCH_PAGE_JOB_FILTER&sortBy=DD'\n",
    "# # x = 'https://www.linkedin.com/jobs/search/?currentJobId=3729935051&distance=25&f_PP=102277331%2C102448103%2C106233382%2C103918656%2C100075706%2C102250832%2C103575230&f_TPR=r86400&f_WT=1%2C3%2C2&geoId=103644278&keywords=data%20scientist&origin=JOB_SEARCH_PAGE_JOB_FILTER&sortBy=DD'\n",
    "# x = 'https://www.linkedin.com/jobs/search/?currentJobId=3741083392&distance=25&f_PP=102277331%2C106233382%2C102448103%2C103918656%2C102250832%2C103575230%2C100075706&f_T=25206%2C340%2C25190%2C25887%2C30209%2C25584%2C2463%2C288%2C30128&f_TPR=r86400&f_WT=1%2C2%2C3&geoId=103644278&keywords=data%20scientist&origin=JOB_SEARCH_PAGE_JOB_FILTER&sortBy=DD'\n",
    "# info_dict['start_url'] = x\n",
    "# driver.get(info_dict['start_url'])\n",
    "# time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6b5fdf4-6d62-407b-b8b3-5e4b36922901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_main = pd.read_csv('/Users/phil/Dropbox/GITHUB/scrapifurs/scrapifurs/data/tempdata/DS_mega_data_V1.csv')\n",
    "# # df_main['job_ids'] = df_main['job_ids'].astype(int)\n",
    "# # df_main.to_csv('/Users/phil/Dropbox/GITHUB/scrapifurs/scrapifurs/data/tempdata/DS_mega_data_V1.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d5b6987-4e37-406f-a2e5-36412a4a305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # job_data = get_job_details(driver)\n",
    "# # df_main = pd.DataFrame(job_data)\n",
    "# # df_main\n",
    "\n",
    "# n_pages_to_scrape = 5\n",
    "# wait_sec_each_page = 10\n",
    "# update_every_n_secs = 60*6\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# while True:\n",
    "#     for n_page in range(n_pages_to_scrape):\n",
    "#         if n_page ==0:\n",
    "#             # go back to the first page\n",
    "#             driver.get(info_dict['start_url'])\n",
    "#         else:\n",
    "#             go_to_next_page(driver)\n",
    "#         time.sleep(wait_sec_each_page)\n",
    "#         # update data\n",
    "#         job_data = get_job_details(driver)\n",
    "#         job_data = pd.DataFrame(job_data)\n",
    "#         job_data['job_ids'] = job_data['job_ids'].astype('int')\n",
    "#         df_main = update_dataframe(df_main, job_data, ['job_ids'])\n",
    "    \n",
    "#     # save CSV \n",
    "#     df_main.to_csv('/Users/phil/Dropbox/GITHUB/scrapifurs/scrapifurs/data/tempdata/DS_mega_data_V1.csv', index=False)\n",
    "    \n",
    "#     print('_____________________________________________\\n\\n____________________')\n",
    "#     time.sleep(update_every_n_secs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d49988a-3cf8-48c6-9d2c-4dcfd6e96a81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706a2824-8f09-4596-a587-26c7622b9923",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673c3b54-eae9-41c8-b373-b4d6d9773d69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebeaa4bf-7157-4085-8f48-e9f84db6f913",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beab480d-8d33-409a-a469-18afbdc2266f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e2a3a7-7e35-460c-8557-1c0004823be5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c6a129-d894-4534-8cfd-69d9c7ff1e7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c64a4b-a57c-43b2-8ffd-098c32e79eae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346a35a0-369c-4a76-a687-d8edb7dc6ed6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541668a6-cab7-4885-90bf-3193dc898ba7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bf7096-c4b4-4d31-a236-a464eafaa3bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddff5ae9-0104-43ae-97de-9bde1b71c04c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e623dd13-6cde-4426-91f9-eefbeacdb45e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba675596-c736-424c-847f-d013d810f1d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eec75c3-a495-4ce8-a35f-ea43bc5eb0b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3463aa-54fd-4bf7-b1c7-47d7d462a740",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87587382-4eaf-4197-b95a-8146e58ca309",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f482ff80-a8d6-496c-9a41-9704c79831ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199feb0f-76c6-42e6-b848-c2e7b69a3b8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e10b9a-6f68-4e5a-8248-9e05b003cc27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74c1894-590a-473d-8267-d57662de9a96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d3bceb-d72f-4f65-84b3-11b5e862de54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c767fdd8-76fc-4c67-ae80-d09983648bad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19973bfe-762f-4e06-9719-2f273b4e316e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354b6d38-e576-4003-a296-ccf0e0f6f7ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87ed016-d637-4eb2-a4c8-35269fc4724f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrapifurs",
   "language": "python",
   "name": "scrapifurs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
