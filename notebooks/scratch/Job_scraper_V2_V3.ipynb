{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cae7f1fe",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbdc7a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\npip install scipy\\npip install h5py\\npip install matplotlib\\npip install opencv-python\\npip install seaborn\\npip install matplotlib\\npip install ipywidgets\\npip install matplotlib install ipywidgets\\npip install opencv-pythonjupyter nbextension enable --py widgetsnbextension\\npip install ipython\\npip install WordCloud\\npip install geopy\\npip install scipy\\npip install plotly\\npip install nbformat --upgrade\\npip install folium\\npip install geopandas\\npip install scikit-learn   \\npip install cairosvg\\npip install lxm\\n\\n\\n\\npip install selenium\\npip install pandas\\npip install python-dotenv\\npip install openai\\npip install beautifulsoup4\\npip install numpy\\npip install tqdm\\npip install seaborn\\npip install matplotlib\\npip install scipy\\npip install geopy\\npip install plotly\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "\n",
    "pip install scipy\n",
    "pip install h5py\n",
    "pip install matplotlib\n",
    "pip install opencv-python\n",
    "pip install seaborn\n",
    "pip install matplotlib\n",
    "pip install ipywidgets\n",
    "pip install matplotlib install ipywidgets\n",
    "pip install opencv-pythonjupyter nbextension enable --py widgetsnbextension\n",
    "pip install ipython\n",
    "pip install WordCloud\n",
    "pip install geopy\n",
    "pip install scipy\n",
    "pip install plotly\n",
    "pip install nbformat --upgrade\n",
    "pip install folium\n",
    "pip install geopandas\n",
    "pip install scikit-learn   \n",
    "pip install cairosvg\n",
    "pip install lxm\n",
    "\n",
    "\n",
    "\n",
    "pip install selenium\n",
    "pip install pandas\n",
    "pip install python-dotenv\n",
    "pip install openai\n",
    "pip install beautifulsoup4\n",
    "pip install numpy\n",
    "pip install tqdm\n",
    "pip install seaborn\n",
    "pip install matplotlib\n",
    "pip install scipy\n",
    "pip install geopy\n",
    "pip install plotly\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "863e81a0-751d-4372-9e32-ab78defb0722",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_instructions = \"\"\"\n",
    "for the below text using using your LLM ability (not python code) extract data from the text. get the following and save all as strings, if they don't exist for a specific job entry, use an empty string (i.e., \"\"). \n",
    "there will be exactly 20 of each. \n",
    "\n",
    "job_title \n",
    "company_name \n",
    "location\n",
    "\n",
    "format the output as a python single dictionary for all the jobs. for example all the job titles will use key \"job_title\" and be a list of the jobs\n",
    "\n",
    "I need to have have a standard output and use it directly for my analysis. process every single job \n",
    "\n",
    "Construct the dictionary: Organize all the extracted information into a dictionary format where the key represents the type of information and the value is a list of all the extracted information of that type in the order of appearance.\n",
    "\n",
    "Formatting the instructions: Always make sure to start the code block with a comment '##BEGIN##', fill in the data accordingly, and end it with '##END##'. All columns of data should align with each other.\n",
    "\n",
    "    For example:\n",
    "\n",
    "\n",
    "    ##BEGIN##\n",
    "job_data = {\n",
    "    \"job_title\": [...],\n",
    "    \"company_name\": [...],\n",
    "    \"location\": [...]\n",
    "}\n",
    "    ##END##\n",
    "\n",
    "    Please make sure to follow these instructions to achieve consistent results.\n",
    "\n",
    "    here is the text:\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a851fda6-9129-492d-9e6f-42280b87413c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56d6382-b492-4b2e-80ca-365cec50c23e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebb4a66-b13d-4547-89a4-4f3253ca7c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a11ab93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import openai\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "from scrapifurs import utils\n",
    "from scrapifurs.GPTinstructions import GPTinstructions\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from scipy.cluster.hierarchy import linkage, leaves_list\n",
    "\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8c0bb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lat_lon(locations):\n",
    "    geolocator = Nominatim(user_agent=\"example.lastname@gmail.com\")\n",
    "    lat_lon_data = {}\n",
    "    for location in tqdm(locations):\n",
    "        location_data = geolocator.geocode(location)\n",
    "        if location_data is not None:\n",
    "            lat_lon_data[location] = (location_data.latitude, location_data.longitude)\n",
    "        else:\n",
    "            lat_lon_data[location] = (None, None)\n",
    "    return lat_lon_data\n",
    "\n",
    "def calculate_distances(lat_lon_data, target_loc_str):\n",
    "    target_location = get_lat_lon([target_loc_str])[target_loc_str]\n",
    "    if target_location[0] is None:\n",
    "        raise ValueError(\"No latitude and longitude found for the provided location.\")\n",
    "    distances = {}\n",
    "    for location, loc_lat_lon in lat_lon_data.items():\n",
    "        if loc_lat_lon[0] is not None:\n",
    "            distance = geodesic(target_location, loc_lat_lon).miles\n",
    "            distances[location] = distance\n",
    "        else:\n",
    "            distances[location] = None\n",
    "    return distances\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def jaccard_similarity(str1, str2):\n",
    "    set1 = set(str1.split())\n",
    "    set2 = set(str2.split())\n",
    "    union_length = len(set1.union(set2))\n",
    "    if union_length == 0:\n",
    "        return np.nan\n",
    "    return len(set1.intersection(set2)) / union_length\n",
    "\n",
    "\n",
    "class GPT_StringDataCleaner:\n",
    "    def __init__(self, original_df, key_name, instructions, overwrite_cleaned_key = True, model=\"gpt-4\"):\n",
    "        self.original_df = original_df\n",
    "        self.key_name = key_name\n",
    "        self.instructions = instructions\n",
    "        self.model = model\n",
    "        self.clean_key = key_name + '_CLEANED_BY_GPT'\n",
    "        self.overwrite_cleaned_key = overwrite_cleaned_key\n",
    "\n",
    "    def send_to_gpt(self):\n",
    "        self.unique_names = sorted(self.original_df[self.key_name].unique())\n",
    "\n",
    "        messages = [{\"role\": \"system\", \"content\": self.instructions}]\n",
    "        messages.append({\"role\": \"user\", \"content\": str(self.unique_names)})\n",
    "        chat = openai.ChatCompletion.create(model=self.model, messages=messages)\n",
    "        self.reply = chat.choices[0].message.content\n",
    "    def test_lengths(self):\n",
    "        print(f\" len input is {len(self.unique_names)}, but len output is {len(cleaned_data)}\")\n",
    "    def process_data(self):\n",
    "        cleaned_data =  eval(self.reply)\n",
    "        \n",
    "        if len(self.unique_names) != len(cleaned_data):\n",
    "            print(f\" len input is {len(self.unique_names)}, but len output is {len(cleaned_data)}\")\n",
    "            raise ValueError(\"Input and output lists must have the same length\")\n",
    "\n",
    "        temp_df = pd.DataFrame({self.key_name: self.unique_names, self.clean_key: cleaned_data})\n",
    "        # delete cleaned key data if we want to overwrite\n",
    "        if self.clean_key in self.original_df.columns and self.overwrite_cleaned_key:\n",
    "            self.original_df = self.original_df.drop(self.clean_key, axis=1)\n",
    "        if self.clean_key not in self.original_df.columns:\n",
    "            merged_df = pd.merge(self.original_df, temp_df[[self.clean_key, self.key_name]],\n",
    "                                 on=self.key_name, how='left')\n",
    "            self.original_df[self.clean_key] = merged_df[self.clean_key]\n",
    "            merged_df = None\n",
    "\n",
    "        return self.original_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbf4af56-ba53-49e0-9bf6-84fd28ead7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def update_master_df(job_data_all, job_data):\n",
    "    # Reverse the order of job_data to check oldest entries first\n",
    "    job_data_reversed = job_data.iloc[::-1]\n",
    "    \n",
    "    # List to hold new entries\n",
    "    new_entries = []\n",
    "    \n",
    "    for i, row in job_data_reversed.iterrows():\n",
    "        # Check if row exists in job_data_all\n",
    "        is_exist = job_data_all[(job_data_all['job_title'] == row['job_title']) & \n",
    "                                (job_data_all['company_name'] == row['company_name']) &\n",
    "                                (job_data_all['location'] == row['location'])].shape[0]\n",
    "        \n",
    "        # If row does not exist in job_data_all, add it to new_entries\n",
    "        if is_exist == 0:\n",
    "            print('NEWWWWWW')\n",
    "            print(row)\n",
    "            new_entries.append(row)\n",
    "    \n",
    "    # Convert new_entries to DataFrame and concatenate it with job_data_all\n",
    "    if new_entries:\n",
    "        new_entries_df = pd.DataFrame(new_entries)\n",
    "        job_data_all = pd.concat([new_entries_df, job_data_all], ignore_index=True)\n",
    "    \n",
    "    return job_data_all\n",
    "\n",
    "\n",
    "def get_job_blocks(gpt_text, job_data):\n",
    "    job_blocks = []\n",
    "    combined_titles = \"|\".join([re.escape(title) for title in job_data['job_title']])\n",
    "    combined_companies = \"|\".join([re.escape(company) for company in job_data['company_name']])\n",
    "    pattern = f\"({combined_titles}).*?({combined_companies}).*?(?=({combined_titles}).*?({combined_companies})|$)\"\n",
    "    \n",
    "    for match in re.finditer(pattern, gpt_text, re.DOTALL):\n",
    "        job_blocks.append(match.group(0))\n",
    "\n",
    "    return job_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a482f315-cd3f-465d-b35e-867b73505133",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "class JobListings:\n",
    "    def __init__(self, driver):\n",
    "        self.driver = driver\n",
    "        self.job_titles = []\n",
    "        self.current_job_index = -1  # Start before the first index\n",
    "    \n",
    "    def load_job_titles(self):\n",
    "        \"\"\"Loads all job titles into the list.\"\"\"\n",
    "        try:\n",
    "            # XPath to match all job title anchor tags\n",
    "            job_title_elements_xpath = \"//a[contains(@class,'job-card-list__title')]\"\n",
    "            # Wait for the job titles to be present to ensure the page has loaded\n",
    "            WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_all_elements_located((By.XPATH, job_title_elements_xpath))\n",
    "            )\n",
    "            # Find all job title elements\n",
    "            self.job_titles = self.driver.find_elements(By.XPATH, job_title_elements_xpath)\n",
    "        except TimeoutException:\n",
    "            print(\"Timed out waiting for job titles to appear.\")\n",
    "            self.job_titles = []\n",
    "    \n",
    "    def get_job_count(self):\n",
    "        \"\"\"Returns the number of jobs loaded.\"\"\"\n",
    "        return len(self.job_titles)\n",
    "    \n",
    "    def click_next_job(self):\n",
    "        \"\"\"Clicks the next job title in the list.\"\"\"\n",
    "        self.current_job_index += 1\n",
    "        if self.current_job_index < len(self.job_titles):\n",
    "            job_title_element = self.job_titles[self.current_job_index]\n",
    "            self.driver.execute_script(\"arguments[0].scrollIntoView(true);\", job_title_element)\n",
    "            job_title_element.click()\n",
    "            # You can add a wait here for the job details to load if necessary\n",
    "        else:\n",
    "            print(\"No more jobs to click.\")\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def get_about_job_info(self):\n",
    "        \"\"\"Extracts the 'About the job' information from the job details page.\"\"\"\n",
    "        try:\n",
    "            # Assuming the job details have been loaded at this point\n",
    "            about_job_xpath = \"//h2[text()='About the job']/following-sibling::span\"\n",
    "            about_job_element = WebDriverWait(self.driver, 10).until(\n",
    "                EC.visibility_of_element_located((By.XPATH, about_job_xpath))\n",
    "            )\n",
    "            return about_job_element.text\n",
    "        except TimeoutException:\n",
    "            print(\"Timed out waiting for the 'About the job' section to appear.\")\n",
    "            return None\n",
    "    def click_job_by_index(self, index):\n",
    "        \"\"\"Clicks a job title in the list based on its index.\"\"\"\n",
    "        if index < len(self.job_titles):\n",
    "            job_title_element = self.job_titles[index]\n",
    "            self.driver.execute_script(\"arguments[0].scrollIntoView(true);\", job_title_element)\n",
    "            job_title_element.click()\n",
    "            # Add a wait here for the job details to load if necessary\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Index {index} is out of range for the job titles list.\")\n",
    "            return False\n",
    "            \n",
    "def open_browser(info_dict):\n",
    "    #init chrome \n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    \n",
    "    driver.get(info_dict['init_url'])\n",
    "    time.sleep(1)\n",
    "    driver.get(info_dict['init_url'])\n",
    "    time.sleep(2)\n",
    "    \n",
    "    \n",
    "    # Load cookies if they exist\n",
    "    try:\n",
    "        cookies = pickle.load(open(info_dict['full_cookies_save_path'], \"rb\"))\n",
    "        for cookie in cookies:\n",
    "            driver.add_cookie(cookie)\n",
    "        driver.refresh()\n",
    "        assert(not not cookies)# if empty try a different method\n",
    "    except:\n",
    "        print(\"No cookies found. Manual login required.\")\n",
    "        # If not logged in\n",
    "        input('Please login and press Enter to continue...')\n",
    "        pickle.dump(driver.get_cookies(), open(info_dict['full_cookies_save_path'], \"wb\")) # save cookies after login\n",
    "    time.sleep(4)\n",
    "    return driver\n",
    "        \n",
    "    \n",
    "    time.sleep(4)\n",
    "def go_to_next_page(driver):\n",
    "    try:\n",
    "        # Step 1: Find the pagination element\n",
    "        pagination = driver.find_element(By.CSS_SELECTOR, 'ul.artdeco-pagination__pages')\n",
    "    except NoSuchElementException:\n",
    "        print(\"Pagination not found\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Step 2: Find the current active page\n",
    "        current_page_elem = pagination.find_element(By.CSS_SELECTOR, 'li.active')\n",
    "        current_page = int(current_page_elem.text)\n",
    "    except NoSuchElementException:\n",
    "        print(\"Current page not found\")\n",
    "        return\n",
    "\n",
    "    # Step 3: Find the next page\n",
    "    next_page = current_page + 1\n",
    "    next_page_selector = f'li[data-test-pagination-page-btn=\"{next_page}\"]'\n",
    "\n",
    "    try:\n",
    "        next_page_elem = pagination.find_element(By.CSS_SELECTOR, next_page_selector)\n",
    "    except NoSuchElementException:\n",
    "        print(\"Next page not found\")\n",
    "        return\n",
    "\n",
    "    # Step 4: Click the next page\n",
    "    next_page_elem.click()\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def update_dataframe(df_main, df_new, keys=None):\n",
    "    # Updated keys to include all from the job_dict\n",
    "    if keys is None:\n",
    "        keys = ['job_ids', 'is_promoted', 'job_title', 'company_name', 'Location', 'pay', \n",
    "                'job_link', 'time_added', 'about_the_job', '0_new__1_applied__2_skipped']\n",
    "\n",
    "    # Initialize df_main if it is None\n",
    "    if df_main is None:\n",
    "        df_main = pd.DataFrame(columns=keys)\n",
    "\n",
    "    # Check if types are the same for each key and correct them if needed\n",
    "    for key in keys:\n",
    "        if key in df_new and key in df_main:\n",
    "            # Ensure the data type of df_new is the same as that of df_main\n",
    "            df_new[key] = df_new[key].astype(df_main[key].dtype)\n",
    "        else:\n",
    "            # If the key does not exist in df_main, add it\n",
    "            df_main[key] = pd.Series(dtype=df_new[key].dtype)\n",
    "\n",
    "    # Merge new rows into the main dataframe\n",
    "    for _, new_row in df_new.iterrows():\n",
    "        mask = (df_main[keys].to_numpy() == new_row[keys].to_numpy()).all(axis=1)\n",
    "        if not any(mask):\n",
    "            df_main = pd.concat([df_main, pd.DataFrame([new_row])], ignore_index=True)\n",
    "            os.system('say \"youve got jobs\"')  # macOS specific command\n",
    "            print('\\nNEW JOB')\n",
    "            print(new_row['job_link'])\n",
    "            print(new_row)\n",
    "\n",
    "    return df_main\n",
    "\n",
    "\n",
    "# def update_dataframe(df_main, df_new, keys=None):\n",
    "#     if keys is None:\n",
    "#         keys = ['job_ids', 'job_title', 'company_name', 'Location', 'pay']\n",
    "\n",
    "#     # Check if types are the same for each key\n",
    "#     mismatched_keys = []\n",
    "#     for key in keys:\n",
    "#         print(key)\n",
    "#         print(df_main)\n",
    "#         type_main = df_main[key].dtype\n",
    "#         type_new = df_new[key].dtype\n",
    "#         if type_main != type_new:\n",
    "#             mismatched_keys.append((key, type_main, type_new))\n",
    "\n",
    "#     if mismatched_keys:\n",
    "#         print(\"Type mismatch for these keys:\")\n",
    "#         for key, type_main, type_new in mismatched_keys:\n",
    "#             print(f\"Key: {key}, Type in df_main: {type_main}, Type in df_new: {type_new}\")\n",
    "#         raise TypeError(\"Type mismatch detected.\")\n",
    "        \n",
    "#     for _, new_row in df_new.iterrows():\n",
    "#         mask = df_main[keys].eq(new_row[keys]).all(axis=1)\n",
    "#         if not any(mask):\n",
    "#             df_main = pd.concat([df_main, pd.DataFrame([new_row])], ignore_index=True)\n",
    "#             os.system('say \"youve got jobs\"')  # macOS\n",
    "#             print('\\nNNNNNEEWWWWWWW')\n",
    "#             print(new_row['job_link'])\n",
    "#             print(new_row)\n",
    "\n",
    "#     return df_main\n",
    "\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "\n",
    "def grab_all_job_ids_in_folder(bd):\n",
    "    files = glob.glob(f'{bd}/*.xlsx')\n",
    "    all_ids = []\n",
    "    for fn in files:\n",
    "        data = data_file(fn)\n",
    "        all_ids.append(data.df_main['job_ids'])\n",
    "    return np.unique(np.concatenate(all_ids))\n",
    "\n",
    "\n",
    "\n",
    "def get_job_details(driver, existing_job_ids=[]):\n",
    "    job_dict = {\n",
    "        \"job_ids\": [],\n",
    "        \"is_promoted\": [],\n",
    "        \"job_title\": [],\n",
    "        \"company_name\": [],\n",
    "        \"Location\": [],\n",
    "        \"pay\": [],\n",
    "        \"job_link\": [],\n",
    "        \"time_added\": [],\n",
    "        \"about_the_job\": [],\n",
    "        \"0_new__1_applied__2_skipped\": [],\n",
    "    }\n",
    "    \n",
    "    # setup the \"about section\" grabbing class\n",
    "    job_listings = JobListings(driver)\n",
    "    job_listings.load_job_titles()\n",
    "\n",
    "\n",
    "    # get all the job data for processing\n",
    "    jobs = driver.find_elements(By.CSS_SELECTOR, '[data-occludable-job-id]')\n",
    "    about_the_job_list = []  # Assuming this is populated earlier in your code\n",
    "\n",
    "    for job_ind, job in enumerate(jobs):\n",
    "        try:\n",
    "            job_id = job.get_attribute(\"data-occludable-job-id\")\n",
    "            job_id = int(job_id)\n",
    "        except NoSuchElementException:\n",
    "            job_id = -1\n",
    "        print(job_id in existing_job_ids)\n",
    "        print(job_id)\n",
    "\n",
    "        # Skip the job if its ID is -1 or already exists in the existing_job_ids list or variable passed in of previous job IDs\n",
    "        if job_id == -1 or job_id in existing_job_ids or job_id in job_dict[\"job_ids\"]:\n",
    "            print(f'Job with ID {job_id} exists, ... skipping')\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            promoted_elements = job.find_elements(By.CSS_SELECTOR, '.job-card-container__footer-item')\n",
    "            is_promoted = \"Promoted\" in [e.text for e in promoted_elements]\n",
    "        except NoSuchElementException:\n",
    "            is_promoted = False\n",
    "\n",
    "        try:\n",
    "            job_title = job.find_element(By.CSS_SELECTOR, '.job-card-list__title').text\n",
    "        except NoSuchElementException:\n",
    "            job_title = \"\"\n",
    "\n",
    "        try:\n",
    "            company_name = job.find_element(By.CSS_SELECTOR, '.job-card-container__primary-description').text\n",
    "        except NoSuchElementException:\n",
    "            company_name = \"\"\n",
    "\n",
    "        try:\n",
    "            Location = job.find_element(By.CSS_SELECTOR, '.job-card-container__metadata-wrapper li').text\n",
    "        except NoSuchElementException:\n",
    "            Location = \"\"\n",
    "\n",
    "        try:\n",
    "            pay = job.find_element(By.CSS_SELECTOR, '.mt1 .job-card-container__metadata-wrapper li').text\n",
    "        except NoSuchElementException:\n",
    "            pay = \"\"\n",
    "            \n",
    "        try:\n",
    "            job_link = job.find_element(By.CSS_SELECTOR, \"a.job-card-container__link\").get_attribute(\"href\")\n",
    "        except NoSuchElementException:\n",
    "            job_link = \"\"\n",
    "\n",
    "        if job_title != \"\" and company_name != \"\":\n",
    "            job_dict[\"job_ids\"].append(job_id)\n",
    "            job_dict[\"is_promoted\"].append(is_promoted)\n",
    "            job_dict[\"job_title\"].append(job_title)\n",
    "            job_dict[\"company_name\"].append(company_name)\n",
    "            job_dict[\"Location\"].append(Location)\n",
    "            job_dict[\"pay\"].append(pay)\n",
    "            job_dict[\"job_link\"].append(job_link)\n",
    "            \n",
    "            current_time = datetime.now()\n",
    "            formatted_time = current_time.strftime('%Y%m%d%H%M%S')\n",
    "            formatted_time_as_int = int(formatted_time)\n",
    "            job_dict[\"time_added\"].append(formatted_time_as_int)\n",
    "\n",
    "            \n",
    "            if job_listings.click_job_by_index(job_ind): #$%^ validate clicks only new ones\n",
    "                job_dict[\"about_the_job\"].append(job_listings.get_about_job_info())\n",
    "                time.sleep(np.random.uniform(0.5, 1.5)) # if its too fast i think linked in will flag \n",
    "            else:\n",
    "                job_dict[\"about_the_job\"].append('NA')\n",
    "\n",
    "            \n",
    "            # job_dict[\"about_the_job\"].append(about_the_job)\n",
    "            job_dict[\"0_new__1_applied__2_skipped\"].append(-1)\n",
    "    \n",
    "        \n",
    "    return job_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class data_file():\n",
    "\n",
    "    def __init__(self, fn, include_date=True):\n",
    "        add_date_str = datetime.now().strftime('%Y_%m_%d') if include_date else ''\n",
    "        self.fn = fn\n",
    "        self.ft = fn.split('.')[-1].lower()\n",
    "        self.exists = self.check_if_exists()\n",
    "        self.df_main = None\n",
    "        self.load_it()\n",
    "        \n",
    "    \n",
    "    def check_if_exists(self):\n",
    "        return os.path.exists(self.fn)\n",
    "    \n",
    "    def load_it(self):\n",
    "        try:\n",
    "            self.df_main = pd.read_excel(self.fn) \n",
    "        except:\n",
    "            try:\n",
    "                self.df_main = pd.read_csv(self.fn)\n",
    "            except:\n",
    "                print(f\"File type for {self.fn} is unsupported\")\n",
    "        \n",
    "    def save_it(self):\n",
    "        # larger job IDs are always more recent. \n",
    "        # self.df_main = self.df_main.sort_values(by='job_ids', ascending=False) \n",
    "        self.df_main = self.df_main.sort_values(by='job_ids', ascending=False).reset_index(drop=True)\n",
    "        \n",
    "\n",
    "        try:\n",
    "            self.df_main.to_excel(self.fn, index=False)\n",
    "        except:\n",
    "            try: \n",
    "                self.df_main.to_csv(self.fn, index=False)  \n",
    "            except:\n",
    "                print(f\"Error saving {self.fn}\")\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming DataClass is defined elsewhere and has methods df_main, save_it()\n",
    "\n",
    "grab_all_job_ids_in_folder\n",
    "def scrape_job_data(info_dict, data_class, n_pages_to_scrape=25, wait_sec_each_page=5, update_every_n_secs=360, existing_job_ids=[]):\n",
    "    \"\"\"\n",
    "    Scrapes job data for a specified number of pages and intervals.\n",
    "\n",
    "    Args:\n",
    "    - info_dict: A dictionary containing start_url and other necessary information.\n",
    "    - data_class: An instance of a DataClass containing df_main and a save_it method.\n",
    "    - n_pages_to_scrape: Number of pages to scrape.\n",
    "    - wait_sec_each_page: Time to wait on each page before scraping.\n",
    "    - update_every_n_secs: How often to update the data in seconds.\n",
    "    \"\"\"\n",
    "\n",
    "    driver = open_browser(info_dict)\n",
    "    driver.get(info_dict['start_url'])\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            for n_page in range(n_pages_to_scrape):\n",
    "                if n_page == 0:\n",
    "                    # Go back to the first page\n",
    "                    driver.get(info_dict['start_url'])\n",
    "                else:\n",
    "                    go_to_next_page(driver)\n",
    "                time.sleep(wait_sec_each_page)\n",
    "                # Update data\n",
    "                job_data = get_job_details(driver, existing_job_ids)\n",
    "                job_data = pd.DataFrame(job_data)\n",
    "                job_data['job_ids'] = job_data['job_ids'].astype('int')\n",
    "                data_class.df_main = update_dataframe(data_class.df_main, job_data, ['job_ids'])\n",
    "            \n",
    "                data_class.save_it()\n",
    "            \n",
    "            print('_____________________________________________\\n\\n____________________')\n",
    "            time.sleep(update_every_n_secs)\n",
    "    finally:\n",
    "        driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d676d81c",
   "metadata": {},
   "source": [
    "### INIT API key,  intrucitons for GPT which can be saved as text files in the correct data/instrucitons dir of the package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75257fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # setup API key for chatGPT \n",
    "# load_dotenv()  # take environment variables from .env.\n",
    "# os.environ[\"OPENAI_API_KEY\"]  = os.getenv(\"OPENAI_API_KEY\")\n",
    "# openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2700525-eb14-4581-8a73-e0b219c6bef1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e95ee35f",
   "metadata": {},
   "source": [
    "#### navigate to the first page we want to scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76c2195",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b5fdf4-6d62-407b-b8b3-5e4b36922901",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd803c7f-8925-4a9b-b952-d1441cc3944f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8f98b7-e29d-41bf-884d-ff741c8bcdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_data = get_job_details(driver)\n",
    "# job_data = pd.DataFrame(job_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb7a091-743b-4e0d-b965-885451cdfc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdeb2ff-d7c4-4d89-b584-d410529bfd20",
   "metadata": {},
   "source": [
    "# Info_dict to set up all used variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "379f097d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup basic variable as dict \n",
    "info_dict = {'init_url':'https://www.linkedin.com/',\n",
    "             'save_password_dir':'/Users/phil/Dropbox/GITHUB/DATA/scrapifurs/saved_cookies/',\n",
    "             'start_url':'https://www.linkedin.com/search/results/people/?keywords=data%20scientist&origin=CLUSTER_EXPANSION&sid=fRq'}\n",
    "info_dict['full_cookies_save_path'] = info_dict['save_password_dir']+os.sep+\"linkedin_cookies.pkl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac45c456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76c4a57-4226-4bcf-9909-f1b12d4e2b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for the recomended jobs file\n",
    "\n",
    "\n",
    "# x = 'https://www.linkedin.com/jobs/collections/recommended/?currentJobId=3758777094'\n",
    "# x = 'https://www.linkedin.com/jobs/collections/recommended/?currentJobId=3759374185&origin=JOB_ALERT_IN_APP_NOTIFICATION&originToLandingJobPostings=3752871491%2C3759374185'\n",
    "# info_dict['start_url'] = x\n",
    "# fn = '/Users/phil/Library/CloudStorage/Dropbox/GITHUB/scrapifurs/scrapifurs/data/tempdata/recommended_jobs.xlsx'\n",
    "# data_class = data_file(fn)     \n",
    "# scrape_job_data(info_dict, data_class, n_pages_to_scrape=25, wait_sec_each_page=5, update_every_n_secs=60*15)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bdbda4-9081-4a7c-867c-6f9bb65bdeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a85e73f-b1b8-4cca-8551-974ee525e163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for the job search \n",
    "\n",
    "# x = 'https://www.linkedin.com/jobs/search/?currentJobId=3742729997&distance=25&f_PP=102277331%2C106233382%2C102448103%2C103918656%2C102250832%2C103575230%2C100075706&f_T=25206%2C340%2C25190%2C25887%2C30209%2C288%2C2463%2C25584&f_WT=1&geoId=103644278&keywords=data%20scientist&origin=JOBS_HOME_SEARCH_CARDS'\n",
    "# info_dict['start_url'] = x\n",
    "# fn = '/Users/phil/Library/CloudStorage/Dropbox/GITHUB/scrapifurs/scrapifurs/data/tempdata/DS_mega_data_V1.xlsx'\n",
    "# data_class = data_file(fn)     \n",
    "# scrape_job_data(info_dict, data_class, n_pages_to_scrape=9, wait_sec_each_page=5, update_every_n_secs=60*15)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac8de62-8608-4d98-9553-a9fab36848a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "# # class data_file\n",
    "\n",
    "# files = glob.glob('/Users/phil/Library/CloudStorage/Dropbox/GITHUB/scrapifurs/scrapifurs/data/data_by_date/*.xlsx')\n",
    "# for fn in files:\n",
    "#     data_class = data_file(fn)\n",
    "# data_class.df_main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "274945a4-0e05-4653-8c2b-43fe7cc9034f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=119.0.6045.123)\nStacktrace:\n0   chromedriver                        0x0000000101112004 chromedriver + 4169732\n1   chromedriver                        0x0000000101109ff8 chromedriver + 4136952\n2   chromedriver                        0x0000000100d5f500 chromedriver + 292096\n3   chromedriver                        0x0000000100d382d8 chromedriver + 131800\n4   chromedriver                        0x0000000100dcbf68 chromedriver + 737128\n5   chromedriver                        0x0000000100ddefac chromedriver + 815020\n6   chromedriver                        0x0000000100d985e8 chromedriver + 525800\n7   chromedriver                        0x0000000100d994b8 chromedriver + 529592\n8   chromedriver                        0x00000001010d8334 chromedriver + 3932980\n9   chromedriver                        0x00000001010dc970 chromedriver + 3950960\n10  chromedriver                        0x00000001010c0774 chromedriver + 3835764\n11  chromedriver                        0x00000001010dd478 chromedriver + 3953784\n12  chromedriver                        0x00000001010b2ab4 chromedriver + 3779252\n13  chromedriver                        0x00000001010f9914 chromedriver + 4069652\n14  chromedriver                        0x00000001010f9a90 chromedriver + 4070032\n15  chromedriver                        0x0000000101109c70 chromedriver + 4136048\n16  libsystem_pthread.dylib             0x000000018a71ffa8 _pthread_start + 148\n17  libsystem_pthread.dylib             0x000000018a71ada0 thread_start + 8\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m data_class \u001b[38;5;241m=\u001b[39m DataFile(fn, include_date\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m existing_job_ids \u001b[38;5;241m=\u001b[39m grab_all_job_ids_in_folder(bd)\n\u001b[0;32m---> 17\u001b[0m \u001b[43mscrape_job_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfo_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_pages_to_scrape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_sec_each_page\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_every_n_secs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexisting_job_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexisting_job_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03mfor each file in the the data by date folder \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m \n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 401\u001b[0m, in \u001b[0;36mscrape_job_data\u001b[0;34m(info_dict, data_class, n_pages_to_scrape, wait_sec_each_page, update_every_n_secs, existing_job_ids)\u001b[0m\n\u001b[1;32m    399\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(wait_sec_each_page)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;66;03m# Update data\u001b[39;00m\n\u001b[0;32m--> 401\u001b[0m job_data \u001b[38;5;241m=\u001b[39m \u001b[43mget_job_details\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexisting_job_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    402\u001b[0m job_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(job_data)\n\u001b[1;32m    403\u001b[0m job_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjob_ids\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m job_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjob_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 265\u001b[0m, in \u001b[0;36mget_job_details\u001b[0;34m(driver, existing_job_ids)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;66;03m#         # ... rest of your code for extracting job details    \u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;66;03m# #$%^\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;66;03m#     selected_job_indices = []\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m#         except NoSuchElementException:\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m#             job_id = -1\u001b[39;00m\n\u001b[1;32m    264\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 265\u001b[0m             promoted_elements \u001b[38;5;241m=\u001b[39m \u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_elements\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCSS_SELECTOR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.job-card-container__footer-item\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m             is_promoted \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPromoted\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m [e\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m promoted_elements]\n\u001b[1;32m    267\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m NoSuchElementException:\n",
      "File \u001b[0;32m~/Library/CloudStorage/Dropbox/GITHUB/scrapifurs/.venv_M1/lib/python3.11/site-packages/selenium/webdriver/remote/webelement.py:439\u001b[0m, in \u001b[0;36mWebElement.find_elements\u001b[0;34m(self, by, value)\u001b[0m\n\u001b[1;32m    436\u001b[0m     by \u001b[38;5;241m=\u001b[39m By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR\n\u001b[1;32m    437\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[name=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 439\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFIND_CHILD_ELEMENTS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43musing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Library/CloudStorage/Dropbox/GITHUB/scrapifurs/.venv_M1/lib/python3.11/site-packages/selenium/webdriver/remote/webelement.py:395\u001b[0m, in \u001b[0;36mWebElement._execute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    393\u001b[0m     params \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    394\u001b[0m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id\n\u001b[0;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/CloudStorage/Dropbox/GITHUB/scrapifurs/.venv_M1/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py:345\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    343\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m--> 345\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Library/CloudStorage/Dropbox/GITHUB/scrapifurs/.venv_M1/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[0;31mNoSuchWindowException\u001b[0m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=119.0.6045.123)\nStacktrace:\n0   chromedriver                        0x0000000101112004 chromedriver + 4169732\n1   chromedriver                        0x0000000101109ff8 chromedriver + 4136952\n2   chromedriver                        0x0000000100d5f500 chromedriver + 292096\n3   chromedriver                        0x0000000100d382d8 chromedriver + 131800\n4   chromedriver                        0x0000000100dcbf68 chromedriver + 737128\n5   chromedriver                        0x0000000100ddefac chromedriver + 815020\n6   chromedriver                        0x0000000100d985e8 chromedriver + 525800\n7   chromedriver                        0x0000000100d994b8 chromedriver + 529592\n8   chromedriver                        0x00000001010d8334 chromedriver + 3932980\n9   chromedriver                        0x00000001010dc970 chromedriver + 3950960\n10  chromedriver                        0x00000001010c0774 chromedriver + 3835764\n11  chromedriver                        0x00000001010dd478 chromedriver + 3953784\n12  chromedriver                        0x00000001010b2ab4 chromedriver + 3779252\n13  chromedriver                        0x00000001010f9914 chromedriver + 4069652\n14  chromedriver                        0x00000001010f9a90 chromedriver + 4070032\n15  chromedriver                        0x0000000101109c70 chromedriver + 4136048\n16  libsystem_pthread.dylib             0x000000018a71ffa8 _pthread_start + 148\n17  libsystem_pthread.dylib             0x000000018a71ada0 thread_start + 8\n"
     ]
    }
   ],
   "source": [
    "# testing this version\n",
    "\n",
    "x = 'https://www.linkedin.com/jobs/search/?currentJobId=3742729997&distance=25&f_PP=102277331%2C106233382%2C102448103%2C103918656%2C102250832%2C103575230%2C100075706&f_T=25206%2C340%2C25190%2C25887%2C30209%2C288%2C2463%2C25584&f_WT=1&geoId=103644278&keywords=data%20scientist&origin=JOBS_HOME_SEARCH_CARDS'\n",
    "\n",
    "# x = 'https://www.linkedin.com/jobs/search/?currentJobId=3759816746&distance=25&f_E=2%2C3&f_WT=1%2C2%2C3&geoId=102095887&keywords=data%20scientist&origin=JOBS_HOME_SEARCH_CARDS'\n",
    "\n",
    "\n",
    "info_dict['start_url'] = x\n",
    "\n",
    "bd = '/Users/phil/Library/CloudStorage/Dropbox/GITHUB/scrapifurs/scrapifurs/data/data_by_date/'\n",
    "fn = bd+'search_one_data2.xlsx'\n",
    "\n",
    "data_class = DataFile(fn, include_date=True)\n",
    "\n",
    "\n",
    "existing_job_ids = grab_all_job_ids_in_folder(bd)\n",
    "scrape_job_data(info_dict, data_class, n_pages_to_scrape=1, wait_sec_each_page=2, update_every_n_secs=60*15, existing_job_ids=existing_job_ids)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "for each file in the the data by date folder \n",
    "find all the instance where the key \"0_new__1_applied__2_skipped\" is equal to 1, then update file we input as \"fn_applied\" \n",
    "then do it again for where the key \"0_new__1_applied__2_skipped\" is equal to 2, then update file we input as \"fn_skipped\" \n",
    "\n",
    "check against master file and add in where needed \n",
    "\n",
    "update function for data I am looing in \n",
    "simply load master file and change all the values to red or have a seperate column \n",
    "\n",
    "\n",
    "ok that all works so now I can \n",
    "1) add in the quick apply option or dat ain that section to see if i should make a quick move \n",
    "2) set default sheet name to year month day format and then allow adding an addition term\n",
    "\n",
    "next \n",
    "- clickable links \n",
    "- formatting \n",
    "- having it update with my current jobs \n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c38d0a-5116-45ba-aff0-0b4bba1a2e8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3dfcaf0-9ec6-4658-b8f2-5fb7c2008f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #testing !!!!\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# from openpyxl import load_workbook\n",
    "# from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "# from openpyxl import Workbook\n",
    "\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# from openpyxl import load_workbook\n",
    "# from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "\n",
    "# class DataFile:\n",
    "\n",
    "#     def __init__(self, filename, sheet_name='Sheet1'):\n",
    "#         self.filename = filename\n",
    "#         self.sheet_name = sheet_name\n",
    "#         self.df_main = None\n",
    "#         self.load_it()\n",
    "\n",
    "#     def load_it(self):\n",
    "#         if self.filename.endswith('.xlsx') and os.path.exists(self.filename):\n",
    "#             self.df_main = pd.read_excel(self.filename, sheet_name=self.sheet_name)\n",
    "#         elif self.filename.endswith('.csv') and os.path.exists(self.filename):\n",
    "#             self.df_main = pd.read_csv(self.filename)\n",
    "#         else:\n",
    "#             print(f\"File type for {self.filename} is unsupported or file does not exist.\")\n",
    "\n",
    "#     def save_it(self):\n",
    "#         self.df_main = self.df_main.sort_values(by='job_ids', ascending=False).reset_index(drop=True)\n",
    "#         if self.filename.endswith('.xlsx'):\n",
    "#             self.save_to_excel()\n",
    "#         elif self.filename.endswith('.csv'):\n",
    "#             self.df_main.to_csv(self.filename, index=False)\n",
    "#         else:\n",
    "#             print(f\"Error: Unsupported file type for {self.filename}\")\n",
    "    \n",
    "\n",
    "#     def save_to_excel(self):\n",
    "#         try:\n",
    "#             # Load the workbook or create it if it doesn't exist\n",
    "#             if os.path.exists(self.filename):\n",
    "#                 wb = load_workbook(self.filename)\n",
    "#             else:\n",
    "#                 wb = Workbook()\n",
    "#                 wb.save(self.filename)\n",
    "#                 wb = load_workbook(self.filename)\n",
    "\n",
    "#             # Ensure the workbook has the sheet\n",
    "#             if self.sheet_name not in wb.sheetnames:\n",
    "#                 wb.create_sheet(self.sheet_name)\n",
    "#             sheet = wb[self.sheet_name]\n",
    "\n",
    "#             # Load existing data into a DataFrame\n",
    "#             if sheet.max_row > 1:  # Assuming header is present\n",
    "#                 existing_data = pd.DataFrame(sheet.values)\n",
    "#                 headers = existing_data.iloc[0]  # Take the first row for the header\n",
    "#                 existing_data = existing_data[1:]  # Take the data less the header row\n",
    "#                 existing_data.columns = headers  # Set the header row as the df header\n",
    "#                 combined_data = pd.concat([self.df_main, existing_data])\n",
    "#             else:\n",
    "#                 combined_data = self.df_main\n",
    "\n",
    "#             # Sort combined data by 'job_ids' in descending order\n",
    "#             combined_data = combined_data.sort_values(by='job_ids', ascending=False).reset_index(drop=True)\n",
    "\n",
    "#             # Clear the sheet\n",
    "#             for row in sheet.iter_rows(min_row=2, max_col=sheet.max_column, max_row=sheet.max_row):\n",
    "#                 for cell in row:\n",
    "#                     cell.value = None\n",
    "\n",
    "#             # Write the sorted DataFrame to the Excel file\n",
    "#             for r_idx, row in enumerate(dataframe_to_rows(combined_data, index=False, header=True), start=1):\n",
    "#                 for c_idx, value in enumerate(row, start=1):\n",
    "#                     sheet.cell(row=r_idx, column=c_idx, value=value)\n",
    "\n",
    "#             # Save the workbook\n",
    "#             wb.save(self.filename)\n",
    "        \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error saving {self.filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38e0b758-03d7-49db-aa2a-a57167e736b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # working!!!!!\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# from openpyxl import load_workbook\n",
    "# from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "\n",
    "# class DataFile:\n",
    "\n",
    "#     def __init__(self, filename, sheet_name='Sheet1'):\n",
    "#         self.filename = filename\n",
    "#         self.sheet_name = sheet_name\n",
    "#         self.df_main = None\n",
    "#         self.load_it()\n",
    "\n",
    "#     def load_it(self):\n",
    "#         if self.filename.endswith('.xlsx') and os.path.exists(self.filename):\n",
    "#             self.df_main = pd.read_excel(self.filename, sheet_name=self.sheet_name)\n",
    "#         elif self.filename.endswith('.csv') and os.path.exists(self.filename):\n",
    "#             self.df_main = pd.read_csv(self.filename)\n",
    "#         else:\n",
    "#             print(f\"File type for {self.filename} is unsupported or file does not exist.\")\n",
    "\n",
    "#     def save_it(self):\n",
    "#         self.df_main = self.df_main.sort_values(by='job_ids', ascending=False).reset_index(drop=True)\n",
    "#         if self.filename.endswith('.xlsx'):\n",
    "#             self.save_to_excel()\n",
    "#         elif self.filename.endswith('.csv'):\n",
    "#             self.df_main.to_csv(self.filename, index=False)\n",
    "#         else:\n",
    "#             print(f\"Error: Unsupported file type for {self.filename}\")\n",
    "    \n",
    "#     def save_to_excel(self):\n",
    "#         try:\n",
    "#             # Load the workbook or create it if it doesn't exist\n",
    "#             if os.path.exists(self.filename):\n",
    "#                 wb = load_workbook(self.filename)\n",
    "#             else:\n",
    "#                 wb = Workbook()\n",
    "#                 wb.save(self.filename)\n",
    "#                 wb = load_workbook(self.filename)\n",
    "\n",
    "#             # Ensure the workbook has the sheet\n",
    "#             if self.sheet_name not in wb.sheetnames:\n",
    "#                 wb.create_sheet(self.sheet_name)\n",
    "            \n",
    "#             sheet = wb[self.sheet_name]\n",
    "\n",
    "#             # Find the first empty row to start writing data to\n",
    "#             empty_row = max((c.row for c in sheet['A'] if c.value is not None), default=0) + 1\n",
    "\n",
    "#             # Write the data from the DataFrame to the sheet, starting from the first empty row\n",
    "#             for r_idx, row in enumerate(dataframe_to_rows(self.df_main, index=False, header=False), start=empty_row):\n",
    "#                 for c_idx, value in enumerate(row, start=1):\n",
    "#                     sheet.cell(row=r_idx, column=c_idx, value=value)\n",
    "\n",
    "#             # Save the workbook\n",
    "#             wb.save(self.filename)\n",
    "        \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error saving {self.filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "17c0ebb1-c013-406d-8e0e-3ac55b01fd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# from openpyxl import load_workbook\n",
    "\n",
    "# class DataFile:\n",
    "\n",
    "#     def __init__(self, filename, sheet_name='Sheet1'):\n",
    "#         self.filename = filename\n",
    "#         self.sheet_name = sheet_name\n",
    "#         self.df_main = None\n",
    "#         self.load_it()\n",
    "\n",
    "#     def load_it(self):\n",
    "#         if self.filename.endswith('.xlsx') and os.path.exists(self.filename):\n",
    "#             self.df_main = pd.read_excel(self.filename, sheet_name=self.sheet_name)\n",
    "#         elif self.filename.endswith('.csv') and os.path.exists(self.filename):\n",
    "#             self.df_main = pd.read_csv(self.filename)\n",
    "#         else:\n",
    "#             print(f\"File type for {self.filename} is unsupported or file does not exist.\")\n",
    "\n",
    "#     def save_it(self):\n",
    "#         self.df_main = self.df_main.sort_values(by='job_ids', ascending=False).reset_index(drop=True)\n",
    "#         if self.filename.endswith('.xlsx'):\n",
    "#             self.save_to_excel()\n",
    "#         elif self.filename.endswith('.csv'):\n",
    "#             self.df_main.to_csv(self.filename, index=False)\n",
    "#         else:\n",
    "#             print(f\"Error: Unsupported file type for {self.filename}\")\n",
    "    \n",
    "\n",
    "#     def save_to_excel(self):\n",
    "#         try:\n",
    "#             # Load the workbook, and then get the sheet, or create it if it doesn't exist\n",
    "#             if os.path.exists(self.filename):\n",
    "#                 book = load_workbook(self.filename)\n",
    "#                 writer = pd.ExcelWriter(self.filename, engine='openpyxl')\n",
    "#                 writer.book = book\n",
    "#                 writer.sheets = {ws.title: ws for ws in book.worksheets}\n",
    "#             else:\n",
    "#                 writer = pd.ExcelWriter(self.filename, engine='openpyxl')\n",
    "\n",
    "#             # Write data to the specified sheet, preserving existing sheets and their data\n",
    "#             self.df_main.to_excel(writer, self.sheet_name, index=False)\n",
    "\n",
    "#             # Save the workbook\n",
    "#             writer.close()\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error saving {self.filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd93a60f-3662-4f8c-99fe-d17fff7930b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c4a8cf-28b7-4e4d-9e85-c449396afa97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ce6a79-cbc1-40b6-8eba-4c31e67fe247",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8477d6f-429e-46de-a6e0-8d6f26854736",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfecb31-0e58-4961-919f-d0fe0acdfdd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8a07c4-7e2d-443e-b693-389d5e899a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbc4db1-7594-47af-8c1d-3fd2d51b3f3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d19b6ef-a32c-4396-aeeb-103a1fc438e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ccded2-2bc3-485a-912b-13fb7637a0d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa56ce0-d21c-4e55-8d92-8a83a9c9f70e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f63ee17f-7d80-4a99-8b49-20c04f307cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    \"$184K/yr - $338K/yr ¬∑ Medical, 401(k), +1 benefit\",\n",
    "    # More data...\n",
    "]\n",
    "\n",
    "# Function to extract salary information\n",
    "def extract_salary_info(text):\n",
    "    # Find all patterns of money amounts followed by either \"hr\" or \"yr\"\n",
    "    patterns = re.findall(r'\\$(\\S+)(?= |hr|yr)', text)\n",
    "    time_basis = re.findall(r'per (hr|yr)', text.lower())\n",
    "\n",
    "    # Convert to float and scale numbers ending in 'K'\n",
    "    amounts = [float(amount.rstrip('K')) * 1000 if 'K' in amount else float(amount) for amount in patterns]\n",
    "    # Determine min and max salaries\n",
    "    min_salary = min(amounts) if amounts else None\n",
    "    max_salary = max(amounts) if amounts else None\n",
    "    # Determine time basis ('hr' or 'yr'), default to None if not found\n",
    "    per_time = time_basis[0] if time_basis else None\n",
    "\n",
    "    return min_salary, max_salary, per_time\n",
    "\n",
    "# Apply the function to each item in the data list and create a DataFrame\n",
    "extracted_data = [extract_salary_info(item) for item in data]\n",
    "df = pd.DataFrame(extracted_data, columns=['Min_Salary', 'Max_Salary', 'Per'])\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d436dfa6-2a88-4919-99b9-d9622d76edd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4450a4-11f2-4298-9679-a3f4bd786991",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196ad515-623e-4c24-8527-026763445b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f489a846-9e97-4775-b16e-c0351d4073ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bb925a-c627-42ad-b1d7-2a2fc324b208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2bd0e70-be20-4eb7-aca6-6c70eb9a6e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = open_browser(info_dict)\n",
    "driver.get('https://www.linkedin.com/jobs/search/?currentJobId=3627822818&distance=25&f_PP=102277331%2C106233382%2C102448103%2C103918656%2C102250832%2C103575230%2C100075706&f_T=25206%2C340%2C25190%2C25887%2C30209%2C288%2C2463%2C25584&f_WT=1&geoId=103644278&keywords=data%20scientist&origin=JOBS_HOME_SEARCH_CARDS')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fffd22ca-2614-4c62-9ee9-aafc06150e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more jobs to click.\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "# driver = your WebDriver instance\n",
    "job_listings = JobListings(driver)\n",
    "job_listings.load_job_titles()\n",
    "about_job_info = []\n",
    "while job_listings.click_next_job():\n",
    "    about_job_info.append(job_listings.get_about_job_info())\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce47486c-7336-48de-8717-a1ed084e818d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(about_job_info)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "29f64628-9297-42f8-a914-a0f407012112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Founded by Larry Ellison and David B. Agus, MD, the Ellison Institute of Technology (EIT) works to develop and deploy advanced technology in pursuit of solving some of humanity’s most challenging and enduring problems. Guided by world leaders, scientists, and entrepreneurs, EIT seeks to accelerate innovation by driving scientific and technological advancements across four humane endeavors: medical sciences and healthcare, food security and sustainable agriculture, clean energy and climate change, and government policy and economics.\\n\\nPlease visit eit.org for more details.\\n\\nJob Summary\\nThe Ellison Institute of Technology is seeking a talented and passionate Research Scientist I, Metabolomics Data to join its team. As part of EIT, The Research Scientist I, Metabolomics Data is primarily responsible for the interpretation and analysis of mass spectrometry data obtained from metabolomics experiments. This individual must have a deep understanding of metabolomics experiments, mass spectrometry, and various analytical techniques. Special focus will be given to the ability to register features across batches and datasets using a combination of MS2, ion mobility, and other techniques.\\n\\nThe Research Scientist I, Metabolomics Data is responsible for performing the following job duties:\\n\\nJob Accountabilities:\\nAnalyze and interpret metabolomics data sets derived from mass spectrometry.\\nImplement and optimize data processing workflows to convert raw mass spectrometry data into interpretable metabolite information.\\nPerform spectral analysis, peak curation and assignment.\\nPerform feature registration across different batches and datasets using a combination of MS2, ion mobility, and other techniques.\\nUtilize multivariate statistical analysis, metabolic pathway analysis, and other advanced techniques for the interpretation of metabolomics data.\\nCollaborate with scientists and bioinformaticians to improve data quality, consistency, and interpretability.\\nDraft technical reports, prepare presentations, and publish findings in peer-reviewed journals.\\nKeep abreast of the latest trends, techniques, and technologies in metabolomics and mass spectrometry.\\nContribute to the design and interpretation of metabolomics experiments.\\n \\nADDITIONAL INFORMATION\\n\\nMinimum Education: Master’s degree or Ph.D. in bioinformatics, statistics, or a related field with an emphasis on metabolomics.\\nMinimum Experience: 5 years\\nMinimum Field of Expertise: Experience in research specialization with advanced knowledge of equipment, procedures and analysis methods and ability to supervise on a regular or project basis.\\nStrong experience with mass spectrometry data and its applications in metabolomics.\\nProficiency in data processing software for mass spectrometry and metabolomics, such as R, MPP, MassHunter, XCMS, Metaboscape, MZmine, or similar.\\nExtensive knowledge of statistical methods, data analysis, and machine learning techniques as they apply to metabolomics data.\\nProficient in scripting languages such as Python or R for automation of data analysis workflows.\\nFamiliarity with metabolic pathways and biochemistry.\\nExcellent communication skills and the ability to work in a multidisciplinary team.\\nCreative problem-solving skills.\\n \\nPreferred Education: Master’s degree in molecular/cellular biology, biochemistry, statistical analysis or other relevant disciplines.\\nPreferred Experience: Experience with LC/MS, HPLC, metabolomics, proteomics\\nPreferred Field of Expertise: Molecular/cellular biology, biochemistry, or other relevant discipline.\\nExperience in applying metabolomics in a specific research area, such as clinical research, pathway analysis, exposomics, etc.\\nExperience with integrating metabolomics data with other omics data types, such as genomics or proteomics.\\nPublications in peer-reviewed scientific journals demonstrating expertise in metabolomics and data analysis.\\n \\n\\nSalary: $83,000-$130,000 DOE\\n\\n\\nFor the safety and health of employees, guests, and patients, the Ellison Institute of Technology may mandate vaccination requirements for employment. The Ellison Institute of Technology's policies are always subject to review and change to ensure they are appropriate under the circumstances.\\n\\n\\nThe Ellison Institute of Technology is an equal opportunity employer. We believe that an inclusive, collaborative team environment is just as important to our mission as stethoscopes and microscopes. We strive to always provide employees a supportive atmosphere, so they feel confident taking creative risks toward innovation. The Ellison Institute of Technology values emotional intelligence and communication with empathy and respect for others. We seek to build a diverse group of people who are curious, have a deep sense of responsibility, and the grit needed to achieve excellence.\", 'AppZen is the leader in autonomous spend-to-pay software. Its patented artificial intelligence accurately and efficiently processes information from thousands of data sources so that organizations can better understand enterprise spend at scale to make smarter business decisions. It seamlessly integrates with existing accounts payable, expense, and card workflows to read, understand, and make real-time decisions based on your unique spend profile, leading to faster processing times and fewer instances of fraud or wasteful spend. Global enterprises, including one-third of the Fortune 500, use AppZen’s invoice, expense, and card transaction solutions to replace manual finance processes and accelerate the speed and agility of their businesses. To learn more, visit us at www.appzen.com .\\n\\nWe are looking for an experienced Machine Learning Scientist to come and work on our growing AI stack. If you are excited about cutting-edge work in Generative AI, NLP, Reinforcement Learning, Scalable Model Deployment, then AppZen is the right place for you to use and grow your skills.\\n\\nRequirements\\n\\nBuilding models using state of the art deep learning methods such as transformers\\nExperience solving novel problems using natural language processing\\nExperience training/fine-tuning LLMs or experience with reinforcement learning highly desired\\nKnowledge of graph-based machine learning or document understanding methods a plus\\nExpert in object oriented Python\\nExperience with Numpy, Pytorch, TensorFlow/Keras, NLTK or other associated packages\\nAbility and desire to work in all stages of ML pipeline, from data exploration to production deployment\\nSelf starter and ability to manage and own projects\\nMS with 1-2 years of related work experience\\nAble to work onsite in San Jose, CA\\n\\nBenefits\\n\\nCompetitive salary package and bonus or variable incentive pay depending on role\\nComprehensive medical, dental, vision and life insurance benefits\\nUnlimited PTO\\nPaid parental leave for eligible employees\\n401(k) match\\n$250 annual reimbursement for continuous learning\\nOnsite gym access for HQ (San Jose) employees\\nLots of office perks like snacks, happy hours, company events\\nThese benefits are only applicable to full time employees\\n\\n$130,000 - $160,000 a year\\n\\nAppZen is committed to fair and equitable compensation practices.\\n\\nThe base pay range for this role is listed above. Actual compensation packages are based on several factors that are unique to each candidate, including but not limited to skill set, depth of experience, certifications, and specific work location. This may be different in other locations due to differences in the cost of labor.\\n\\nThe total compensation package for this position may also include annual performance bonus, stock, benefits and/or other applicable incentive compensation plans.\\n\\nWe are equal opportunity employer and value diversity. All employment is decided on the basis of qualifications, merit and business need.', \"As an essential contributor to the professional services team, the Data Scientist is not just a data analyst but a data innovator. You'll utilize sophisticated analytics to mine valuable knowledge from intricate datasets, ultimately assisting in both the creation and fine-tuning of client-specific supply chain solutions. Collaborating closely with various departments—ranging from product management and engineering to client engagement—this role focuses on isolating pressing business issues and developing mitigation strategies informed by data analytics to elevate operational efficacy, client satisfaction, and holistic business results.\\n\\nData is the cornerstone of our business, and as a Data Scientist, you'll be tasked with developing first-in-class, data-driven products and solutions that help our customers improve their supply chain initiatives, making it both easy and efficient. Working in tandem with multidisciplinary teams spanning product development, engineering, and business operations, you will employ advanced Operations Research (OR) and Machine Learning (ML) techniques to produce data-backed insights and automated decision-making capabilities for our suite of supply chain products.\\n\\nIn your capacity, you'll harness the power of data to uncover actionable insight, locate areas ripe for improvement, and propel forward-thinking innovation across our range of products. Your work will shape the future of supply chain management, providing intelligent, data-based solutions that are at the forefront of industry innovation.\\n\\nKey Responsibilities\\nDevelop, implement, and validate predictive models and algorithms to improve and optimize user experience and supply chain efficiency.\\nWork with large, complex datasets, solving problems using advanced statistical and machine learning techniques.\\nCollaborate with cross-functional teams to define problems, identify data sources, and prepare datasets for analysis.\\nCommunicate findings and insights to technical and non-technical stakeholders through clear visualizations, reports, and presentations.\\nKeep abreast of the latest industry trends and research to continuously innovate and improve our products.\\nParticipate in the data governance and quality initiatives, ensuring the highest level of data integrity.\\nEngage with expansive and intricate datasets, addressing challenging data analysis issues through the application of advanced analytical techniques as required.\\n\\nKey Skills & Experience\\nMaster’s degree or Ph.D. in Computer Science, Statistics, Applied Mathematics, or related field.\\nProven experience in data science, machine learning, or related roles.\\nProficiency in data manipulation and analysis tools such as Python, R, SQL, and data warehousing solutions.\\nProficient in creating and executing optimization frameworks and algorithms geared towards tackling large-scale, discrete, and permutation-based challenges such as vehicle routing, inventory management, network architecture, labor management, scheduling, and facility siting.\\nStrong understanding of machine learning techniques and algorithms.\\nAbility to translate complex findings into compelling recommendations, formulating them into actionable plans for the team.\\nPrevious experience in supply chain optimization or retail/manufacturing sectors, emphasizing quantitative modeling and analysis.\\nFamiliarity with Big Data tools like Hadoop, Spark, or equivalent.\\nPortfolio of projects or published research demonstrating your ability to solve complex problems using data.\\nExcellent analytical and problem-solving skills.\\nStrong communication and interpersonal skills.\\nAbility to work independently and as part of a team.\\nDemonstrated leadership and mentoring skills.\", \"About The Team\\n\\nCome help us build the world's most reliable on-demand, logistics engine for last-mile retail delivery! We're looking for an experienced machine learning engineer to help us develop the AI/ML that powers DoorDash's growing Drive Business.\\n\\nAbout The Role\\n\\nWe’re looking for a passionate Applied Machine Learning expert to join our team. In this role, you will utilize our robust data and machine learning infrastructure to implement top-notch AI/ML solutions to build a seamless consumer voice ordering experience, and improve delivery quality for merchants, dashers, and consumers for the Drive Business. As a Staff Machine Learning Scientist, you’ll be conceptualizing, designing, evaluating, and implementing new AI solutions to DoorDash, building and fine-tuning Large Language Models. You will be expected to demonstrate a strong command of production-level machine learning, a passion for solving end-user problems, leadership skills to collaborate well with multi-disciplinary teams, and execution focused to prioritize effectively in a dynamic environment. You will be reporting into the Machine Learning Engineering Manager. This will be a hybrid position in San Francisco, Sunnyvale, Los Angeles, Seattle, or New York.\\n\\nYou’re Excited About This Opportunity Because You Will…\\n\\nDevelop production machine learning solutions to build a world-class voice ordering AI solution.\\nBring top-notch Large Language Model solutions to DoorDash\\nPartner with engineering and product leaders to help shape the product roadmap leveraging AI. \\nLead cross-functional pods to generate collective impact. \\n\\nYou can find out more on our ML blog here\\n\\nWe’re Excited About You Because You Have…\\n\\n7+ years of industry experience developing machine learning models with business impact, and shipping ML solutions to production. \\n1+ years in a tech lead capacity \\nDeep expertise in large language models, deep learning, and fine-tuning. \\nStrong machine learning background in Python; experience with Spark, PyTorch or TensorFlow preferred. \\nYou must be located near one of our engineering hubs which includes: San Francisco, Sunnyvale, Los Angeles, Seattle, and New York\\nM.S., or PhD. in Statistics, Computer Science, Math, Operations Research, Physics, Economics, or other quantitative field\\nFamiliarity with causal inference and experimentation preferred\\n\\nAbout DoorDash\\n\\nAt DoorDash, our mission to empower local economies shapes how our team members move quickly, learn, and reiterate in order to make impactful decisions that display empathy for our range of users—from Dashers to merchant partners to consumers. We are a technology and logistics company that started with door-to-door delivery, and we are looking for team members who can help us go from a company that is known for delivering food to a company that people turn to for any and all goods.\\n\\nDoorDash is growing rapidly and changing constantly, which gives our team members the opportunity to share their unique perspectives, solve new challenges, and own their careers. We're committed to supporting employees’ happiness, healthiness, and overall well-being by providing comprehensive benefits and perks including premium healthcare, wellness expense reimbursement, paid parental leave and more.\\n\\nOur Commitment to Diversity and Inclusion\\n\\nWe’re committed to growing and empowering a more inclusive community within our company, industry, and cities. That’s why we hire and cultivate diverse teams of people from all backgrounds, experiences, and perspectives. We believe that true innovation happens when everyone has room at the table and the tools, resources, and opportunity to excel.\\n\\nStatement of Non-Discrimination: In keeping with our beliefs and goals, no employee or applicant will face discrimination or harassment based on: race, color, ancestry, national origin, religion, age, gender, marital/domestic partner status, sexual orientation, gender identity or expression, disability status, or veteran status. Above and beyond discrimination and harassment based on “protected categories,” we also strive to prevent other subtler forms of inappropriate behavior (i.e., stereotyping) from ever gaining a foothold in our office. Whether blatant or hidden, barriers to success have no place at DoorDash. We value a diverse workforce – people who identify as women, non-binary or gender non-conforming, LGBTQIA+, American Indian or Native Alaskan, Black or African American, Hispanic or Latinx, Native Hawaiian or Other Pacific Islander, differently-abled, caretakers and parents, and veterans are strongly encouraged to apply. Thank you to the Level Playing Field Institute for this statement of non-discrimination.\\n\\nPursuant to the San Francisco Fair Chance Ordinance, Los Angeles Fair Chance Initiative for Hiring Ordinance, and any other state or local hiring regulations, we will consider for employment any qualified applicant, including those with arrest and conviction records, in a manner consistent with the applicable regulation.\\n\\nIf you need any accommodations, please inform your recruiting contact upon initial connection.\\n\\nCompensation\\n\\nThe location-specific base salary range for this position is listed below. Compensation in other geographies may vary.\\n\\nActual compensation within the pay range will be decided based on factors including, but not limited to, skills, prior relevant experience, and specific work location. For roles that are available to be filled remotely, base salary is localized according to employee work location. Please discuss your intended work location with your recruiter for more information.\\n\\nDoorDash cares about you and your overall well-being, and that’s why we offer a comprehensive benefits package, for full-time employees, that includes healthcare benefits, a 401(k) plan including an employer match, short-term and long-term disability coverage, basic life insurance, wellbeing benefits, paid time off, paid parental leave, and several paid holidays, among others.\\n\\nIn addition to base salary, the compensation package for this role also includes opportunities for equity grants.\\n\\nCalifornia Pay Range:\\n\\n$176,000—$238,000 USD\\n\\nNew York Pay Range:\\n\\n$176,000—$238,000 USD\\n\\nWashington Pay Range:\\n\\n$176,000—$238,000 USD\", 'Want to learn more about this role and Jobot? Click our Jobot logo and follow our LinkedIn page!\\n\\nJob details\\n\\nExciting Senior Machine Learning Engineer role with rapidly growing Biotech on the cutting edge of protein design!\\n\\nThis Jobot Job is hosted by Coalter Powers\\n\\nAre you a fit? Easy Apply now by clicking the \"Easy Apply\" button and sending us your resume.\\n\\nSalary $125,000 - $175,000 per year\\n\\nA Bit About Us\\n\\n****REMOTE OR HYBRID****\\n\\nOur client, a cutting-edge biotech company recently announced a large partnership with Google Cloud to build a generative AI platform for engineering biology and for biosecurity.\\n\\nWe are exclusively partnering with them on the build-out of the newly created Digital Tech / AI Enablement team.\\n\\nThis is an excellent opportunity to get in early and be a part of it all.\\n\\n\\n\\nWhy join us?\\n\\n\\nThe AI Enablement team is responsible for delivering the ML expertise required to make this happen. With nearly limitless compute capacity CPU, GPU, or TPU; you will have the opportunity to partner with biologists, software and DevOps engineers, and data scientists to create the necessary ML infrastructure.\\n\\nAs one of the first members of this new team, you will have a large role in molding our approaches to creating foundation models for biology, as well as creating fine-tuned and derived models for specific applications in bioengineering.\\n\\n Work remotely, hybrid, or in-office\\n Competitive Compensation including industry leading equity program\\n Nearly limitless AI/ML resources and capacity\\n Unlimited opportunity for career development and growth\\n\\nJob Details\\n\\nWhile the main focus of your work will be on building and evaluating new ML models for biology, many other types of work will come your way. You may need to do data archaeology, create and debug pipelines in tools like Kubeflow or Flyte, quickly learn the basics of protein folding or codon optimization, become the company’s expert on a new tool, debug odd results created by a production model for a project under a time crunch, contribute to brainstorming, planning and prioritization, make presentations, give feedback on others’ proposals and code, and more.\\n\\nThis is a new team, a significant company focus, and a rapidly evolving field. You will need to be able to handle ambiguity and uncertainty; on the flip side, you will be able to influence where things go and how they change.\\n\\nYou will identify what needs to happen, bring it to the team’s attention, and make it happen.\\n\\nYou will not be expected to be an expert in “All The Things”. You will be expected to have a high level of general technical competency, be a fast learner brimming with curiosity, and an expert in a few things - deep learning, in particular.\\n\\nResponsibilities\\n\\n Build, manage, and evolve a GCP-based platform for large scale (up to 100B+ parameters) training, evaluation, and serving of Foundation Models for biology. \\n Develop, implement and maintain a system for creating smaller models that combine large FMs with additional experimental data to address specific needs and applications.\\n Own processes for data ingestion, data prep, data and model provenance tracking, and various other data engineering and ML Ops activities.\\n Contribute to model design and experimentation.\\n Identify opportunities for application of AI and ML across the company, create prototypes, and contribute to overall prioritization and roadmap development for AI.\\n\\nMinimum Requirements\\n\\n PhD in a scientific discipline and a minimum of 5 years related experience; may include post-doctoral experience; Masters and 7 years of related experience; Bachelors and 9 years of related experience in data engineering, systems engineering, machine learning and operations, MLOps, or similar roles; or equivalent industry experience.\\n Deep experience with Python.\\n Experience with ML and data orchestration and workflow engines like Airflow, Kubeflow, Flyte, or Dagster.\\n Familiarity with recent literature and state of the art for large model architectures and training approaches\\n Experience with building machine/deep learning models with at least one common framework such as PyTorch, Tensorflow, or JAX.\\n\\nPreferred Capabilities And Experience\\n\\n Practical experience iterating on LLM design \\n Familiarity with the ML ecosystem, including MLFlow and related tools\\n Experience with Terraform or Pulumi, Kubernetes\\n Experience operating non-trivial GCP deployments\\n Experience with Vertex AI services\\n Experience with “Cloud Life Sciences” / Google Batch\\n\\nInterested in hearing more? Easy Apply now by clicking the \"Easy Apply\" button.\\n\\nWant to learn more about this role and Jobot?\\n\\nClick our Jobot logo and follow our LinkedIn page!', \"LiveRamp is the data collaboration platform of choice for the world’s most innovative companies. A groundbreaking leader in consumer privacy, data ethics, and foundational identity, LiveRamp is setting the new standard for building a connected customer view with unmatched clarity and context while protecting precious brand and consumer trust. LiveRamp offers complete flexibility to collaborate wherever data lives to support the widest range of data collaboration use cases—within organizations, between brands, and across its premier global network of top-quality partners. \\n\\nHundreds of global innovators, from iconic consumer brands and tech giants to banks, retailers, and healthcare leaders turn to LiveRamp to build enduring brand and business value by deepening customer engagement and loyalty, activating new partnerships, and maximizing the value of their first-party data while staying on the forefront of rapidly evolving compliance and privacy requirements. \\n\\nAbout This Role\\n\\nAs a Senior Data Scientist on the Measurement Services team, you will be responsible for supporting multiple clients in the development and delivery of measurement solutions using LiveRamp’s suite of products. This role is dynamic and diverse, requiring you to be innovative yet logical in applying statistical methods and analytical solutions. You will support LiveRamp customers and senior members of the team by generating campaign analysis, building attribution models, and creating custom reports and queries that utilize large volumes of data from a variety of sources including media partners, 3P syndicated partners, and first-party sales and CRM. You will leverage your technical and analytical skills to deliver thoughtful insights that inform client business decisions.\\n\\nTo be successful, you should be data-curious, comfortable using technical tools, and eager to dive into technical and analytical detail, familiar with statistical concepts. You should be an avid problem solver and embrace new and evolving data tools.\\n\\nYou Will:\\n\\nBecome an expert in LiveRamp products and be able to anticipate customer needs and suggest appropriate solutions\\nLead data discovery sessions with clients to understand their business needs and translate those needs into specific analysis requirements\\nSupport Engineering and Product teams to evolve and develop new measurement product features and capabilities based on client requirements and feedback\\nCollaborate with Product teams on the development of training material and best practices for new clients\\nProvide consultation and advisory support for clients across multiple measurement-related products\\nDevelop and deliver measurement analysis using statistical and mathematical concepts to drive business solutions, and interpret those results into actionable, tactical recommendations\\nCreate compelling presentations and deliver actionable marketing insights and recommendations based on analysis\\nProvide mentorship and guidance to other team members where necessary\\n\\nAbout You: \\n\\nHave a Bachelor’s degree in Business, Marketing, Math, Statistics, Economics, Computer Science or other quantitative discipline\\n5+ years of relevant experience in an analytical role within advertising, advertising technology, or marketing technology industry\\n5+ years of marketing analytics experience with designing experiments, developing statistical models, measuring campaigns, and identifying opportunities for optimization and improvement or comparable experience using data and analytics\\nStrong quantitative and research skills with proven ability to demonstrate data interpretation capabilities in terms of reporting and dashboards\\nAttention to detail and time management delivering high-quality, client-ready work for multiple projects across several client engagements while meeting deadlines\\nAbility to articulate strategic marketing implications of campaign performance results for business audiences with clarity and persuasiveness\\nHands-on fluency in performing data and statistical analysis using SQL, Python, BigQuery, R, Tableau, and/or others with minimal supervision\\nProficient in integrating multiple data sources (1st-party, 3rd-party, media activity) and utilizing creative analytical approaches to answer client audience insight or campaign performance questions\\nPractical experience with statistical modeling, marketing test design, and audience segmentation\\nComfortable with and has experience working in ambiguous environments while working in a consultative manner with customers \\nPreferred Skills:\\nCritical thinking – capable of generating consistently accurate, useful reports as well as enthusiasm for translating data into actionable insight relevant to marketing program objectives\\nEntrepreneurial and action-oriented with experience collaborating across functional areas to fulfill client requirements\\nWillingness and desire to learn and adopt new skills - both technical and non-technical - in order to craft and deliver the best solutions\\nExceptional EQ\\n\\nBenefits:\\n\\nPeople: Work with talented, collaborative, and friendly people who love what they do.\\nFun: We host in-person and virtual events such as game nights, happy hours, camping trips, and sports leagues. \\nWork/Life Harmony: Flexible paid time off, paid holidays, options for working from home, and paid parental leave.\\nComprehensive Benefits Package: Medical, dental, vision, life, and disability. Plus, mental health support (via Talkspace), flexible time off, parental leave, family forming benefits, and a flexible lifestyle and wellbeing reimbursement program (up to $375 per quarter, U.S. LiveRampers)\\nSavings: Our 401K matching plan—1:1 match up to 6% of salary—helps you plan ahead. Also Employee Stock Purchase Plan - 15% discount off purchase price of LiveRamp stock (U.S. LiveRampers)\\nRampRemote: A comprehensive office equipment and ergonomics program—we provide you with equipment and tools to be your most productive self, no matter where you're located\\n\\nLocation: work from any LiveRamp location or home\\n\\nMore About Us:\\n\\nLiveRampers are empowered to live our values of committing to shared goals and operational excellence. Connecting LiveRampers to new ideas and to one another is one of our guiding principles—one that informs how we hire, train, and grow our global teams across nine countries and four continents. By continually building inclusive, high belonging teams, LiveRampers can deliver exceptional work, champion innovative ideas, and be their best selves. Click here to learn more about Diversity, Inclusion, & Belonging (DIB) at LiveRamp.\\n\\nMore About Us: \\n\\nLiveRamp’s mission is to connect data in ways that matter, and doing so starts with our people. We know that inspired teams enlist people from a blend of backgrounds and experiences. And we know that individuals do their best when they not only bring their full selves to work but feel like they truly belong. Connecting LiveRampers to new ideas and one another is one of our guiding principles—one that informs how we hire, train, and grow our global team across nine countries and four continents. Click here to learn more about Diversity, Inclusion, & Belonging (DIB) at LiveRamp.\", 'Company Overview\\n\\nFanatics is building a leading global digital sports platform. The company ignites the passions of global sports fans and maximizes the presence and reach for hundreds of sports partners globally by offering innovative products and services across Fanatics Commerce, Fanatics Collectibles, and Fanatics Betting & Gaming, allowing sports fans to Buy, Collect and Bet. Through the Fanatics platform, sports fans can buy licensed fan gear, jerseys, lifestyle and streetwear products, headwear, and hardgoods; collect physical and digital trading cards, sports memorabilia, and other digital assets; and bet as the company builds its Sportsbook and iGaming platform. Fanatics has an established database of over 100 million global sports fans, a global partner network with over 900 sports properties, including major national and international professional sports leagues, teams, players associations, athletes, celebrities, colleges, and college conferences, and over 2,000 retail locations, including its Lids retail business stores.\\n\\nAs a market leader with more than 18,000 employees, and hundreds of partners, suppliers, and vendors worldwide, we take responsibility for driving toward more ethical and sustainable practices. We are committed to building an inclusive Fanatics community, reflecting and representing society at every level of the business, including our employees, vendors, partners and fans. Fanatics is also dedicated to making a positive impact in the communities where we all live, work, and play through strategic philanthropic initiatives.\\n\\nWe are looking for senior-level Data Scientists to join our Data Engineering, Science, and Analytics team. Do you thrive at applying data science to solve business problems? As a data scientist, you will have ample opportunities to apply your data science skillset to unlock vast business opportunities by extracting key insights using a wide range of data sources, advanced statistical models, and machine learning algorithms and translating results into meaningful, goal-oriented business actions. Each day, you will be presented with a variety of new challenges and interesting projects that tap your interests and strengths.\\n\\nResponsibilities\\nCollaborate with cross-functional partners in operations, finance, marketing, and engineering to understand business need and scope data science projects.\\nWrangle, process, cleanse, verify, and enrich data from different sources used for analysis.\\nHelp build data-informed business strategy and roadmaps.\\nTranslate business needs into data product requirements, evaluate technologies, and identify opportunities to innovate and improve our data science capabilities.\\nUse creative problem-solving skills to analyze data and build statistical / machine learning models to help solve business problems from different perspectives.\\nWork closely with data engineers to create services that can ingest and supply data to and from both internal and external sources and ensure data quality and timeliness.\\nLead the development of data visualization and dashboards to help the organization monitor performance, generate insights, and continuously improve user experience.\\nEstablish playbooks to drive process and consistent outcomes.\\nParticipating in the full life cycle of model development, which spans from business problem discovery, data discovery to model deployment and monitoring.\\nQualifications\\nMaster’s or Doctoral degree in Computer Science (with a focus on Deep Learning, Generative AI, Machine Learning, and Data Mining), Statistics, Econometrics, Physics, or other rigorous quantitative disciplines that require processing and modeling data at a complex and large scale.\\n6+ years of professional experience as a data scientist or an AI specialist.\\nProficient in Python, SQL, Spark, the associated Python and Spark packages commonly used by data scientists, and Deep Learning libraries, such as PyTorch.\\nProficient in wrangle and analyze data with complex relationships and time scale.\\nStrong understanding of and practical experience in a wide range of machine learning algorithms and statistical modeling.\\nExperience in using data visualization and dashboard tools.\\nWorking experience in cloud-native technology.\\nExperience working with large structured and unstructured datasets stored in relational and NOSQL databases.\\nOut of-the-box thinker and problem solver who can turn ambiguous business problems into clear data-driven solutions that deliver meaningful business impacts.\\nExcellent organizational skills, verbal and written communication skills, and presentation skills.\\nThe Following Is a Plus\\nProficient in other languages, such as R, used in data science.\\nBasic knowledge of data engineering and MLOps.\\nCertificates earned in data mining, machine learning, deep learning, statistical modeling, cloud computing, and data engineering.\\n$167,000 - $186,000 a year\\n\\nThe salary range for this position is $167,000-$186,000 which represents base pay only and does not include short-term or long-term incentive compensation. When determining base pay, as part of a final compensation package, we consider several factors such as location, experience, qualifications, and training.\\n\\nEnsure your Fanatics job offer is legitimate and don’t fall victim to fraud. Fanatics never seeks payment from job applicants. Feel free to ask your recruiter for a phone call or other type of communication for interview, and ensure your communication is coming from a Fanatics or Fanatics Brand email address. For added security, where possible, apply through our company website at www.fanaticsinc.com/careers\\n\\nTryouts are open at Fanatics! Our team is passionate, talented, unified, and charged with creating the fan experience of tomorrow. The ball is in your court now.\\n\\nFanatics is committed to responsible planning and purchasing (RPP) practices, working with its business partners across its global and multi-layered supply chain, to ensure that planning, sourcing, and purchasing decisions, along with other supporting processes, do not impede or conflict with the fulfillment of Fanatics’ fair labor practices.\\n\\nNOTICE TO CALIFORNIA RESIDENTS/APPLICANTS: In connection with your application, we collect information that identifies, reasonably relates to or describes you (“Personal Information”). The categories of Personal Information that we collect include your name, government issued identification number(s), email address, mailing address, other contact information, emergency contact information, employment history, educational history, criminal record, and demographic information. We collect and use those categories of Personal Information about you for human resources and other business management purposes, including identifying and evaluating you as a candidate for potential or future employment or other types of positions, recordkeeping in relation to recruiting and hiring, conducting criminal background checks as permitted by law, conducting analytics, and ensuring compliance with applicable legal requirements and Company policies. For additional information on how we collect and use personal information in connection with your job application, review our Candidate Privacy Policy-CA', 'About Ghost \\n\\nAt Ghost, our mission is to make self-driving for everyone. We build autonomous driving software for automakers, based on a breakthrough in artificial intelligence that finally makes highway autonomy safe and scalable for the consumer car market.\\n\\nGhost helps automakers reimagine the car of the future with a complete autonomy solution that can be fully customized and continuously upgraded, delivering a car that keeps getting better year after year.\\n\\nAt Ghost, we are responsible for both invention and productization – not only solving complex problems with novel technology but making sure that it can scale to millions of drivers on the road. It’s a bold undertaking, but one that makes for constant learning, real-world impact, and fulfilling work. Together, we are a small, multi-disciplinary team tackling one of the hardest challenges in technology today.\\n\\nGhost was founded in 2017 by John Hayes and Volkmar Uhlig. Before Ghost, John co-founded Pure Storage, taking the company public in 2015.\\n\\nGhost has over a hundred employees across its headquarters in Mountain View and additional offices in Dallas, Detroit, and Sydney. Ghost has raised over $200 million from investors including Mike Speiser at Sutter Hill Ventures, Keith Rabois at Founders Fund, and Vinod Khosla at Khosla Ventures.\\n\\nLearn more at https://ghostautonomy.com.\\n\\nThe Role\\n\\nAs a Machine Learning Engineer at Ghost, you will be part of a broader Model Engineering team responsible for building and testing the models that literally drive our vehicles. Operating on the noisy data of a real-world environment, the problems you face will be complex and open-ended, and the solutions you create have the potential for enormous impact. With a proven founding team and compelling plan, Ghost knows where it is going - but how it gets there will be up to you.\\n\\nRequirements:\\nProfessional experience working with real-world, physical data\\nStrong knowledge of machine learning, data engineering, deep learning frameworks, and related testing techniques\\nAbility to fine tune GPT models\\nA track record of having shipped models and code into production\\nStrong programming skills in object-oriented languages (functional programming is a plus), including ability to debug and optimize code\\n\\nBenefits\\n\\nMedical, Vision and Dental coverage (PPO, HMO, and HSA options available; 100% premium coverage of several PPO and HMO plans for employees)\\n401(k) plan\\nLife Insurance\\n\\nCompensation\\n\\nCompensation for this role consists of a base salary and an options grant, with the base salary expected to range from $175,000 to $275,000+. Individual compensation will be commensurate with the candidate’s experience.\\n\\nGhost is committed to equal employment opportunity. We will not discriminate against employees or applicants for employment on any legally‑recognized basis [“protected class”] including, but not limited to: veteran status, uniform service member status, race, color, religion, sex, national origin, age, physical or mental disability or any other protected class under federal, state or local law.', \"Responsibilities\\n\\n TikTok is the leading destination for short-form mobile video. Our mission is to inspire creativity and bring joy. TikTok has global offices including Los Angeles, New York, London, Paris, Berlin, Dubai, Singapore, Jakarta, Seoul and Tokyo. \\n\\nWhy Join Us\\nCreation is the core of TikTok's purpose. Our platform is built to help imaginations thrive. This is doubly true of the teams that make TikTok possible. \\nTogether, we inspire creativity and bring joy - a mission we all believe in and aim towards achieving every day. \\nTo us, every challenge, no matter how difficult, is an opportunity; to learn, to innovate, and to grow as one team. Status quo? Never. Courage? Always. \\nAt TikTok, we create together and grow together. That's how we drive impact - for ourselves, our company, and the communities we serve. \\nJoin us.\\n\\nAbout the team\\nPDPO(Privacy and Data Protection Office) is the organization to lead, supervise, and empower all TikTok's privacy work in an accountable and industry leading way. This team is the expert in the landscape of privacy risks and passionate about consulting across the company on implementing the proper safeguards and technical mitigations to ensure that our users’ privacy is honored across the TikTok's products and platforms. \\n\\nAbout the role\\nThe Data Scientist will lead programs to solve complex problems and deliver insights on trends as well as provide mentorship to other data scientists/analysts on the team. You will utilize data-driven techniques in building solutions to help measure, validate, identify, and envision TikTok's privacy evolution. You will design and scope out projects, build operationalized analytics tools and platforms, and most importantly, use data to find insights, diagnose problems and tell compelling stories. Besides the technical side, you will also get the opportunity to work closely with various XFN teams, including but not limited to legal, audit, security, public and government relations, and product. You will be expected to build deep domain knowledge from your day by day work and build a strong domain expertise in privacy. \\n- Use data to conduct impacted user and root cause analysis in privacy incidents, and suggest potential opportunities for privacy gaps and improvements. \\n- Conceptualize, develop and maintain dashboards for risk detection and monitoring. \\n- Collaborate with members of XFN teams (eg. domain experts, engineers) and provide data\\nanalysis on various privacy projects. \\n\\nQualifications\\n\\n - Bachelor degree in Mathematics, Statistics, Science, Engineering, Business, or similar\\n- 3+ years of experience in data analysis and statistical analysis tools such as SQL, R, or Python\\n- Strong analytical and reporting skills. Experience with data visualization tools or platforms (e.g. Dashboards, Tableau, etc.)\\n- Strong communication skills, written and verbal; ability to present data and its implications in a clear, concise manner\\n\\nPreferred Qualifications\\n- Advanced degree in Mathematics, Statistics, Science, Engineering, Business, or similar\\n- Experience leading complex programs and teams\\n- Data pipeline or cloud engineering experiences\\n- Familiarity with common Machine Learning application knowledge is a plus\\n\\nTikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too.\\n\\nTikTok is committed to providing reasonable accommodations in our recruitment processes for candidates with disabilities, pregnancy, sincerely held religious beliefs or other reasons protected by applicable laws. If you need assistance or a reasonable accommodation, please reach out to us at gprd.accommodations@tiktok.com. \\n\\nJob Information:\\n\\n【For Pay Transparency】Compensation Description (annually) \\nThe base salary range for this position in the selected city is $144000 - $240000 annually.\\nCompensation may vary outside of this range depending on a number of factors, including a candidate’s qualifications, skills, competencies and experience, and location. Base pay is one part of the Total Package that is provided to compensate and recognize employees for their work, and this role may be eligible for additional discretionary bonuses/incentives, and restricted stock units.\\nOur company benefits are designed to convey company culture and values, to create an efficient and inspiring work environment, and to support our employees to give their best in both work and life. We offer the following benefits to eligible employees:\\nWe cover 100% premium coverage for employee medical insurance, approximately 75% premium coverage for dependents and offer a Health Savings Account(HSA) with a company match. As well as Dental, Vision, Short/Long term Disability, Basic Life, Voluntary Life and AD&D insurance plans. In addition to Flexible Spending Account(FSA) Options like Health Care, Limited Purpose and Dependent Care.\\nOur time off and leave plans are: 10 paid holidays per year plus 17 days of Paid Personal Time Off (PPTO) (prorated upon hire and increased by tenure) and 10 paid sick days per year as well as 12 weeks of paid Parental leave and 8 weeks of paid Supplemental Disability.\\nWe also provide generous benefits like mental and emotional health benefits through our EAP and Lyra. A 401K company match, gym and cellphone service reimbursements. The Company reserves the right to modify or change these benefits programs at any time, with or without notice.\", 'GRAIL is a healthcare company whose mission is to detect cancer early, when it can be cured. GRAIL is focused on alleviating the global burden of cancer by developing pioneering technology to detect and identify multiple deadly cancer types early. The company is using the power of next-generation sequencing, population-scale clinical studies, and state-of-the-art computer science and data science to enhance the scientific understanding of cancer biology, and to develop its multi-cancer early detection blood test. GRAIL is headquartered in Menlo Park, CA with locations in Washington, D.C., North Carolina, and the United Kingdom. GRAIL, LLC is a wholly-owned subsidiary of Illumina, Inc. (NASDAQ:ILMN). For more information, please visit www.grail.com .\\n\\nGRAIL is seeking a Staff Bioinformatics Scientist to join the Machine Learning and Classifier Development team within the Bioinformatics and Data Science group. In this role, you will collaborate with scientists, engineers, and clinicians to identify signals and methods for cancer detection and categorization. You will apply cutting-edge machine learning algorithms to GRAIL’s rich sequencing datasets, and write robust software to implement and test new ideas. Your efforts will contribute to exciting biological discoveries, influential scientific publications, and impactful products for the early detection of cancer.\\n\\nResponsibilities\\n\\nEnvision, design, and own projects to investigate classifier performance and exploit opportunities for improvement\\nCollaborate both within the Bioinformatics group and cross-functionally to plan, execute, and interpret experiments\\nDevelop high-quality, reproducible software to achieve project goals in accordance with sound engineering principles\\nFollow best practices from machine learning and statistics to generate interpretable, reliable results\\nPresent project updates and results regularly at technical meetings\\n\\nQualifications\\n\\nB.S. or M.S. with 8+ years relevant experience, or Ph.D. with 5+ years relevant experience in Bioinformatics, Computational Biology, Computer Science, or a related field, or equivalent preparation and experience\\nDeep understanding of modern statistical and machine learning practices\\nExperience analyzing sequencing and genomics data to derive biological insights\\nExpertise in data analysis using R or Python\\nPreferred, experience with at least one system-level programming language (e.g. Go, Java, C, C++)\\nExperience with version control tools (e.g. git) and reproducible research practices in a Linux environment\\nTrack record of scientific contributions (highly cited publications, tools, products, or data sets used in commercial products and by the scientific community, presentations at international conferences, intellectual property, and awards)\\nSelf-direction, willingness to both teach others and learn new techniques\\nStrong written and verbal communication skills\\n\\nThe estimated, full-time, annual base pay scale for this position is $173,000 - $200,000. Actual base pay will consider skills, experience, and location.\\n\\nBased on the role, colleagues may be eligible to participate in an annual bonus plan tied to company and individual performance, or an incentive plan. We also offer a long-term incentive plan to align company and colleague success over time.\\n\\nIn addition, GRAIL offers a progressive benefit package, including flexible time-off, a 401k with a company match, and alongside our medical, dental, vision plans, carefully selected mindfulness offerings.\\n\\nGRAIL is an Equal Employment Office and Affirmative Action Employer and does not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, disability or any other legally protected status. We will reasonably accommodate all individuals with disabilities so that they can participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation. GRAIL maintains a drug-free workplace.', \"Sr Biostatistician - TMTT (Irvine, CA)\\n\\nPatients with mitral and tricuspid heart valve disease often have complex conditions with limited treatment options. Our Transcatheter Mitral and Tricuspid Therapies (TMTT) business unit is boldly pursuing an innovative portfolio of technologies to address a patient’s unmet clinical needs. It’s our driving force to help patients live longer and healthier lives. Join us and be part of our inspiring journey.\\n\\nThis position participates in the planning and execution of statistical tasks for assigned studies within a project under high level guidance from management or more senior level project statistician.\\n\\nYou will make an impact by...\\n\\nDevelop Statistical Analysis Plans (SAPs) and lead the execution effort for assigned studies.\\nDevelop statistical programs as necessary to ensure the accuracy of the planned and completed analyses or exploratory analyses.\\nReview protocols and contribute to protocol statistical analysis sections, and generate study randomization scheme when appropriate.\\nAuthor results sections of the clinical study reports and supply statistical input for PMA submissions and in response to regulatory questions.\\nProvide independent validation of the statistical content in study documents including randomization, study reports, briefing documents, patient brochures, publications, and other content delivered to external entities.\\nKeep abreast of new developments in statistics, drug development, and regulatory guidance through literature review and attendance at workshops and professional meetings.\\nRepresent Biostatistics in study team and work with management and other team members regarding study status and timeline update.\\nParticipate in developing case report forms and clinical database and data cleaning to ensure quality data collection.\\n\\nWhat you'll need (Required):\\n\\nPh.D. or equivalent in Biostatistics, or related field, including internship, senior projects, or thesis in Biostatistics or related field Required OR\\nMaster's Degree or equivalent in Biostatistics or related field PLUS 2 years of previous analytical experience in clinical trials Required\\n\\nWhat else we look for (Preferred):\\n\\nTraining, experience and/or publication in Bayesian Methodology Preferred\\nSubstantial understanding and knowledge of SAS; proficiency with additional software packages such as R or S-Plus desired\\nSound knowledge of theoretical and applied statistics.\\nStrong understanding of regulatory guidelines (e.g., GCP, FDA) as they apply to a Pharmaceutical/Medical Device research setting.\\nEffective skills in communication and team collaboration.\\nStrong documentation, communication skills and interpersonal relationship management skills\\nStrong problem-solving, organizational, analytical and critical thinking skills\\nAbility to interact professionally with all organizational levels\\nAbility to manage priorities in a fast paced environment\\nAdhere to all company rules and requirements (e.g., pandemic protocols, Environmental Health & Safety rules) and take adequate control measures in preventing injuries to themselves and others as well as to the protection of environment and prevention of pollution under their span of influence/control\\n\\nAligning our overall business objectives with performance, we offer competitive salaries, performance-based incentives, and a wide variety of benefits programs to address the diverse individual needs of our employees and their families.\\n\\nFor California, the base pay range for this position is $111,000 to $157,000 (highly experienced).\\n\\nThe pay for the successful candidate will depend on various factors (e.g., qualifications, education, prior experience).\\n\\nEdwards is an Equal Opportunity/Affirmative Action employer including protected Veterans and individuals with disabilities.\\n\\nCOVID Vaccination Requirement\\n\\nEdwards is committed to complying with the requirements and guidance from our government authorities and to protecting our vulnerable patients and the healthcare providers who are treating them around the world. As such, all Healthcare Interacting positions require COVID-19 vaccination, which includes anyone who directly interfaces with patients and those who interact with healthcare providers as part of their role. If hired, as a condition of employment, you will be required to submit proof that you have been fully vaccinated for COVID-19, unless you request and are granted a medical or religious accommodation for exemption from the vaccination requirement. This vaccination requirement does not apply in countries where it is prohibited by law to impose vaccination. In countries where vaccines are less available, or other requirements exist, we may institute alternate measures that optimize patient safety and healthcare provider safety, which may include regular COVID testing or specific masking requirements.\", 'At Coinbase, our mission is to increase economic freedom around the world, and we couldn’t do this without hiring the best people. We’re a group of hard-working overachievers who are deeply focused on building the future of finance and Web3 for our users across the globe, whether they’re trading, storing, staking or using crypto. Know those people who always lead the group project? That’s us.\\n\\nThere are a few things we look for across all hires we make at Coinbase, regardless of role or team. First, we look for candidates who will thrive in a culture like ours, where we default to trust, embrace feedback, and disrupt ourselves. Second, we expect all employees to commit to our mission-focused approach to our work. Finally, we seek people who are excited to learn about and live crypto, because those are the folks who enjoy the intense moments in our sprint and recharge work culture. We’re a remote-first company looking to hire the absolute best talent all over the world.\\n\\nReady to ? Who you are:\\n\\nYou’ve got positive energy. You’re optimistic about the future and determined to get there. \\nYou’re never tired of learning. You want to be a pro in bleeding edge tech like DeFi, NFTs, DAOs, and Web 3.0. \\nYou appreciate direct communication. You’re both an active communicator and an eager listener - because let’s face it, you can’t have one without the other. You’re cool with candid feedback and see every setback as an opportunity to grow.\\nYou can pivot on the fly. Crypto is constantly evolving, so our priorities do, too. What you worked on last month may not be what you work on today, and that excites you. You’re not looking for a boring job.\\nYou have a “can do” attitude. Our teams create high-quality work on quick timelines. Owning a problem doesn’t scare you, but rather empowers you to take 100% responsibility for achieving our mission.\\nYou want to be part of a winning team. We’re stronger together, and you’re a person who embraces being pushed out of your comfort zone. \\n\\nData Science is an integral component of Coinbase’s product and decision making process: we work in partnership with Product, Engineering and Design to influence the roadmap and better understand our users. With a deep expertise in experimentation, analytics and advanced modeling, we produce insights which directly move the company’s bottom line.\\n\\nWhat You’ll Be Doing (ie. Job Duties)\\n\\nAct as a strategic partner to functional teams: initiate and execute deep analyses and models to prioritize opportunities and provide actionable recommendations. \\nDesign and guide experiments/analysis to measure impact and drive product improvements. \\nMeasure business performance, develop / refine core metrics and create reporting to understand and monitor them. \\nSynthesize data learnings into compelling stories and communicate them throughout Coinbase and to senior executives. \\nConduct deep dives to help solve complex problems to drive impact for our business. \\nInitiate, develop, and maintain data pipelines with outstanding craftsmanship. \\n\\nWhat We Look For In You (ie. Job Requirements)\\n\\nBA / BS degree in a quantitative field (ex Math, Stats, Physics, or Computer Science) with 5+ years relevant experience, or or PhD degree in a related field with + 2 years of relevant experience. \\nAbility to independently create plans for projects and a track record of overseeing large / complex analytical projects spanning multiple teams. \\nUnderstanding of statistical concepts and practical experience applying them (in A|B testing, causal inference, ML, etc.). \\nExperience in data analyses using SQL. \\nExperience in programming/modeling in Python. \\nDemonstration of our core cultural values: clear communication, positive energy, continuous learning, and efficient execution. \\n\\nNice To Haves\\n\\nData engineering skills + a willingness to do lower level work to improve data foundations as needed. \\n\\nID: G2462\\n\\nPay Transparency Notice: Depending on your work location, the target annual salary for this position can range as detailed below. Full time offers from Coinbase also include target bonus + target equity + benefits (including medical, dental, vision and 401(k)).\\n\\nPay Range\\n\\n$140,250—$206,000 USD\\n\\nCommitment to Equal Opportunity\\n\\nCoinbase is committed to diversity in its workforce and is proud to be an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, creed, gender, national origin, age, disability, veteran status, sex, gender expression or identity, sexual orientation or any other basis protected by applicable law. Coinbase will also consider for employment qualified applicants with criminal histories in a manner consistent with applicable federal, state and local law. For US applicants, you may view Pay Transparency, Employee Rights and Know Your Rights notices by clicking on their corresponding links. Additionally, Coinbase participates in the E-Verify program in certain locations, as required by law.\\n\\nCoinbase is also committed to providing reasonable accommodations to individuals with disabilities. If you need a reasonable accommodation because of a disability for any part of the employment process, please send an e-mail to accommodations[at]coinbase.com and let us know the nature of your request and your contact information. For quick access to screen reading technology compatible with this site click here to download a free compatible screen reader (free step by step tutorial can be found here).\\n\\nGlobal Data Privacy Notice for Job Candidates and Applicants\\n\\nDepending on your location, the General Data Protection Regulation (GDPR) and California Consumer Privacy Act (CCPA) may regulate the way we manage the data of job applicants. Our full notice outlining how data will be processed as part of the application procedure for applicable locations is available here. By submitting your application, you are agreeing to our use and processing of your data as required. For US applicants only, by submitting your application you are agreeing to arbitration of disputes as outlined here.', 'Want to learn more about this role and Jobot? Click our Jobot logo and follow our LinkedIn page!\\n\\nJob details\\n\\nSenior Data Scientist Opportunity - Video Streaming\\n\\nThis Jobot Job is hosted by Alex Millan\\n\\nAre you a fit? Easy Apply now by clicking the \"Easy Apply\" button and sending us your resume.\\n\\nSalary $165,000 - $185,000 per year\\n\\nA Bit About Us\\n\\nAn exciting Youtube Streaming startup Partner that helps the biggest names in content creation scale their brands is currently looking to hire a Senior Data Scientist with a Computer Vision specialty. To date, they have received over $200 million in funding. Their ideal candidate has 5+ years of experience with a background developing deep learning models and building products backed up Machine Learning/Artificial Intelligence.\\n\\n\\n\\nWhy join us?\\n\\n\\nCompetitive base salary\\n\\n100% paid for benefits\\n\\nEquity\\n\\nBonus\\n\\ngreat work culture\\n\\n401k matching\\n\\nHybrid work schedule\\n\\nBeautiful new headquarters in West LA\\n\\nJob Details\\n\\nMaster\\'s degree or PhD in Computer Science, Data Science, Statistics, or a related field.Minimum of 3 years of experience in data science roles with a focus on machine learning and deep learning.\\n\\nProven experience with computer vision solutions, deploying CNNs, transformer models, diffusion, GANs, and variational autoencoders.\\n\\nStrong programming skills in Python, including experience with machine learning libraries and frameworks such as TensorFlow, Keras, or PyTorch.\\n\\nExperience with cloud services (AWS, Google Cloud, Azure) and understanding of distributed data/computing tools Map/Reduce, Hadoop, Hive, Spark, Gurobi, MySQL, etc.\\n\\nExcellent understanding of machine learning algorithms, processes, tools and platforms.\\n\\nStrong problem-solving skills with an emphasis on product development.\\n\\nExcellent written and verbal communication skills for coordinating across teams.\\n\\nPassion for discovering solutions hidden in large data sets and working with stakeholders to improve business outcomes.\\n\\nAbility to drive project work to successful completion, in a fast-paced, business critical environment.\\n\\nInterested in hearing more? Easy Apply now by clicking the \"Easy Apply\" button.\\n\\nWant to learn more about this role and Jobot?\\n\\nClick our Jobot logo and follow our LinkedIn page!', \"Overview\\n\\nIntuit is looking for innovative and hands-on Senior Data Scientist to join the Intuit AI team.\\n\\nThis team embeds artificial intelligence and machine learning into our product portfolio and business to create smarter products, improve anti-fraud and security and enhance customer care. Come join our collaborative and creative group of data scientists and machine learning engineers and build models that directly affect hundreds of thousands of our customers. In this role you will be building and deploying machine learning models using both analytical algorithms and deep learning approaches.\\n\\nWhat You'll Bring\\n\\nBS, MS, or PhD in an appropriate technology field (Computer Science, Statistics, Applied Math, Operations Research, etc.) \\n1+ years of industry experience with data science \\n1+ years of experience in modern advanced analytical tools and programming languages such as R or Python with scikit-learn \\nEfficient in SQL, Hive, or SparkSQL, etc. \\nComfortable in Linux environment \\n1+ years of experience in data mining algorithms and statistical modeling techniques such as clustering, classification, regression, decision trees, neural nets, support vector machines, anomaly detection, recommender systems, sequential pattern discovery, and text mining \\nSolid communication skills: Demonstrated ability to explain complex technical issues to both technical and non-technical audiences \\n\\nHow You Will Lead\\n\\nPerform hands-on data analysis and modeling with huge data sets \\nApply data mining, NLP, and machine learning (both supervised and unsupervised) to improve relevance and personalization algorithms \\nWork side-by-side with product managers, software engineers, and designers in designing experiments and minimum viable products \\nDiscover data sources, get access to them, import them, clean them up, and make them “model-ready”. You need to be willing and able to do your own ETL \\nCreate and refine features from the underlying data. You’ll enjoy developing just enough subject matter expertise to have an intuition about what features might make your model perform better, and then you’ll lather, rinse and repeat \\nRun regular A/B tests, gather data, perform statistical analysis, draw conclusions on the impact of your optimizations and communicate results to peers and leaders \\nExplore new design or technology shifts in order to determine how they might connect with the customer benefits we wish to deliver\", \"At Lyft, our mission is to improve people’s lives with the world’s best transportation. To do this, we start with our own community by creating an open, inclusive, and diverse organization.\\n\\nLyft’s Data Science Team builds mathematical models underpinning the platform’s core services. Compared to other technology companies of a similar size, the set of problems that we tackle is incredibly diverse. They cut across optimization, prediction, modeling, inference, transportation, and mapping. We are hiring motivated experts in each of these fields. We're looking for someone who is passionate about solving mathematical problems with data, and are excited about working in a fast-paced, innovative and collegial environment.\\n\\nYou will report into a Science Manager.\\n\\nResponsibilities:\\n\\nPartner with Engineers, Product Managers, and Business Partners to frame problems, both mathematically and within the business context.\\nPerform exploratory data analysis to gain a deeper understanding of the problem\\nConstruct and fit statistical, machine learning, or optimization models\\nWrite production modeling code; collaborate with Software Engineers to implement algorithms in production\\nDesign and run both simulated and live traffic experiments\\nAnalyze experimental and observational data; communicate findings; facilitate launch decisions\\n\\nExperience:\\n\\nM.S. or Ph.D. in Statistics, Operations Research, Mathematics, Computer Science, or other quantitative fields\\n3+ years professional experience\\nPassion for solving unstructured and non-standard mathematical problems\\nEnd-to-end experience with data, including querying, aggregation, analysis, and visualization\\nProficiency with Python, or another interpreted programming language like R or Matlab\\nWillingness to collaborate and communicate with others to solve a problem\\n\\nBenefits:\\n\\nGreat medical, dental, and vision insurance options\\nMental health benefits\\nFamily building benefits\\nIn addition to 12 observed holidays, salaried team members have unlimited paid time off, hourly team members have 15 days paid time off\\n401(k) plan to help save for your future\\n18 weeks of paid parental leave. Biological, adoptive, and foster parents are all eligible\\nPre-tax commuter benefits\\nLyft Pink - Lyft team members get an exclusive opportunity to test new benefits of our Ridership Program\\n\\nLyft is an equal opportunity/affirmative action employer committed to an inclusive and diverse workplace. All qualified applicants will receive consideration for employment without regards to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status or any other basis prohibited by law. We also consider qualified applicants with criminal histories consistent with applicable federal, state and local law. \\n\\nThis role will be in-office on a hybrid schedule — Team Members will be expected to work in the office 3 days per week on Mondays, Thursdays and a team-specific third day. Additionally, hybrid roles have the flexibility to work from anywhere for up to 4 weeks per year.\\n\\nThe expected range of pay for this position in the San Francisco area is $130,500 - $155,000. Salary ranges are dependent on a variety of factors, including qualifications, experience and geographic location. Range is not inclusive of potential equity offering, bonus or benefits. Your recruiter can share more information about the salary range specific to your working location and other factors during the hiring process.\", 'Position Title: Data Scientist II\\n\\nAbout The Role\\n\\nLTK’s Data Science and Machine Learning department is an integral component of the company’s Central Technology mission, playing a key role in the organization’s general data-driven transformation. As a Data Scientist II, you will be an integral part of and technical expert within a growth-hungry, acceleration-minded team of problem-solvers and product-builders. We work to creatively solve the needs of our different customers (brands, creators, and consumers). You will be a thought-partner who can collaborate effectively with engineering, product, and marketing teams to deliver quality solutions rapidly. You will work closely with others on the team as well as a senior mentor.\\n\\nHow You Will Make An Impact\\n\\nLeverage Python to train and and deploy data science algorithms or machine learning models\\nDesign and present high-level model architectures\\nCollaborate with other teams to integrate with data science products and create business value\\nDemonstrate ongoing work and finished products to stakeholders throughout the company\\nMake use of data visualization tools to explain / interpret results in a clear, digestible way\\nLeverage cloud technologies to deploy and integrate predictive and prescriptive models into operational applications\\nComplete root cause analysis and debug issues and bugs in deployed services\\nExperiment and research new Data Science and Machine Learning techniques or related tools and share those learnings with the team\\n\\n\\nWhat You Will Bring To LTK\\n\\nKnowledge of Python and fundamental libraries for statistics, machine learning, and deep learning (scikit learn, pandas, numpy etc…)\\nExperience collaborating with non-data science teams\\nThorough understanding of visual storytelling with python libraries like matplotlib, plotly, and seaborn\\nAbility and interest in conversing about fundamental areas of machine and deep learning such as exploratory analysis, feature engineering, sampling, validation, monitoring, as well as standard deep learning architectures\\nExperience working and troubleshooting with others in a collaborative environment\\nAt least some experience with cloud technologies (AWS preferred)\\nKnowledge of agile development principles\\nAt least some experience breaking down data science initiatives and presenting results to stakeholders required\\nDesire to work in a collaborative team environment\\n\\n\\nWhat We Offer\\n\\nThe opportunity to be part of the leading global company in creator commerce\\nA remote-first, productivity-first environment\\nCompetitive compensation and benefits package to meet the needs of you and your family\\n401(k) with LTK matching\\nWellness reimbursement\\nPaid parental leave program\\nSummer Fridays, birthday PTO, and paid volunteerism days\\nIn-person team events\\n\\n\\nBenefits package includes: Medical insurance, PPO, HSA, Wellness benefits, Vision insurance, Dental insurance, Paid maternity leave, Paid paternity leave, Family Bonding Time, Disability insurance, Life insurance, AD&D, Short-Term disability, Paid time off, Pet insurance benefits and more', \"At Qualia, we've built the leading B2B real estate technology that transforms the home buying and selling experience into a simple, secure, and enjoyable process. Our SMB and Enterprise products bring together users from across the real estate ecosystem---homebuyers and sellers, lenders, title and escrow agents, and real estate agents---onto a single shared digital closing platform, providing greater clarity and transparency to real estate transactions. Today, through our business customers across the country, millions of consumers use Qualia to close on homes every year.\\n\\nWhat You'll Work On\\n\\nWe are seeking a highly skilled and entrepreneurial Data Scientist to join our dynamic Engineering Team, who will partner closely with RevOps and Finance on Go-To-Market execution and internal strategy. As a Data Scientist, you will be responsible for extracting insights from Qualia’s rich datasets that drive the delivery of actionable recommendations to the business. In close partnership with our PM of Growth and our Finance team, your work will enable Qualia to deliver information and recommendations to our customers through our GTM teams, which will enable them to improve their own business operations through principled, data-informed decision processes. Ideal candidates possess strong understanding of software engineering, fluency in Python/pandas, and passion for delivering concise, high-impact analysis with conclusions that jump off the page. This is Qualia’s first Data Science role, and you will help to define our data functions while capitalizing on the significant greenfields opportunities.\\n\\nResponsibilities\\n\\nDevelop a deep understanding of Qualia’s data generating processes, data lifecycles, business strategy, and customers\\nIn partnership with Qualia’s Product, Finance, and GTM leadership, prioritize how we will use our data resources to deliver superior GTM results, to power our business operations, and to achieve strategic objectives\\nIntegrate data operations into our Engineering Team process without creating significant dependencies or operational burden\\nFormalize Qualia’s data ecosystem by deploying robust, maintainable, documented pipelines that reduce friction for data users across the organization\\nCollaborate closely with Product, Revenue Operations and Finance, key producers and consumers of our data\\nImplement machine learning models that target business problems and which can be deployed and maintained with a low operational footprint\\n\\n\\nYOUR BACKGROUND THAT LIKELY MAKES YOU A MATCH\\n\\nBachelor’s degree in Data Science, Computer Science, Statistics, or a related field\\n3+ years experience as a Data Scientist, or comparable background\\nDeep fluency in Python/pandas for data manipulation, modeling, and visualization\\nStrong SQL skills to query and manipulate data from data warehouses (BigQuery)\\nTime and effort are scarce, so we value sound business strategy, introspection, and application of common sense to guide how we spend those resources\\nExcellent verbal and written communication skills \\nCalifornia and Colorado Applicants: This role has a base annual salary of $130,000-$150,000 plus a competitive equity and benefits package. (Salary to be determined by relevant experience, location, knowledge, and skills of the applicant, internal equity, and alignment with market data.)\\n\\n\\nWHY QUALIA\\n\\nQualia is made up of incredibly bright, mission-driven coworkers who are passionate about using technology to solve real world problems---and we're growing quickly. In order to continue building an engaging and dynamic organization, we're committed to giving everyone the support they need to do great work.\\n\\nOur benefits package is designed to allow our team members to be their best selves, both in and out of the workplace. In addition to comprehensive health plans, a 401k program, and commuter benefits, we prioritize family and personal well-being through professional development, parental leave, and a flexible time off policy. Qualia offers a robust online onboarding program to train new hires, biweekly all hands meetings, and a variety of internal virtual events to keep employees connected.\\n\\nWe believe diverse perspectives and backgrounds are critical to building great technology, and our goal is to cultivate an environment where people feel equally valued and respected. Qualia is proud to be an equal opportunity workplace, and we welcome applicants from all backgrounds regardless of race, color, ancestry, religion, gender identity or expression, sexual orientation, marital status, age, citizenship, socioeconomic status, disability, or veteran status.\", 'Job Description:\\n\\nAt Warner Music Group, we’re a global collective of music makers and music lovers, tech innovators and inspired entrepreneurs, game-changing creatives, and passionate team members. Here, we know that each talent makes our collective bolder and brighter.\\n\\nWe remain committed to Diversity, Equity, and Inclusion. We know it fosters a culture where you can truly belong, contribute, and grow. We encourage applications from people of any age, gender identity, sexual orientation, race, religion, ethnicity, disability, veteran status, and any other characteristic or identity.\\n\\nTechnology is one of the most important parts of our business. Whether it’s finding and signing new artists, helping artists use the latest AI tools, or helping the business make thoughtful decisions with data-driven insights, technology plays an invaluable role in our success. The Data Science team at Warner Music Group (WMG) helps make all of that a reality.\\n\\nThe Data Science team builds predictive models, clustering and segmentation analyses, and deep learning and generative AI systems to provide actionable insights that drive business growth and to help WMG operate quickly and accurately at scale. Our ideal candidate will have a strong background in model-building and data analysis, along with curiosity and good communication skills.\\n\\nResponsibilities\\n\\nDevelop a thorough understanding of our complex datasets and interpret them as they relate to understanding our artists, songs, and fans and other new product development\\nBuild predictive models and generative AI solutions to capture opportunities and solve business problems\\nDesign production-level (efficient and scalable) models and robust data pipelines to manage the lifecycle of data and model\\nMentor and perform code & design reviews for more junior data scientists\\nCollaborate with other members of the Data Science and Engineering teams on ways to approach problems, augment code, and share new techniques\\nWork closely with cross-functional teams to define project objectives and deliverables, and to explain your approach, recommendations, and models\\nFind opportunities for improvement across the business and assist in decision-making across teams\\n\\nRequirements\\n\\n5 or more years of full-time, hands-on experience as a Data Scientist or equivalent\\nHigh degree of proficiency with one of Python or R (preferably Python) and SQL\\nDeep understanding of statistical modeling concepts and ML algorithms \\nStrong written communication and presentation skills (we have a writing & reading culture)\\nAbility to work independently and collaboratively in a fast-paced environment\\nExperience with cloud computing services or platforms (preferably AWS)\\nExperience with both Snowflake and Databricks is a plus\\nBachelor’s Degree or above in a quantitative field\\n\\nSalary Range\\n\\n$140,000 - $185,000 Annually\\n\\nSalary ranges are included for job postings where required by law. The actual base pay is dependent upon many factors, such as work experience and business needs. The pay range is subject to change at any time dependent on a variety of internal and external factors.\\n\\nWMG is committed to inclusion and diversity in all aspects of our business. We are proud to be an equal opportunity workplace and will evaluate qualified applicants without regard to race, religious creed, color, age, sex, sexual orientation, gender, gender identity, gender expression, national origin, ancestry, marital status, medical condition as defined by state law (genetic characteristics or cancer), physical or mental disability, military service or veteran status, pregnancy, childbirth and related medical conditions, genetic information or any other characteristic protected by applicable federal, state or local law.\\n\\nCopyright © 2023 Warner Music Inc.\\n\\nLinks to relevant documents:\\n\\n2023 Benefits At A Glance final.pdf\\n\\nEVerify Participation Poster.pdf\\n\\nRight To Work - English.pdf\\n\\nRight to Work - Spanish.pdf', 'Team Name\\n\\nGlobal Insights\\n\\nJob Title\\n\\nSenior Data Scientist, Computer Graphic\\n\\nRequisition ID\\n\\nR021029\\n\\nJob Description\\n\\nIt takes a Blizzard to make a Blizzard game, and that level of effort doesn’t end once the game is released; we support our games for years after they’re in the hands of gamers world-wide. Global Insights works closely with the game development, business, and internal service teams at Blizzard to produce insights and models that enable us to make the most epic player experiences. The machine learning & AI group within Global Insights focuses on applying the latest in key research areas to build lasting data products and tools for our game and business teams.\\n\\nWe are looking for a Senior Data Scientist who specializes in machine learning (ML) application in computer graphics to join our Global Insights team. Blizzard data scientists design and deploy data-powered products and services that serve the business and development of Blizzard games every day. The ideal candidate is passionate about the latest innovations in ML and AI; has expertise in designing deep learning system in computer graphics domain; with strong programming skill to help deploy and maintain them.\\n\\nThis role is anticipated to be a hybrid work position, with some work on-site and some work-from-home. The potential home studio for this role is Irvine, California.\\n\\nResponsibilities\\n\\nResearch and develop state-of-the-art machine learning solutions for computer graphic problems to enhance game development and gameplay objectives.\\nWork closely with software engineers and tech artists to deploy, automate, and maintain models seamlessly on production systems.\\nCommunicate about machine learning models and products to both technical and non-technical audiences.\\n\\nMinimum Qualifications\\n\\nHas 2+ years’ work experience with Master or PhD in ML and computer graphics\\nHands-on experience with one or more of the following: deep learning on 3D representations, generative models such as diffusion models and generative adversarial network (GAN), and deep learning for character animation\\nHands-on applied or research experience developing machine learning models on large image/3D model data sets\\nProficient in Python and experienced in at least one other higher-level programming languages like Java, C++, C#, etc.\\nPractical experience with deep learning libraries/frameworks like Tensorflow, Pytorch, Keras, or JAX\\n\\nPreferred Qualifications\\n\\nExperience with 3D graphics programs like 3ds Max, Maya, Blender, or Houdini\\nExperience with game engines like Unreal or Unity\\nExperience with procedural content generation for video games\\nExperience with cloud computing services like Google Cloud, AWS (Amazon Web Service), or Azure\\nExperience with real-time computer graphics libraries like OpenGL, DirectX, or Vulkan\\nA track record of original contributions to research communities of ML and computer graphics\\n\\nRequired Application Materials\\n\\nResume\\nCover Letter which should include:\\nWhy you are interested in working at Blizzard\\nHow you think machine learning can support game development and business\\nHow you can contribute to the effort of using machine learning to support Blizzard\\nThe Global Insights team prides in its dedication to fostering a healthy work-life balance, promoting mentorship at every level, and prioritizing mental and physical wellbeing. We offer a robust mentorship program for career development and provides opportunities to build rapport among team members. We are committed to providing growth opportunities for team members to look forward to.\\n\\nWe love hearing from anyone who is enthusiastic about changing the games industry. Not sure you meet all qualifications? Let us decide! Research shows that women and members of other under-represented groups tend to not apply to jobs when they think they may not meet every qualification, when, in fact, they often do! We are committed to crafting a diverse and inclusive environment and strongly encourage you to apply.\\n\\nWe are committed to working with and providing reasonable assistance to individuals with physical and mental disabilities. If you are a disabled individual requiring an accommodation to apply for an open position, please email your request to accommodationrequests@activisionblizzard.com. General employment questions cannot be accepted or processed here. Thank you for your interest.\\n\\nOur World \\n\\nAt Blizzard Entertainment, we pour our hearts and souls into everything we create. Best known for iconic video game universes including Warcraft, Overwatch, Diablo, and StarCraft, we’ve been creating genre-defining games for millions of players around the world for more than 30 years. We’re on a quest: bring our dreams to life and craft the most epic entertainment experiences…ever. Hard work, iteration and polish go into the Blizzard “secret recipe,” but the most important ingredients come from talented people who share our vision. If that sounds like you, join us.\\n\\nThe videogame industry and therefore our business is fast-paced and will continue to evolve. As such, the duties and responsibilities of this role may be changed as directed by the Company at any time to promote and support our business and relationships with industry partners.\\n\\nWe are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, gender identity, age, marital status, veteran status, or disability status, among other characteristics.\\n\\nRewards\\n\\nRequirements\\n\\nWe provide a suite of benefits that promote physical, emotional and financial well-being for ‘Every World’ - we’ve got our employees covered! Subject to eligibility requirements, the Company offers comprehensive benefits including:\\n\\nMedical, dental, vision, health savings account or health reimbursement account, healthcare spending accounts, dependent care spending accounts, life and AD&D insurance, disability insurance;\\n401(k) with Company match, tuition reimbursement, charitable donation matching;\\nPaid holidays and vacation, paid sick time, floating holidays, compassion and bereavement leaves, parental leave;\\nMental health & wellbeing programs, fitness programs, free and discounted games, and a variety of other voluntary benefit programs like supplemental life & disability, legal service, ID protection, rental insurance, and others;\\nIf the Company requires that you move geographic locations for the job, then you may also be eligible for relocation assistance.\\n\\nEligibility to participate in these benefits may vary for part time and temporary full-time employees and interns with the Company. You can learn more by visiting https://www.benefitsforeveryworld.com/.\\n\\nIn the U.S., the standard base pay range for this role is $92,920.00 - $171,814.00 Annual. These values reflect the expected base pay range of new hires across all U.S. locations. Ultimately, your specific range and offer will be based on several factors, including relevant experience, performance, and work location. Your Talent Professional can share this role’s range details for your local geography during the hiring process. In addition to a competitive base pay, employees in this role may be eligible for incentive compensation. Incentive compensation is not guaranteed.', \"We are looking for a Machine Learning engineer to join us in expanding Moveworks NLU capabilities, crystallizing enterprise text into rich and actionable representations to enable magical user experiences and improving Moveworks generative and conversational AI capabilities platform-wide.\\n\\nAs a member of the NLU team, you will have all the tools of modern NLP and NLG at your disposal, from best-in-class LLMs, MLMs, and hybrid vector databases to all the infrastructure needed to fine-tune, evaluate, and serve your own models in production. We are a data-centric team, and you will have the assistance of a world-class annotation team to build error-free, inclusive, and privacy-preserving datasets for model training and evaluation. Your work will impact our team’s core objective to understand every enterprise issue, in deep collaboration with other functions within Moveworks.\\n\\nIf you are looking to do the best work of your career with a bright, dedicated, and impact-driven team alongside you, we’d love to have a conversation.\\n\\nWho we are:\\n\\nMoveworks is on a mission to make language the universal UI. We give enterprises a conversational interface that works across every system — from Microsoft to Workday to Salesforce. Powered by GPT-class machine learning models, the Moveworks enterprise copilot learns the unique language of each organization to solve thousands of use cases. Brands like Databricks, Broadcom, DocuSign, and Palo Alto Networks leverage Moveworks’ proprietary enterprise data, out-of-the-box solutions, and intuitive developer tools to bring conversational automation to all aspects of their business.\\n\\nFounded in 2016, Moveworks has raised $315 million in funding, at a valuation of $2.1 billion. We’ve been named to the 2023 Forbes Cloud 100 list as well as the Forbes AI 50 list for five consecutive years, while also earning recognition as the winner of the 2023 Edison Awards for AI Optimized Productivity, and as the Best Bot Solution at the 2022 AI Breakthrough Awards.\\n\\nMoveworks has over 500 employees in six offices around the world, and is backed by some of the world's most prominent investors, including Kleiner Perkins, Lightspeed, Bain Capital Ventures, Sapphire Ventures, Iconiq, and more.\\n\\nCome join one of the most innovative teams on the planet!\\n\\nWhat you will do:\\n\\nApply machine learning and software engineering to create lasting value for all our customers\\nTake on exciting and difficult challenges in NLU, LLM, and conversational agent domains, such as coreference-preserving data masking, RLHF/RLAIF/DPO, fine-tuning LLMs for tool use and enterprise reasoning, machine translation, speech recognition, agent evaluation, active learning of exemplars for few-shot text classification, abstractive summarization, and grounding & verifiability for generated text.\\nPush the envelope of Moveworks commitments to responsible AI, expanding our infrastructure for ensuring models work equally well for all people, red-teaming models to ensure they behave safely and as intended, and keeping our ML at the cutting edge of data privacy and security\\nUse your knowledge of machine learning fundamentals to design new algorithms and architectures, evaluate them with small scale experiments and productionize your solutions at scale\\nResearch and develop innovative, scalable and dynamic solutions to hard problems\\nUse the latest advances in machine learning and LLMs to enhance our products and create delightful user experiences\\nSpend time weekly reading, discussing, and potentially building models out of the latest ML research and open-source code\\n\\nWhat you bring to the table:\\n\\nAbility to solve problems end-to-end with machine learning, including dataset curation, model training, evaluation, and deployment\\nDrive to ship product improvements with production-quality, fully unit-tested code and rigorously-evaluated model updates\\nSolid grasp of model evaluation fundamentals, especially for text classification and non-uniform sampling regimes\\nWorking familiarity with machine learning frameworks such as PyTorch and LightGBM\\nComfort developing on Mac and Linux with languages like Python, C++, and/or Go\\nKnowledge of deep learning architectures and algorithms and leading large language models\\nAttention to detail and high standard of data quality for training and especially evaluation datasets\\nDesire to work at a startup pace in a medium-sized company with a high degree of ownership\\nStrong appetite for continuous incremental wins and completing challenging projects fast\\nHigh level of curiosity about engineering outside of immediate discipline and ongoing desire to learn and stay at the cutting edge of NLU & AI\\n\\nNice to haves:\\n\\nExperience productionizing ML models at scale\\nExperience in AI fairness, privacy, safety, and/or security\\nExperience with NLP libraries such as HuggingFace Transformers, PEFT, and SpaCy 3\\nExperience iterating on prompts for large language models in a data-driven way\\n\\nBase Compensation Range: $129,000 - $221,000\\n\\nOur total compensation package includes a market competitive salary, equity for all full time roles, exceptional benefits, and, for applicable roles, commissions or bonus plans.\\n\\nUltimately, in determining pay, final offers may vary from the amount listed based on geography, the role’s scope and complexity, the candidate’s experience and expertise, and other factors.\\n\\nMoveworks Is An Equal Opportunity Employer\\n\\nMoveworks is proud to be an equal opportunity employer. We provide employment opportunities without regard to age, race, color, ancestry, national origin, religion, disability, sex, gender identity or expression, sexual orientation, veteran status, or any other characteristics protected by law.\", \"SENIOR DATA SCIENTIST - REMOTE - MUST HAVE ACTIVE TOP SECURITY CLEARANCE\\n\\n\\nTHE COMPANY\\n This advanced analytics company puts AI into use across the enterprise with enhanced analytics as they strive to help enterprises and governments make more intelligent and advanced business decisions. Their platform enables organizations to rapidly process complex data into multi-dimensional graph visualizations and predict future business outcomes with no-code AI modeling.\\n\\nTHE ROLE\\n You will get to work on projects such as forecasting, NLP, deep learning, computer vision, and more in a vast range of spaces like DoD, healthcare, IoT, finance, etc. You'll also own projects end-to-end, from inception to deployment while working closely with the client.\\n YOUR SKILLS AND EXPERIENCE\\n To be a great addition to the team, you must have experience in the following:\\nMUST HAVE: Active top security clearance\\n4+ years of full-time industry data science experience\\nPython, SQL, and the rest of the python data stack\\nDeploying ML models into production\\nTime Series or temporal data\\nDetail-oriented\\nB.S., Masters, or Ph.D. required in computer science, electrical engineering, or another related field\\n\\nTHE BENEFITS\\n As a Data Scientist/Senior Data Scientist, you can expect a salary between $150,000 to $180,000 (based on experience) plus competitive benefits.\\n HOW TO APPLY\\n Please register your interest by sending your CV to Mikayla Memoracion via the Apply link on this page.\", 'Our Client is seeking a Healthcare Data Scientist for a W2 only, extendable 6-12 month contract in Mountain View, CA. This position pays $50-65/hr.\\nResponsibilities:\\nAnalyze healthcare claims, referrals, and clinical data to identify trends and recognize outliers.\\nBuild predictive models to gauge capacity, identify denials, estimate revenue for certain types of care or interventions.\\nRequired Skills:\\n5+ years of experience as a Data Scientist working in the Healthcare industry.\\nExpertise using SQL and Python to build and deploy Machine Learning solutions.\\nExperience with EPIC data models.\\n\\n\\nPay for this position is based on market location and may vary depending on job-related knowledge, skills, and experience. As a contractor you may also be eligible for health benefits such as health, dental, and vision as well as access to a 401K plan. \\n Applicants should apply via The Mice Groups Inc. website (www.micegroups.com) or through this careers site posting. \\nWe are an equal opportunity employer and value diversity at The Mice Groups Inc. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.\\nPursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.\\nPursuant to the Los Angeles Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.\\nThe Mice Groups Inc. values your privacy. Please consult our Candidate Privacy Notice, for information about how we collect, use, and disclose personal information of our candidates.\\n Privacy Policy\\nOne of the basic principles The Mice Groups follows in designing and operating this website is that we ask for only the information we need to provide the service you’ve requested.\\nThe Mice Groups does not currently collect personal identifying information via its website except (i) to the extent that you provide this information in an online job application and (ii) to the extent that your web browser provides personal identifying information.\\nThe Mice Groups will use your personally identifying information solely for the purpose for which you submitted the information. The Mice Groups may, however, aggregate certain elements of your personal identifying information with the information of other users of our website to analyze the usefulness and popularity of various web pages on its website.\\nThe Mice Groups reserves the right to change this policy at any time by posting a new privacy policy at this location. Questions regarding this statement should be directed to info@micegroups.com', \"Note: By applying to this position you will have an opportunity to share your preferred working location from the following: Mountain View, CA, USA; New York, NY, USA; Redwood City, CA, USA; San Francisco, CA, USA.Minimum qualifications:\\n\\nMaster's degree in Statistics, Mathematics, Bioinformatics, Economics, a related field, or equivalent practical experience.\\n2 years of experience in a data science field.\\nExperience with statistical software (e.g., R, Python, MATLAB) and database languages (i.e., SQL).\\nExperience leveraging data insights into storytelling for business stakeholders.\\n\\nPreferred qualifications:\\n\\nPhD in Statistics or a related field.\\n2 years of experience with statistical data analysis such as generalized linear models, multivariate analysis, clustering/segmentation, and sampling methods.\\nExperience with machine learning on large-scale computing systems like Hadoop, MapReduce, or similar environments.\\nExperience in controlled experiment design and causal inference methods.\\nAbility to prioritize requests and partner well in an environment with competing demands from stakeholders.\\nExcellent communication skills.\\n\\nAbout The Job\\n\\nGoogle Ads Marketing aims to help advertisers of all sizes succeed with digital marketing. In this role, you will work with a team to advance the science of Marketing to customers that use Google’s advertising solutions. This is a unique opportunity to apply the tools of Data Science to accelerate Ads business growth, working cross-functionally with Sales, Marketing, and Product teams. You will perform data analytics, drive initiatives in experimentation, measurement, and advance machine learning modeling capability to support global marketing programs. In collaboration with a multidisciplinary team of Marketing, Product Management, Data Scientists, and Engineers, you will tap into the underlying data, develop and align on key metrics/methodologies, and generate insights that enable marketers to develop powerful, highly effective marketing programs.\\n\\nYou will leverage core Data Science expertise to design, prototype, and build out analysis pipelines to support initiatives and Marketing campaigns at scale. You'll perform analytics, design and execute on experimentation, and conduct incremental measurement analysis to inform strategic decisions of the marketing programs across the entire Ads Marketing space, from acquisition, onboarding, growth, and retention. You will build analytical frameworks and measurement capabilities to generate data driven insights that drive business growth. You will present and communicate effectively to marketing partners and leadership, data-driven insights and analytic results to inform on decision making.\\n\\nThe US base salary range for this full-time position is $124,000-$182,000 + bonus + equity + benefits. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process.\\n\\nPlease note that the compensation details listed in US role postings reflect the base salary only, and do not include bonus, equity, or benefits. Learn more about benefits at Google .\\n\\nResponsibilities\\n\\nWork with large, complex data sets. Solve complex analysis problems, applying advanced problem-solving methods (such as statistical and machine learning models) as needed. Conduct analysis that includes problem formulation, data gathering and requirements specification, processing, analysis, ongoing deliverables, and presentations.\\nDesign and analyze controlled experiments or counterfactual causal inference studies to examine the incremental impact of Ads marketing programs. \\nBuild and prototype analysis pipelines iteratively to provide insights at scale. Develop comprehensive knowledge of Google data structures and metrics, advocating for changes where needed.\\nInteract cross-functionally, making business recommendations (e.g., cost-benefit, forecasting, experiment analysis) with effective presentations of findings at multiple levels of stakeholders through visual displays of quantitative information.\\nDevelop and automate reports, iteratively build and prototype dashboards to provide insights at scale, solving for business priorities.\\n\\n\\nGoogle is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing our Accommodations for Applicants form .\", 'Snap Inc is a technology company. We believe the camera presents the greatest opportunity to improve the way people live and communicate. Snap contributes to human progress by empowering people to express themselves, live in the moment, learn about the world, and have fun together. The Company’s three core products are Snapchat, a visual messaging app that enhances your relationships with friends, family, and the world; Lens Studio, an augmented reality platform that powers AR across Snapchat and other services; and it\\'s AR glasses, Spectacles.\\n\\nThe Product team uses creativity, insights, and operational excellence to steer our product vision across Snap Inc. This team of designers, scientists, and product managers work in a highly collaborative environment to build the products and experiences that bring our community together in new and special ways.\\n\\nWe’re looking for a Data Scientist to join Snap Inc!\\n\\nWhat you’ll do:\\n\\nApply your expertise in quantitative analysis, data mining, and statistical modeling to deliver impactful, objective, and actionable data insights that enable informed business and product decisions\\nDrive informed and timely decision-making that improves and optimizes the way our products are built, completed, and adopted\\'\\nCollaborate with product managers, engineers, product marketers, and designers\\n\\nKnowledge, Skills & Abilities:\\n\\nExpertise using data modeling skills to identify key product trends and new product opportunities\\nAbility to design implement, and track core metrics to analyze the performance of our products\\nAbility to build visuals, dashboards, and reports to optimally communicate your insights\\nAbility to collaborate with engineers, product managers, and other cross-functional teams\\nAbility to initiate and drive projects to completion with minimal guidance\\nAn understanding of Snapchat with phenomenal product sense and product understanding\\n\\nMinimum Qualifications:\\n\\nBS/BA degree in statistics, mathematics, economics, computer science or equivalent years of experience\\n3+ years of experience in quantitative analysis & data science or a related field\\n3+ years of Experience using SQL or similar big data querying languages\\n3+ years of Experience with programming language, such as Python or R\\n3+ years of experience with applied statistical techniques, such as inferential methods, causal methods, A/B testing, or statistical modeling techniques\\n\\nPreferred Qualifications:\\n\\nAdvanced degree in applied mathematics, statistics, actuarial science, economics or related field\\nExperience in a product-focused role at a social media and/or mobile technology company\\nExperience using machine learning and statistical analysis for building data-driven product solutions or performing methodological research. \\n\\n\"Default Together\" Policy at Snap: At Snap Inc. we believe that being together in person helps us build our culture faster, reinforce our values, and serve our community, customers and partners better through dynamic collaboration. To reflect this, we practice a “default together” approach and expect our team members to work in an office at least 80% of the time (an average of 4 days per week). For roles with remote consideration: Remote team members still are expected to travel for mandatory in-person gatherings and to fulfill business needs, at least 4 to 6 times per year.\\n\\nAt Snap, we believe that having a team of diverse backgrounds and voices working together will enable us to create innovative products that improve the way people live and communicate. Snap is proud to be an equal opportunity employer, and committed to providing employment opportunities regardless of race, religious creed, color, national origin, ancestry, physical disability, mental disability, medical condition, genetic information, marital status, sex, gender, gender identity, gender expression, pregnancy, childbirth and breastfeeding, age, sexual orientation, military or veteran status, or any other protected classification, in accordance with applicable federal, state, and local laws. EOE, including disability/vets. If you have a disability or special need that requires accommodation, please don’t be shy and contact us at accommodations-ext@snap.com.\\n\\nOur Benefits: Snap Inc. is its own community, so we’ve got your back! We do our best to make sure you and your loved ones have everything you need to be happy and healthy, on your own terms. Our benefits are built around your needs and include paid parental leave, comprehensive medical coverage, emotional and mental health support programs, and compensation packages that let you share in Snap’s long-term success!\\n\\nCompensation\\n\\nIn the United States, work locations are assigned a pay zone which determines the salary range for the position. The successful candidate’s starting pay will be determined based on job-related skills, experience, qualifications, work location, and market conditions. These pay zones may be modified in the future.\\n\\nZone A (CA, WA, NYC):\\n\\nThe base salary range for this position is $165,000-$230,000 annually.\\n\\nZone B:\\n\\nThe base salary range for this position is $157,000-$219,000 annually.\\n\\nZone C:\\n\\nThe base salary range for this position is $140,000-$196,000 annually.\\n\\nThis position is eligible for equity in the form of RSUs.', 'Employee Applicant Privacy Notice\\n\\nWho we are:\\n\\nShape a brighter financial future with us.\\n\\nTogether with our members, we’re changing the way people think about and interact with personal finance.\\n\\nWe’re a next-generation fintech company using innovative, mobile-first technology to help our millions of members reach their goals. The industry is going through an unprecedented transformation, and we’re at the forefront. We’re proud to come to work every day knowing that what we do has a direct impact on people’s lives, with our core values guiding us every step of the way. Join us to invest in yourself, your career, and the financial world.\\n\\nThe role\\n\\nWe are seeking a Principal Data Scientist to focus on the most strategic priorities within Data Science and to collaborate broadly to help others in the organization grow. In this role you will report directly to the VP of Data Science and accomplish high-impact work across multiple teams, divisions and organizations as the most senior technical contributor in the DS org. Beyond direct impact on key projects, your responsibility will include mentorship, leadership in shaping standards for technical design and execution, hiring practices, and operational excellence.\\n\\nThe ideal candidate is very hands-on and equally motivated to develop strategies and plans that will move us forward. This role is comfortable making opinionated decisions and very often those decisions prove to be useful. This role will work closely with engineering, product, data science, executive teams and more to drive vision, strategy and execution. You’ll play the critical role of bridging the technical and business worlds to deliver solutions that enable data scientists and engineers to get things done while driving our business forward.\\n\\nWhat you’ll do:\\n\\n\\nDeliver high quality designs and code that benefits our Members\\nMake significant contributions to the technical architecture of the various products and systems.\\nGenerate ideas for new business-impacting initiatives and ways of leveraging data and machine learning in the pursuit of our goals.\\nAdvocate for continually-improving practices and drive inspiring standards for hiring, coding and collaboration.\\nDevelop, apply, and socialize impactful technology, engineering principles and concepts. \\nWork across teams at SoFi to help develop & enhance products and systems.\\nProvide technical mentorship to data scientists and continually improve our data-driven culture.\\nLead & present in broad forums with senior technical leaders across the company.\\n\\n\\nWhat you’ll need:\\n\\n\\nLikely 10+ years of experience, with significant depth in multiple areas such as applied machine learning, experimentation and scalable insights and advanced analytics. If you have less experience but feel this is the right job for you, we want to hear what has been exceptional about your faster journey.\\nExpert understanding and hands-on experience building and operating production data products and infrastructure. Your focus is data, but you have solid engineering skills. The lessons you have learned in your career so far will help accelerate the team.\\nDeep hands-on experience with major technologies such as Prefect, Airflow, DBT, git, docker/k8s, BI tools, Spark, Pytorch/TF, AWS, Snowflake, sklearn, sequence models / LLMs, etc. No one is an expert in everything but you have real breadth and depth.\\nExcellent verbal and written communication skills and a desire to share the right information with the right people. You do not need to be a polished speaker, you just need to be great at collaborating and helping others understand your work.\\n\\n\\nWho you are:\\n\\n\\nYou have a very high sense of agency in your work. In situations where some might get blocked by challenges you find paths forward.\\nYou work and play well with others; SoFi is a highly collaborative workplace and a melting pot of diverse talent. You make others feel safe to bring their entire selves to work and foster a positive environment around you.\\nYou care personally about your team and you know that sometimes the best way to express that care is to challenge directly to get to the truth.\\nYou thrive on developing creative solutions to hard problems.\\nYou take pride in your craft and seek practical excellence. You make high judgment decisions to deliver world-class results without getting distracted by impossible perfection.\\nYou are customer-obsessed and have an affinity for solving complex problems and shipping impactful features.\\nYou are confident in asking difficult questions at all levels of the organization.\\nYou listen and eagerly accept feedback, knowing that it can help you grow.\\nYou love learning - you have the ability to quickly and independently acquire proficiency in new languages and technologies.\\nYou are either experienced with finance / fintech or you are enthusiastic to learn and grow in this space.\\nMentoring others motivates you. You consistently find ways to encourage technical growth of those around you.\\nHelping people live better lives with the help of technology motivates and inspires you to do your best work.\\n\\n\\nCompensation And Benefits\\n\\nThe base pay range for this role is listed below. Final base pay offer will be determined based on individual factors such as the candidate’s experience, skills, and location.\\n\\nTo view all of our comprehensive and competitive benefits, visit our Benefits at SoFi page!\\n\\nSoFi provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion (including religious dress and grooming practices), sex (including pregnancy, childbirth and related medical conditions, breastfeeding, and conditions related to breastfeeding), gender, gender identity, gender expression, national origin, ancestry, age (40 or over), physical or medical disability, medical condition, marital status, registered domestic partner status, sexual orientation, genetic information, military and/or veteran status, or any other basis prohibited by applicable state or federal law.\\n\\nPursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.\\n\\nNew York applicants: Notice of Employee Rights\\n\\nSoFi is committed to embracing diversity. As part of this commitment, SoFi offers reasonable accommodations to candidates with physical or mental disabilities. If you need accommodations to participate in the job application or interview process, please let your recruiter know or email accommodations@sofi.com.\\n\\nDue to insurance coverage issues, we are unable to accommodate remote work from Hawaii or Alaska at this time.\\n\\nInternal Employees\\n\\nIf you are a current employee, do not apply here - please navigate to our Internal Job Board in Greenhouse to apply to our open roles.']\n"
     ]
    }
   ],
   "source": [
    "print(about_job_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27a1ecc-04f8-4be9-bb56-1f327552a1da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac425971-2ace-489f-acfa-2f0fd62c8544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312a24d0-ba85-4881-b912-1d7f98a7bb49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c7a171-802d-407c-8a08-61878f1cd3fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8910fda-178c-4a3a-afc0-c3059345c726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccc4ff4-dcf0-41f5-b627-220c15ce483b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca81728-975f-41c5-8083-f1873a0b3edd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77523de-0937-473d-a54f-3a0931b5edba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cdcd2b-2ed2-4a08-bf19-189babd0b5a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fead9b1e-3ff5-4b69-9a65-528a916f5913",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d42665c-b6f0-48d0-918e-0accdd6d6664",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408f3f85-1ea9-415a-ac2f-61afbfd96841",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d5b6987-4e37-406f-a2e5-36412a4a305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_data = get_job_details(driver)\n",
    "# df_main = pd.DataFrame(job_data)\n",
    "# df_main\n",
    "\n",
    "driver = open_browser(info_dict)\n",
    "driver.get(info_dict['start_url'])\n",
    "\n",
    "n_pages_to_scrape = 25\n",
    "wait_sec_each_page = 10-5\n",
    "update_every_n_secs = 60*6\n",
    "\n",
    "\n",
    "while True:\n",
    "    for n_page in range(n_pages_to_scrape):\n",
    "        if n_page ==0:\n",
    "            # go back to the first page\n",
    "            driver.get(info_dict['start_url'])\n",
    "        else:\n",
    "            go_to_next_page(driver)\n",
    "        time.sleep(wait_sec_each_page)\n",
    "        # update data\n",
    "        job_data = get_job_details(driver)\n",
    "        job_data = pd.DataFrame(job_data)\n",
    "        job_data['job_ids'] = job_data['job_ids'].astype('int')\n",
    "        data_class.df_main = update_dataframe(data_class.df_main, job_data, ['job_ids'])\n",
    "    \n",
    "    # save CSV \n",
    "    # df_main.to_csv('/Users/phil/Dropbox/GITHUB/scrapifurs/scrapifurs/data/tempdata/DS_mega_data_V1.csv', index=False)\n",
    "    data_class.save_it()\n",
    "    \n",
    "    print('_____________________________________________\\n\\n____________________')\n",
    "    time.sleep(update_every_n_secs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d49988a-3cf8-48c6-9d2c-4dcfd6e96a81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706a2824-8f09-4596-a587-26c7622b9923",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673c3b54-eae9-41c8-b373-b4d6d9773d69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebeaa4bf-7157-4085-8f48-e9f84db6f913",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a2b99e4",
   "metadata": {},
   "source": [
    "# Scraping step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6b98c4",
   "metadata": {},
   "source": [
    "### create rules to trim text based on key workds and search characteristics\n",
    "this willl help reduce the amount of data we pass to GPT API and reduce costs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c03b73d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_finder = utils.StringSectionExtractor()\n",
    "# text_finder.add_start_rule('\\d+ results', True)\n",
    "# text_finder.add_start_rule('All filters', False)\n",
    "# text_finder.add_start_rule('Jump to active job details', False)\n",
    "\n",
    "\n",
    "# text_finder.add_end_rule('Page \\d+ of \\d+', True)\n",
    "# text_finder.add_end_rule(\"Are you finding\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d61225",
   "metadata": {},
   "source": [
    "### INIT: chrome browser, login, save cookies (future login)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d1aa5ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beab480d-8d33-409a-a469-18afbdc2266f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e2a3a7-7e35-460c-8557-1c0004823be5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c6a129-d894-4534-8cfd-69d9c7ff1e7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c64a4b-a57c-43b2-8ffd-098c32e79eae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346a35a0-369c-4a76-a687-d8edb7dc6ed6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541668a6-cab7-4885-90bf-3193dc898ba7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bf7096-c4b4-4d31-a236-a464eafaa3bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddff5ae9-0104-43ae-97de-9bde1b71c04c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e623dd13-6cde-4426-91f9-eefbeacdb45e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba675596-c736-424c-847f-d013d810f1d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eec75c3-a495-4ce8-a35f-ea43bc5eb0b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3463aa-54fd-4bf7-b1c7-47d7d462a740",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87587382-4eaf-4197-b95a-8146e58ca309",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f482ff80-a8d6-496c-9a41-9704c79831ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199feb0f-76c6-42e6-b848-c2e7b69a3b8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e10b9a-6f68-4e5a-8248-9e05b003cc27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74c1894-590a-473d-8267-d57662de9a96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d3bceb-d72f-4f65-84b3-11b5e862de54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c767fdd8-76fc-4c67-ae80-d09983648bad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19973bfe-762f-4e06-9719-2f273b4e316e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354b6d38-e576-4003-a296-ccf0e0f6f7ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87ed016-d637-4eb2-a4c8-35269fc4724f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrapifurs",
   "language": "python",
   "name": "scrapifurs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
