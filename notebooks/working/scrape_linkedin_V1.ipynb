{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dbfade1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.common.keys import Keys\n",
    "# from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "# from selenium.webdriver.common.action_chains import ActionChains\n",
    "# import pandas as pd\n",
    "# import time\n",
    "# from dotenv import load_dotenv\n",
    "# import os\n",
    "# import openai\n",
    "\n",
    "# from bs4 import BeautifulSoup\n",
    "# import re\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.common.keys import Keys\n",
    "# from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "# import time\n",
    "# import pickle\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import openai\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d26a9d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def find_linkedin_info(driver, profile_url):\n",
    "    driver.get(profile_url)\n",
    "    time.sleep(5)  # Allow the page to load\n",
    "\n",
    "    # Get the source HTML of the page\n",
    "    source = driver.page_source\n",
    "\n",
    "    # Parse the source HTML with BeautifulSoup\n",
    "    soup = BeautifulSoup(source, 'lxml')\n",
    "\n",
    "    # Get the text from the parsed HTML\n",
    "    page_text = soup.get_text()\n",
    "    \n",
    "    # Split the text into lines and remove empty lines\n",
    "    lines = page_text.split('\\n')\n",
    "    non_empty_lines = [line for line in lines if line.strip() != \"\"]\n",
    "\n",
    "    # Join the non-empty lines back into a single string\n",
    "    url_text = \"\\n\".join(non_empty_lines)\n",
    "\n",
    "    return url_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1df1177f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "class Extractor:\n",
    "    '''\n",
    "    \n",
    "To request an appropriate pattern or string match for this class, you could ask:\n",
    "\n",
    "\"Please provide a string or a regular expression pattern that we should use for the start \n",
    "rule or end rule. If you provide a regular expression pattern, please specify that it is \n",
    "a regex. Also, note that for regular expressions, we're using Python's 're' module, so the \n",
    "pattern should be compatible with it. If you want to extract from the start or end of the \n",
    "text when no matching rule is found, please indicate that as well.\"\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.start_rules = []\n",
    "        self.end_rules = []\n",
    "\n",
    "    def add_start_rule(self, rule, is_regex=False):\n",
    "        self.start_rules.append((rule, is_regex))\n",
    "\n",
    "    def add_end_rule(self, rule, is_regex=False):\n",
    "        self.end_rules.append((rule, is_regex))\n",
    "\n",
    "    def extract(self, text, extract_if_no_start=False, extract_if_no_end=False):\n",
    "        if len(self.start_rules) > 0 and not extract_if_no_start:\n",
    "            start_index = None\n",
    "        else:\n",
    "            start_index = 0\n",
    "\n",
    "        if len(self.end_rules) > 0 and not extract_if_no_end:\n",
    "            end_index = None\n",
    "        else:\n",
    "            end_index = len(text)\n",
    "\n",
    "\n",
    "\n",
    "        for rule, is_regex in self.start_rules:\n",
    "            if is_regex:\n",
    "                match = re.search(rule, text)\n",
    "                if match is not None:\n",
    "                    start_index = match.end()  # We want the index after the start rule\n",
    "                    break  # If we've found a match, we can break\n",
    "            else:\n",
    "                idx = text.find(rule)\n",
    "                if idx != -1:\n",
    "                    start_index = idx + len(rule)  # We want the index after the start rule\n",
    "                    break  # If we've found a match, we can break\n",
    "\n",
    "        for rule, is_regex in self.end_rules:\n",
    "            if is_regex:\n",
    "                match = re.search(rule, text[start_index if start_index is not None else 0:])\n",
    "                if match is not None:\n",
    "                    end_index = (start_index if start_index is not None else 0) + match.start()  # We want the index before the end rule\n",
    "                    break  # If we've found a match, we can break\n",
    "            else:\n",
    "                idx = text.find(rule, start_index if start_index is not None else 0)  # We search after the start index\n",
    "                if idx != -1:\n",
    "                    end_index = idx\n",
    "                    break  # If we've found a match, we can break\n",
    "\n",
    "        if start_index is None or end_index is None:\n",
    "            return ''\n",
    "        \n",
    "        return text[start_index:end_index]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c5d9e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbec282a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def string_parcer_search_page(s):\n",
    "#     pattern = r\"(?<=search result pages)(.*?)(?=Page \\d+ of \\d+)\"\n",
    "#     match = re.search(pattern, s, re.DOTALL)\n",
    "#     if match:\n",
    "#         print(\"Match found!\")\n",
    "#         string_to_GPT = match.group().strip()\n",
    "#         return string_to_GPT\n",
    "#     else:\n",
    "#         return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "105a86bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error getting version of chromedriver 115. Retrying with chromedriver 114 (attempt 1/5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please login and press Enter to continue...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--disable-extensions\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "driver.get('https://www.linkedin.com/')\n",
    "\n",
    "# Load cookies if they exist\n",
    "try:\n",
    "    cookies = pickle.load(open(\"linkedin_cookies.pkl\", \"rb\"))\n",
    "    for cookie in cookies:\n",
    "        driver.add_cookie(cookie)\n",
    "    driver.refresh()\n",
    "except:\n",
    "    print(\"No cookies found. Manual login required.\")\n",
    "\n",
    "# Check if login is needed\n",
    "try: # not working\n",
    "    element_present = EC.presence_of_element_located((By.ID, 'profile-nav-item')) # check for an element that is only present when logged in\n",
    "    WebDriverWait(driver, 10).until(element_present)\n",
    "except:\n",
    "    # If not logged in\n",
    "    input('Please login and press Enter to continue...')\n",
    "    pickle.dump(driver.get_cookies(), open(\"linkedin_cookies.pkl\", \"wb\")) # save cookies after login\n",
    "\n",
    "\n",
    "# driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c33dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49bc1b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "Extract the names: Please look for patterns indicating a person's name. Names are usually proper nouns and are generally the first information provided in each data point.\n",
    "\n",
    "Find the job titles: Please identify each person's job title. This information follows the name and often contains phrases such as 'Data Scientist', 'Neuroscientist', 'Senior Associate Data Scientist', or a variant that includes the role and any additional qualifiers.\n",
    "\n",
    "Identify the job location: This information typically follows the job title and is usually a city or metropolitan area, such as 'Pasadena, CA' or 'Los Angeles Metropolitan Area'.\n",
    "\n",
    "Find the company names: The company names usually follow the job location. In case there is no current company mentioned, or the person is actively seeking a job, make a note of that by using placeholders like 'None (Job Seeking)' or 'None (No Company Mentioned)'.\n",
    "\n",
    "Construct the dictionary: Organize all the extracted information into a dictionary format where the key represents the type of information (e.g., 'Name', 'Job Titles', 'Job Location', 'Company Name') and the value is a list of all the extracted information of that type in the order of appearance.\n",
    "\n",
    "Formatting the instructions: Always make sure to start the code block with a comment '##BEGIN##', fill in the data accordingly, and end it with '##END##'. All columns of data should align with each other.\n",
    "\n",
    "For example:\n",
    "\n",
    "\n",
    "##BEGIN##\n",
    "\n",
    "DF_TEMP = {\n",
    "    \"Name\": [\"Person 1\", \"Person 2\", ...],\n",
    "    \"Job Titles\": [\"Job 1\", \"Job 2\", ...],\n",
    "    \"Job Location\": [\"Location 1\", \"Location 2\", ...],\n",
    "    \"Company Name\": [\"Company 1\", \"Company 2\", ...]\n",
    "}\n",
    "\n",
    "##END##\n",
    "\n",
    "\n",
    "Replace \"Person 1\", \"Job 1\", \"Location 1\", \"Company 1\", etc., with the actual extracted data.\n",
    "\n",
    "Please make sure to follow these instructions to achieve consistent results.\n",
    "\n",
    "\n",
    "here is the text:\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46c9164",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc52f832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7c0aa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def string_parcer_search_page(s):\n",
    "#     pattern = r\"(?<=search result pages)(.*?)(?=Page \\d+ of \\d+)\"\n",
    "#     match = re.search(pattern, s, re.DOTALL)\n",
    "#     if match:\n",
    "#         print(\"Match found!\")\n",
    "#         string_to_GPT = match.group().strip()\n",
    "#         return string_to_GPT\n",
    "#     else:\n",
    "#         return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025b9930",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "862e7de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url_text = find_linkedin_info(driver, url_in)\n",
    "# text_finder = Extractor()\n",
    "# text_finder.add_start_rule('search result pages', False)\n",
    "# text_finder.add_end_rule('Page \\d+ of \\d+', True)\n",
    "# text_finder.add_end_rule(\"messaging overlay\", False)\n",
    "# out = text_finder.extract(url_text)\n",
    "# print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bba52c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88be0b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_in = 'https://www.linkedin.com/search/results/people/?keywords=data%20scientist&origin=CLUSTER_EXPANSION&sid=fRq'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1b3c961",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_text = find_linkedin_info(driver, url_in)\n",
    "text_finder = Extractor()\n",
    "text_finder.add_start_rule('search result pages', False)\n",
    "text_finder.add_end_rule('Page \\d+ of \\d+', True)\n",
    "text_finder.add_end_rule(\"these results helpful\", False)\n",
    "text_finder.add_end_rule(\"messaging overlay\", False)\n",
    "\n",
    "gpt_text = text_finder.extract(url_text)\n",
    "GPT_INPUT = instructions+gpt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9d102e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e259e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extract the names: Please look for patterns indicating a person's name. Names are usually proper nouns and are generally the first information provided in each data point.\n",
      "\n",
      "Find the job titles: Please identify each person's job title. This information follows the name and often contains phrases such as 'Data Scientist', 'Neuroscientist', 'Senior Associate Data Scientist', or a variant that includes the role and any additional qualifiers.\n",
      "\n",
      "Identify the job location: This information typically follows the job title and is usually a city or metropolitan area, such as 'Pasadena, CA' or 'Los Angeles Metropolitan Area'.\n",
      "\n",
      "Find the company names: The company names usually follow the job location. In case there is no current company mentioned, or the person is actively seeking a job, make a note of that by using placeholders like 'None (Job Seeking)' or 'None (No Company Mentioned)'.\n",
      "\n",
      "Construct the dictionary: Organize all the extracted information into a dictionary format where the key represents the type of information (e.g., 'Name', 'Job Titles', 'Job Location', 'Company Name') and the value is a list of all the extracted information of that type in the order of appearance.\n",
      "\n",
      "Formatting the instructions: Always make sure to start the code block with a comment '##BEGIN##', fill in the data accordingly, and end it with '##END##'. All columns of data should align with each other.\n",
      "\n",
      "For example:\n",
      "\n",
      "\n",
      "##BEGIN##\n",
      "\n",
      "DF_TEMP = {\n",
      "    \"Name\": [\"Person 1\", \"Person 2\", ...],\n",
      "    \"Job Titles\": [\"Job 1\", \"Job 2\", ...],\n",
      "    \"Job Location\": [\"Location 1\", \"Location 2\", ...],\n",
      "    \"Company Name\": [\"Company 1\", \"Company 2\", ...]\n",
      "}\n",
      "\n",
      "##END##\n",
      "\n",
      "\n",
      "Replace \"Person 1\", \"Job 1\", \"Location 1\", \"Company 1\", etc., with the actual extracted data.\n",
      "\n",
      "Please make sure to follow these instructions to achieve consistent results.\n",
      "\n",
      "\n",
      "here is the text:\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "About 620,000 results\n",
      "          Status is offline\n",
      "Abhinav R.View Abhinav R.’s profile\n",
      "• 2nd2nd degree connection\n",
      "• 2nd2nd degree connection\n",
      "Incoming Data Scientist at Chevron | USC Alum\n",
      "Los Angeles, CA\n",
      "Past: Data Scientist at Chevron\n",
      "Navyada Koshatwar and Parth Kapadia are mutual connections\n",
      "    Message\n",
      "          Status is offline\n",
      "Shijie (Selene) XiangView Shijie (Selene) Xiang’s profile\n",
      "• 2nd2nd degree connection\n",
      "• 2nd2nd degree connection\n",
      "Data Scientist at eBay\n",
      "Los Angeles, CA\n",
      "Past: Data Scientist at PennyMac Loan Services, LLC\n",
      "Krystal Xu, Gianluca Turcatel, Phd, PPM, and 2 other mutual connections\n",
      "    Message\n",
      "          Status is offline\n",
      "Jinze XinView Jinze Xin’s profile\n",
      "• 2nd2nd degree connection\n",
      "• 2nd2nd degree connection\n",
      "Data Scientist at Twitter\n",
      "Los Angeles Metropolitan Area\n",
      "Past: Data Analyst at Lions Assurance Financial - ...• Analyzed financial data based on automatically scrapped S&P 500 data via Python pandas library...\n",
      "Siyuan Ni, Yifei Chen, and 5 other mutual connections\n",
      "    Message\n",
      "          Status is offline\n",
      "Syamil Mohd Razak, PhDView Syamil Mohd Razak, PhD’s profile\n",
      "• 2nd2nd degree connection\n",
      "• 2nd2nd degree connection\n",
      "Data Scientist | Petroleum Engineer\n",
      "Los Angeles Metropolitan Area\n",
      "Current: Data Scientist at Phillips 66\n",
      "Kailun Dong is a mutual connection\n",
      "    Connect\n",
      "Justin Chen\n",
      "Justin ChenView Justin Chen’s profile\n",
      "• 2nd2nd degree connection\n",
      "• 2nd2nd degree connection\n",
      "Data Scientist at Snap Inc.\n",
      "Los Angeles Metropolitan Area\n",
      "Past: Business Systems & Data Analytics Intern at City National Bank\n",
      "Krystal Xu is a mutual connection\n",
      "    Connect\n",
      "          Status is offline\n",
      "Malavika Ajith NairView Malavika Ajith Nair’s profile\n",
      "• 2nd2nd degree connection\n",
      "• 2nd2nd degree connection\n",
      "Data Scientist\n",
      "United States\n",
      "Past: Data Scientist at Meta\n",
      "Krystal Xu and Gianluca Turcatel, Phd, PPM are mutual connections\n",
      "    Message\n",
      "Marshall R.\n",
      "Marshall R.View Marshall R.’s profile\n",
      "• 2nd2nd degree connection\n",
      "• 2nd2nd degree connection\n",
      "Data Scientist | web3/DeFi\n",
      "United States\n",
      "Current: Data Scientist at Stealth Startup\n",
      "Gianluca Turcatel, Phd, PPM is a mutual connection\n",
      "    Message\n",
      "          Status is offline\n",
      "Kevin GoldbergView Kevin Goldberg’s profile\n",
      "• 2nd2nd degree connection\n",
      "• 2nd2nd degree connection\n",
      "Data Scientist at Meta\n",
      "Los Angeles, CA\n",
      "Summary: Data Scientist at Meta Reality Labs, focusing on engagement and monetization growth for the Quest...\n",
      "Yifei Chen is a mutual connection\n",
      "    Message\n",
      "hayden thornton\n",
      "hayden thorntonView hayden thornton’s profile\n",
      "• 2nd2nd degree connection\n",
      "• 2nd2nd degree connection\n",
      "Data Scientist at McKinsey & Company\n",
      "Los Angeles, CA\n",
      "Kailun Dong is a mutual connection\n",
      "    Connect\n",
      "Lu L.\n",
      "Lu L.View Lu L.’s profile\n",
      "• 2nd2nd degree connection\n",
      "• 2nd2nd degree connection\n",
      "Data Scientist at Facebook\n",
      "Los Angeles, CA\n",
      "Past: Senior Data Analyst at Grindr\n",
      "Gianluca Turcatel, Phd, PPM is a mutual connection\n",
      "    Message\n",
      "              Are \n"
     ]
    }
   ],
   "source": [
    "print(GPT_INPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e67b2b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# # driver.get(profile_url)\n",
    "# # time.sleep(5)  # Allow the page to load\n",
    "\n",
    "# # Get the source HTML of the page\n",
    "# source = driver.page_source\n",
    "\n",
    "# # Parse the source HTML with BeautifulSoup\n",
    "# soup = BeautifulSoup(source, 'lxml')\n",
    "\n",
    "# # Get the text from the parsed HTML\n",
    "# page_text = soup.get_text()\n",
    "# print(page_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fb8b18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()  # take environment variables from .env.\n",
    "os.environ[\"OPENAI_API_KEY\"]  = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4776bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "messages = [ {\"role\": \"system\", \"content\": \"You are a intelligent assistant.\"} ]\n",
    "messages.append({\"role\": \"user\", \"content\": GPT_INPUT})\n",
    "chat = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=messages)\n",
    "reply = chat.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df3a2ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##BEGIN##\n",
      "\n",
      "DF_TEMP = {\n",
      "    \"Name\": [\"Abhinav R.\", \"Shijie (Selene) Xiang\", \"Jinze Xin\", \"Syamil Mohd Razak, PhD\", \"Justin Chen\", \"Malavika Ajith Nair\", \"Marshall R.\", \"Kevin Goldberg\", \"Hayden Thornton\", \"Lu L.\"],\n",
      "    \"Job Titles\": [\"Incoming Data Scientist at Chevron | USC Alum\", \"Data Scientist at eBay\", \"Data Scientist at Twitter\", \"Data Scientist | Petroleum Engineer\", \"Data Scientist at Snap Inc.\", \"Data Scientist\", \"Data Scientist | web3/DeFi\", \"Data Scientist at Meta\", \"Data Scientist at McKinsey & Company\", \"Data Scientist at Facebook\"],\n",
      "    \"Job Location\": [\"Los Angeles, CA\", \"Los Angeles, CA\", \"Los Angeles Metropolitan Area\", \"Los Angeles Metropolitan Area\", \"Los Angeles Metropolitan Area\", \"United States\", \"United States\", \"Los Angeles, CA\", \"Los Angeles, CA\", \"Los Angeles, CA\"],\n",
      "    \"Company Name\": [\"Chevron\", \"eBay\", \"Twitter\", \"Phillips 66\", \"Snap Inc.\", \"Meta\", \"Stealth Startup\", \"Meta Reality Labs\", \"McKinsey & Company\", \"Facebook\"]\n",
      "}\n",
      "\n",
      "##END##\n"
     ]
    }
   ],
   "source": [
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928aacb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0f34972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Job Titles</th>\n",
       "      <th>Job Location</th>\n",
       "      <th>Company Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abhinav R.</td>\n",
       "      <td>Incoming Data Scientist at Chevron | USC Alum</td>\n",
       "      <td>Los Angeles, CA</td>\n",
       "      <td>Chevron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jinze Xin</td>\n",
       "      <td>Data Scientist at Twitter</td>\n",
       "      <td>Los Angeles Metropolitan Area</td>\n",
       "      <td>Twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Syamil Mohd Razak, PhD</td>\n",
       "      <td>Data Scientist | Petroleum Engineer</td>\n",
       "      <td>Los Angeles Metropolitan Area</td>\n",
       "      <td>Phillips 66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shijie (Selene) Xiang</td>\n",
       "      <td>Data Scientist at eBay</td>\n",
       "      <td>Los Angeles, CA</td>\n",
       "      <td>eBay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Malavika Ajith Nair</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>United States</td>\n",
       "      <td>Meta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Marshall R.</td>\n",
       "      <td>Data Scientist | web3/DeFi</td>\n",
       "      <td>United States</td>\n",
       "      <td>Stealth Startup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mehrnaz Motamed</td>\n",
       "      <td>Data Scientist - Machine Learning | Deep Learn...</td>\n",
       "      <td>Pasadena, CA</td>\n",
       "      <td>Endura Technologies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Farimah Shirmohammadi, PhD</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Los Angeles, CA</td>\n",
       "      <td>Edison International</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Kevin Goldberg</td>\n",
       "      <td>Data Scientist at Meta</td>\n",
       "      <td>Los Angeles, CA</td>\n",
       "      <td>Meta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Lu L.</td>\n",
       "      <td>Data Scientist at Facebook</td>\n",
       "      <td>Los Angeles, CA</td>\n",
       "      <td>Facebook</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Name  \\\n",
       "0                  Abhinav R.   \n",
       "1                   Jinze Xin   \n",
       "2      Syamil Mohd Razak, PhD   \n",
       "3       Shijie (Selene) Xiang   \n",
       "4         Malavika Ajith Nair   \n",
       "5                 Marshall R.   \n",
       "6             Mehrnaz Motamed   \n",
       "7  Farimah Shirmohammadi, PhD   \n",
       "8              Kevin Goldberg   \n",
       "9                       Lu L.   \n",
       "\n",
       "                                          Job Titles  \\\n",
       "0      Incoming Data Scientist at Chevron | USC Alum   \n",
       "1                          Data Scientist at Twitter   \n",
       "2                Data Scientist | Petroleum Engineer   \n",
       "3                             Data Scientist at eBay   \n",
       "4                                     Data Scientist   \n",
       "5                         Data Scientist | web3/DeFi   \n",
       "6  Data Scientist - Machine Learning | Deep Learn...   \n",
       "7                                     Data Scientist   \n",
       "8                             Data Scientist at Meta   \n",
       "9                         Data Scientist at Facebook   \n",
       "\n",
       "                    Job Location          Company Name  \n",
       "0                Los Angeles, CA               Chevron  \n",
       "1  Los Angeles Metropolitan Area               Twitter  \n",
       "2  Los Angeles Metropolitan Area           Phillips 66  \n",
       "3                Los Angeles, CA                  eBay  \n",
       "4                  United States                  Meta  \n",
       "5                  United States       Stealth Startup  \n",
       "6                   Pasadena, CA   Endura Technologies  \n",
       "7                Los Angeles, CA  Edison International  \n",
       "8                Los Angeles, CA                  Meta  \n",
       "9                Los Angeles, CA              Facebook  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Extracted information from the text\n",
    "##BEGIN##\n",
    "\n",
    "DF_TEMP = {\n",
    "    \"Name\": [\"Abhinav R.\", \"Jinze Xin\", \"Syamil Mohd Razak, PhD\", \"Shijie (Selene) Xiang\", \"Malavika Ajith Nair\", \"Marshall R.\", \"Mehrnaz Motamed\", \"Farimah Shirmohammadi, PhD\", \"Kevin Goldberg\", \"Lu L.\"],\n",
    "    \"Job Titles\": [\"Incoming Data Scientist at Chevron | USC Alum\", \"Data Scientist at Twitter\", \"Data Scientist | Petroleum Engineer\", \"Data Scientist at eBay\", \"Data Scientist\", \"Data Scientist | web3/DeFi\", \"Data Scientist - Machine Learning | Deep Learning | Statistics | R | SQL | Python\", \"Data Scientist\", \"Data Scientist at Meta\", \"Data Scientist at Facebook\"],\n",
    "    \"Job Location\": [\"Los Angeles, CA\", \"Los Angeles Metropolitan Area\", \"Los Angeles Metropolitan Area\", \"Los Angeles, CA\", \"United States\", \"United States\", \"Pasadena, CA\", \"Los Angeles, CA\", \"Los Angeles, CA\", \"Los Angeles, CA\"],\n",
    "    \"Company Name\": [\"Chevron\", \"Twitter\", \"Phillips 66\", \"eBay\", \"Meta\", \"Stealth Startup\", \"Endura Technologies\", \"Edison International\", \"Meta\", \"Facebook\"]\n",
    "}\n",
    "\n",
    "\n",
    "# Create a DataFrame\n",
    "DF_TEMP = pd.DataFrame(DF_TEMP)\n",
    "DF_TEMP\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba097356",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840f50a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f56f7f24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Job Titles</th>\n",
       "      <th>Job Location</th>\n",
       "      <th>Company Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abhinav R.</td>\n",
       "      <td>Incoming Data Scientist at Chevron | USC Alum</td>\n",
       "      <td>Los Angeles, CA</td>\n",
       "      <td>Chevron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jinze Xin</td>\n",
       "      <td>Data Scientist at Twitter</td>\n",
       "      <td>Los Angeles Metropolitan Area</td>\n",
       "      <td>Twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Syamil Mohd Razak, PhD</td>\n",
       "      <td>Data Scientist | Petroleum Engineer</td>\n",
       "      <td>Los Angeles Metropolitan Area</td>\n",
       "      <td>Phillips 66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shijie (Selene) Xiang</td>\n",
       "      <td>Data Scientist at eBay</td>\n",
       "      <td>Los Angeles, CA</td>\n",
       "      <td>eBay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Malavika Ajith Nair</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>United States</td>\n",
       "      <td>Meta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Marshall R.</td>\n",
       "      <td>Data Scientist | web3/DeFi</td>\n",
       "      <td>United States</td>\n",
       "      <td>Stealth Startup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mehrnaz Motamed</td>\n",
       "      <td>Data Scientist - Machine Learning | Deep Learn...</td>\n",
       "      <td>Pasadena, CA</td>\n",
       "      <td>Endura Technologies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Farimah Shirmohammadi, PhD</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Los Angeles, CA</td>\n",
       "      <td>Edison International</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Kevin Goldberg</td>\n",
       "      <td>Data Scientist at Meta</td>\n",
       "      <td>Los Angeles, CA</td>\n",
       "      <td>Meta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Lu L.</td>\n",
       "      <td>Data Scientist at Facebook</td>\n",
       "      <td>Los Angeles, CA</td>\n",
       "      <td>Facebook</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Name  \\\n",
       "0                  Abhinav R.   \n",
       "1                   Jinze Xin   \n",
       "2      Syamil Mohd Razak, PhD   \n",
       "3       Shijie (Selene) Xiang   \n",
       "4         Malavika Ajith Nair   \n",
       "5                 Marshall R.   \n",
       "6             Mehrnaz Motamed   \n",
       "7  Farimah Shirmohammadi, PhD   \n",
       "8              Kevin Goldberg   \n",
       "9                       Lu L.   \n",
       "\n",
       "                                          Job Titles  \\\n",
       "0      Incoming Data Scientist at Chevron | USC Alum   \n",
       "1                          Data Scientist at Twitter   \n",
       "2                Data Scientist | Petroleum Engineer   \n",
       "3                             Data Scientist at eBay   \n",
       "4                                     Data Scientist   \n",
       "5                         Data Scientist | web3/DeFi   \n",
       "6  Data Scientist - Machine Learning | Deep Learn...   \n",
       "7                                     Data Scientist   \n",
       "8                             Data Scientist at Meta   \n",
       "9                         Data Scientist at Facebook   \n",
       "\n",
       "                    Job Location          Company Name  \n",
       "0                Los Angeles, CA               Chevron  \n",
       "1  Los Angeles Metropolitan Area               Twitter  \n",
       "2  Los Angeles Metropolitan Area           Phillips 66  \n",
       "3                Los Angeles, CA                  eBay  \n",
       "4                  United States                  Meta  \n",
       "5                  United States       Stealth Startup  \n",
       "6                   Pasadena, CA   Endura Technologies  \n",
       "7                Los Angeles, CA  Edison International  \n",
       "8                Los Angeles, CA                  Meta  \n",
       "9                Los Angeles, CA              Facebook  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF_TEMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ba281c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679c7024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da88891",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1387c650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import openai\n",
    "# openai.api_key = 'YOUR_API_KEY'\n",
    "messages = [ {\"role\": \"system\", \"content\": \n",
    "              \"You are a intelligent assistant.\"} ]\n",
    "while True:\n",
    "    message = input(\"User : \")\n",
    "    if message:\n",
    "        messages.append(\n",
    "            {\"role\": \"user\", \"content\": message},\n",
    "        )\n",
    "        asdf\n",
    "        chat = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\", messages=messages\n",
    "        )\n",
    "    reply = chat.choices[0].message.content\n",
    "    print(f\"ChatGPT: {reply}\")\n",
    "    messages.append({\"role\": \"assistant\", \"content\": reply})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a007e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0aae7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e0a07f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aef8061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the text into lines and remove empty lines\n",
    "lines = page_text.split('\\n')\n",
    "non_empty_lines = [line for line in lines if line.strip() != \"\"]\n",
    "\n",
    "# Join the non-empty lines back into a single string\n",
    "text_without_empty_lines = \"\\n\".join(non_empty_lines)\n",
    "print(text_without_empty_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e09f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# The string to search in\n",
    "s = text_without_empty_lines\n",
    "\n",
    "# The regex pattern with groups\n",
    "pattern = r\"(?<=search result pages)(.*?)(?=Page \\d+ of \\d+)\"\n",
    "\n",
    "s# Search for the pattern\n",
    "match = re.search(pattern, s, re.DOTALL)\n",
    "\n",
    "if match:\n",
    "    print(\"Match found. Extracted text:\")\n",
    "    string_to_GPT = match.group().strip()\n",
    "    print(string_to_GPT)\n",
    "else:\n",
    "    print(\"No match found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fab0e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4896d443",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd5071f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c6d6dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a72b0dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9aaf49e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469fa130",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc87e55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2631b3c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76478c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_linkedin_info(driver, profile_url):\n",
    "    driver.get(profile_url)\n",
    "    time.sleep(5)  # Allow the page to load\n",
    "    page_text = driver.find_element_by_tag_name('body').text\n",
    "    return page_text\n",
    "\n",
    "\n",
    "\n",
    "profile_url = linkedin_urls[0]\n",
    "\n",
    "\n",
    "driver.get(profile_url)\n",
    "time.sleep(5)  # Allow the page to load\n",
    "page_text = driver.find_element(By.TAG_NAME, 'body').text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331bddec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(page_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f75cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# The string to search in\n",
    "s = page_text\n",
    "\n",
    "# The regex pattern with groups\n",
    "pattern = r\"(?<=search result pages)(.*?)(?=Page \\d+ of \\d+)\"\n",
    "\n",
    "s# Search for the pattern\n",
    "match = re.search(pattern, s, re.DOTALL)\n",
    "\n",
    "if match:\n",
    "    print(\"Match found. Extracted text:\")\n",
    "    string_to_GPT = match.group().strip()\n",
    "    print(string_to_GPT)\n",
    "else:\n",
    "    print(\"No match found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1067144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383ff8e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ec1cd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fbc8d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c72ec26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2edfac6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f0321a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d87cd44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0666889",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45cc1f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d729af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82623a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Load the spreadsheet into a pandas DataFrame\n",
    "# df = pd.read_excel(sheet_name)\n",
    "\n",
    "# # Load the Excel workbook using openpyxl\n",
    "# workbook = load_workbook(sheet_name)\n",
    "\n",
    "# # Get the active worksheet\n",
    "# worksheet = workbook.active\n",
    "\n",
    "# # Iterate over each row in the DataFrame\n",
    "# for index, row in df.iterrows():\n",
    "#     # Check if the article has been downloaded already\n",
    "#     if row['is_downloaded'] == 1:\n",
    "#         continue\n",
    "\n",
    "#     # Get the hyperlink from Column B\n",
    "#     cell = worksheet.cell(row=index+1, column=2)\n",
    "#     hyperlink = cell.hyperlink\n",
    "\n",
    "#     # If there is no hyperlink, skip this row\n",
    "#     if hyperlink is None:\n",
    "#         continue\n",
    "\n",
    "#     # Extract the URL of the article from the hyperlink\n",
    "#     article_url = hyperlink.target\n",
    "\n",
    "#     # Use BeautifulSoup to extract the PDF link\n",
    "    \n",
    "#     pdf_link = find_pdf_links(article_url)\n",
    "#     print(index, article_url, '___', pdf_link)\n",
    "#     # response = requests.get(article_url)\n",
    "#     # soup = BeautifulSoup(response.content, 'html.parser')\n",
    "#     # pdf_link = soup.find('a', href=lambda href: href and (href.endswith('.pdf') or ('pdf' in str(href).lower())))\n",
    "#     if pdf_link is None:\n",
    "#         pdf_link = soup.find('a', text=lambda text: text and ('pdf' in text.lower() or 'download' in text.lower()))\n",
    "\n",
    "#     # If a PDF link is found, download the file and mark the row as downloaded\n",
    "#     if pdf_link is not None:\n",
    "#         pdf_url = pdf_link['href']\n",
    "#         if not pdf_url.startswith('http'):\n",
    "#             pdf_url = article_url + pdf_url if pdf_url.startswith('/') else article_url + '/' + pdf_url\n",
    "#         response = requests.get(pdf_url)\n",
    "\n",
    "#         with open(f\"article_{index}.pdf\", 'wb') as f:\n",
    "#             f.write(response.content)\n",
    "\n",
    "#         # Update the DataFrame to mark the row as downloaded\n",
    "#         df.at[index, 'is_downloaded'] = 1\n",
    "        \n",
    "#     asdfasdf\n",
    "\n",
    "# # Save the updated DataFrame to a new spreadsheet\n",
    "# df.to_excel('articles_downloaded.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c2e6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spreadsheet into a pandas DataFrame\n",
    "df = pd.read_excel(sheet_name)\n",
    "\n",
    "# Load the Excel workbook using openpyxl\n",
    "workbook = load_workbook(sheet_name)\n",
    "\n",
    "# Get the active worksheet\n",
    "worksheet = workbook.active\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Check if the article has been downloaded already\n",
    "    if row['is_downloaded'] == 1:\n",
    "        continue\n",
    "\n",
    "    # Get the hyperlink from Column B\n",
    "    cell = worksheet.cell(row=index+1, column=2)\n",
    "    hyperlink = cell.hyperlink\n",
    "\n",
    "    # If there is no hyperlink, skip this row\n",
    "    if hyperlink is None:\n",
    "        continue\n",
    "\n",
    "    # Extract the URL of the article from the hyperlink\n",
    "    article_url = hyperlink.target\n",
    "\n",
    "    # Use BeautifulSoup to extract the PDF link\n",
    "    pdf_url, method, current_url = find_pdf_links(article_url)\n",
    "    print(index, article_url, '___', pdf_url, method, current_url)\n",
    "\n",
    "    if pdf_url is None:\n",
    "        response = requests.get(article_url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        pdf_link = soup.find('a', href=lambda href: href and (href.endswith('.pdf') or ('pdf' in str(href).lower())))\n",
    "\n",
    "        if pdf_link is None:\n",
    "            pdf_link = soup.find('a', text=lambda text: text and ('pdf' in text.lower() or 'download' in text.lower()))\n",
    "\n",
    "        if pdf_link is not None:\n",
    "            pdf_url = pdf_link['href']\n",
    "\n",
    "    # If a PDF link is found, download the file and mark the row as downloaded\n",
    "    if pdf_url is not None:\n",
    "        if not pdf_url.startswith('http'):\n",
    "            pdf_url = article_url + pdf_url if pdf_url.startswith('/') else article_url + '/' + pdf_url\n",
    "        response = requests.get(pdf_url)\n",
    "\n",
    "        with open(f\"article_{index}.pdf\", 'wb') as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "        # Update the DataFrame to mark the row as downloaded\n",
    "        df.at[index, 'is_downloaded'] = 1\n",
    "\n",
    "    # Save the updated DataFrame to a new spreadsheet\n",
    "    df.to_excel('articles_downloaded.xlsx', index=False)\n",
    "\n",
    "    # Sleep for 10 seconds to avoid overwhelming the server\n",
    "    time.sleep(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b63abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set the name of the Excel sheet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4d276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pdf_links(article_url):\n",
    "    # Create a Chrome webdriver\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    # Load the article URL\n",
    "    driver.get(article_url)\n",
    "\n",
    "    try:\n",
    "        # Find all buttons and links that contain the word \"pdf\"\n",
    "        pdf_buttons = driver.find_elements(By.XPATH, \"//button[contains(text(),'PDF')] | //a[contains(text(),'PDF')]\")\n",
    "\n",
    "        # If no buttons or links are found, return None\n",
    "        if len(pdf_buttons) == 0:\n",
    "            return None, None, driver.current_url\n",
    "\n",
    "        # Print all the buttons and links that contain the word \"pdf\"\n",
    "        for pdf_button in pdf_buttons:\n",
    "            print(pdf_button.text)\n",
    "\n",
    "            # Try to click the PDF button using different methods\n",
    "            method = None\n",
    "            try:\n",
    "                pdf_button.click()\n",
    "                method = \"click()\"\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, pdf_button.get_attribute('xpath')))).click()\n",
    "                method = \"xpath\"\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, pdf_button.get_attribute('href')))).click()\n",
    "                method = \"href\"\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                driver.execute_script(\"arguments[0].click();\", pdf_button)\n",
    "                method = \"execute_script\"\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # Wait for 10 seconds to see if the page redirects\n",
    "            time.sleep(10)\n",
    "\n",
    "            # Check if the current URL is the same as the article URL\n",
    "            if driver.current_url == article_url:\n",
    "                continue\n",
    "            else:\n",
    "                return driver.current_url, method,driver.current_url\n",
    "\n",
    "    finally:\n",
    "        # Close the webdriver\n",
    "        driver.quit()\n",
    "\n",
    "    # If no PDF link is found, return None\n",
    "    return None, None,driver.current_url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13d38b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d22568c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabfefc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fa4504",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03e9926",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ff29cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f455b710",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b5a379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e989744d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d16fb62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c75809",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from openpyxl import load_workbook\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import PyPDF2\n",
    "\n",
    "sheet_name = '/Users/phil/Downloads/dissertation REFS.xlsx'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40e5f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pdf_links(driver, article_url):\n",
    "    \"\"\"\n",
    "    Uses Selenium and Chrome to navigate to the article URL and extract the URL of the PDF file.\n",
    "\n",
    "    Parameters:\n",
    "        driver (webdriver.Chrome): The Chrome webdriver instance to use for navigation.\n",
    "        article_url (str): The URL of the article to download.\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing the PDF URL, method used to find it, and the current URL of the driver after the method is used.\n",
    "        Returns (None, None, None) if a PDF link is not found.\n",
    "    \"\"\"\n",
    "    driver.get(article_url)\n",
    "    current_url = driver.current_url\n",
    "\n",
    "    # Find the PDF link using a variety of methods\n",
    "    try:\n",
    "        # Wait for the \"Download PDF\" button to become visible\n",
    "        download_button = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//button[contains(text(),'Download PDF')]\"))\n",
    "        )\n",
    "        download_button.click()\n",
    "        method = 'download_button'\n",
    "\n",
    "    except:\n",
    "        try:\n",
    "            # Find the PDF link using a text-based search\n",
    "            pdf_link = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.XPATH, \"//a[contains(text(),'PDF')]\"))\n",
    "            )\n",
    "            pdf_url = pdf_link.get_attribute('href')\n",
    "            method = 'text_search'\n",
    "\n",
    "        except:\n",
    "            try:\n",
    "                # Find the PDF link using an attribute-based search\n",
    "                pdf_link = WebDriverWait(driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.XPATH, \"//a[@href and contains(@href,'.pdf')]\"))\n",
    "                )\n",
    "                pdf_url = pdf_link.get_attribute('href')\n",
    "                method = 'attribute_search'\n",
    "\n",
    "            except:\n",
    "                # If no PDF link is found, return None\n",
    "                return None, None, driver.current_url\n",
    "\n",
    "        # If a PDF link is found, return it\n",
    "        return pdf_url, method, driver.current_url\n",
    "\n",
    "    return driver.current_url, method, driver.current_url\n",
    "\n",
    "\n",
    "def validate_pdf(filename):\n",
    "    try:\n",
    "        with open(filename, 'rb') as f:\n",
    "            pdf_reader = PyPDF2.PdfFileReader(f)\n",
    "            num_pages = pdf_reader.getNumPages()\n",
    "            return num_pages > 0\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b964a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import PyPDF2\n",
    "\n",
    "# def is_pdf_malformed(file_path):\n",
    "#     try:\n",
    "#         with open(file_path, 'rb') as file:\n",
    "#             pdf_reader = PyPDF2.PdfReader(file)\n",
    "#         return False  # The PDF could be read successfully\n",
    "#     except Exception as e:\n",
    "#         # Check if the exception message contains the specific error related to malformed PDFs\n",
    "#         if \"malformed\" in str(e):\n",
    "#             return True  # The PDF is malformed\n",
    "#     return False  # The PDF is not malformed\n",
    "\n",
    "\n",
    "def validate_not_malformed_pdfs(file_paths):\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "    except Exception as e:\n",
    "        # Check if the exception message contains the specific error related to malformed PDFs\n",
    "        if \"Could not read malformed PDF file\" in str(e):\n",
    "            return False\n",
    "    return True\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def validate_pdf(filename):\n",
    "    try:\n",
    "        with open(filename, 'rb') as f:\n",
    "            pdf_reader = PyPDF2.PdfFileReader(f)\n",
    "            if pdf_reader.isEncrypted:\n",
    "                pdf_reader.decrypt('')\n",
    "            page = pdf_reader.getPage(0)\n",
    "            page_content = page.extractText()\n",
    "            if len(page_content) > 0:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# pdfs are mostly corrupt for somereason and I cant identiy using python IDO why \n",
    "# i get this error \n",
    "# Adobe Acrobat Reader could not open 'article_1.pdf' because it is either not a supported file type or because the file has been damaged (for example, it was sent as an email attachment and wasn't correctly decoded).\n",
    "validate_not_malformed_pdfs(\"/Users/phil/Desktop/Phils_python_code/article_3.pdf\"), validate_not_malformed_pdfs(\"/Users/phil/Desktop/Phils_python_code/article_1.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ed302c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Start the Chrome driver and navigate to the USC library login page\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--disable-extensions\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "# chrome_options.add_argument(\"--headless\")\n",
    "\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "driver.get('https://libproxy.usc.edu/login?url=%27+location.href')\n",
    "\n",
    "# Wait for user input to continue\n",
    "input('Please login and press Enter to continue...')\n",
    "\n",
    "# Load the spreadsheet into a pandas DataFrame\n",
    "\n",
    "df = pd.read_excel(sheet_name)\n",
    "\n",
    "# Load the Excel workbook using openpyxl\n",
    "workbook = load_workbook(sheet_name)\n",
    "\n",
    "# Get the active worksheet\n",
    "worksheet = workbook.active\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Check if the article has been downloaded already\n",
    "    if row['is_downloaded'] == 1:\n",
    "        continue\n",
    "\n",
    "    # Get the hyperlink from Column B\n",
    "    cell = worksheet.cell(row=index+1, column=2)\n",
    "    hyperlink = cell.hyperlink\n",
    "\n",
    "    # If there is no hyperlink, skip this row\n",
    "    if hyperlink is None:\n",
    "        continue\n",
    "\n",
    "    # Extract the URL of the article from the hyperlink\n",
    "    article_url = hyperlink.target\n",
    "\n",
    "    # Use BeautifulSoup to extract the PDF link\n",
    "    pdf_url, method, current_url = find_pdf_links(driver, article_url)\n",
    "    print(index, article_url, '___', pdf_url, method, current_url)\n",
    "\n",
    "    if pdf_url is None:\n",
    "        response = requests.get(article_url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        pdf_link = soup.find('a', href=lambda href: href and (href.endswith('.pdf') or ('pdf' in str(href).lower())))\n",
    "\n",
    "        if pdf_link is None:\n",
    "            pdf_link = soup.find('a', text=lambda text: text and ('pdf' in text.lower() or 'download' in text.lower()))\n",
    "\n",
    "        if pdf_link is not None:\n",
    "            pdf_url = pdf_link['href']\n",
    "\n",
    "    # If a PDF link is found, download the file and mark the row as downloaded\n",
    "    if pdf_url is not None:\n",
    "        if not pdf_url.startswith('http'):\n",
    "            pdf_url = article_url + pdf_url if pdf_url.startswith('/') else article_url + '/' + pdf_url\n",
    "        response = requests.get(pdf_url)\n",
    "\n",
    "        with open(f\"article_{index}.pdf\", 'wb') as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "        # Update the DataFrame to mark the row as downloaded\n",
    "        df.at[index, 'is_downloaded'] = 1\n",
    "\n",
    "    # Save the updated DataFrame to a new spreadsheet\n",
    "    df.to_excel('articles_downloaded.xlsx', index=False)\n",
    "\n",
    "    # Sleep for 10 seconds to avoid overwhelming the server\n",
    "    time.sleep(10)\n",
    "\n",
    "# Close the webdriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3fdeda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705ef017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61960764",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9f97ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf07cb88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83c494b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96759ac2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307c1405",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592e0e2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30eb2922",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7753e936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_pdf_links(article_url):\n",
    "#     # Create a Chrome webdriver\n",
    "#     driver = webdriver.Chrome()\n",
    "\n",
    "#     # Load the article URL\n",
    "#     driver.get(article_url)\n",
    "\n",
    "#     try:\n",
    "#         # Find all buttons and links that contain the word \"pdf\"\n",
    "#         pdf_buttons = driver.find_elements(By.XPATH, \"//button[contains(text(),'PDF')] | //a[contains(text(),'PDF')]\")\n",
    "\n",
    "#         # If no buttons or links are found, return None\n",
    "#         if len(pdf_buttons) == 0:\n",
    "#             return None\n",
    "\n",
    "#         # Print all the buttons and links that contain the word \"pdf\"\n",
    "#         print(pdf_buttons)\n",
    "        \n",
    "#         for pdf_button in pdf_buttons:\n",
    "#             print(pdf_button.text)\n",
    "#             asdf\n",
    "\n",
    "#             # Try to click the PDF button using different methods\n",
    "#             try:\n",
    "#                 pdf_button.click()\n",
    "#             except:\n",
    "#                 pass\n",
    "\n",
    "#             try:\n",
    "#                 WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, pdf_button.get_attribute('xpath')))).click()\n",
    "#             except:\n",
    "#                 pass\n",
    "\n",
    "#             try:\n",
    "#                 WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, pdf_button.get_attribute('href')))).click()\n",
    "#             except:\n",
    "#                 pass\n",
    "\n",
    "#             try:\n",
    "#                 driver.execute_script(\"arguments[0].click();\", pdf_button)\n",
    "#             except:\n",
    "#                 pass\n",
    "            \n",
    "#             # Wait for the PDF link to become visible\n",
    "#             pdf_link = WebDriverWait(driver, 10).until(\n",
    "#                 EC.presence_of_element_located((By.XPATH, \"//a[contains(text(),'PDF')]\"))\n",
    "#             )\n",
    "\n",
    "#             # Extract the URL of the PDF file\n",
    "#             pdf_url = pdf_link.get_attribute('href')\n",
    "\n",
    "#             # If a PDF link is found, return it\n",
    "#             if pdf_url is not None:\n",
    "#                 return pdf_url\n",
    "\n",
    "#     finally:\n",
    "#         # Close the webdriver\n",
    "#         driver.quit()\n",
    "\n",
    "#     # If no PDF link is found, return None\n",
    "#     return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351901ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_pdf_links(article_url):\n",
    "#     # Create a Chrome webdriver\n",
    "#     driver = webdriver.Chrome()\n",
    "\n",
    "#     # Load the article URL\n",
    "#     driver.get(article_url)\n",
    "\n",
    "#     try:\n",
    "#         # Find all buttons and links that contain the word \"pdf\"\n",
    "#         pdf_buttons = driver.find_elements(By.XPATH, \"//button[contains(text(),'PDF')] | //a[contains(text(),'PDF')]\")\n",
    "\n",
    "#         # If no buttons or links are found, return None\n",
    "#         if len(pdf_buttons) == 0:\n",
    "#             return None\n",
    "\n",
    "#         # Print all the buttons and links that contain the word \"pdf\"\n",
    "#         for pdf_button in pdf_buttons:\n",
    "#             print(pdf_button.text)\n",
    "\n",
    "#             # Try to click the PDF button using different methods\n",
    "#             try:\n",
    "#                 pdf_button.click()\n",
    "#             except:\n",
    "#                 pass\n",
    "\n",
    "#             try:\n",
    "#                 WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, pdf_button.get_attribute('xpath')))).click()\n",
    "#             except:\n",
    "#                 pass\n",
    "\n",
    "#             try:\n",
    "#                 WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, pdf_button.get_attribute('href')))).click()\n",
    "#             except:\n",
    "#                 pass\n",
    "\n",
    "#             try:\n",
    "#                 driver.execute_script(\"arguments[0].click();\", pdf_button)\n",
    "#             except:\n",
    "#                 pass\n",
    "\n",
    "#             # Wait for 10 seconds to see if the page redirects\n",
    "#             time.sleep(10)\n",
    "\n",
    "#             # Check if the current URL is the same as the article URL\n",
    "#             if driver.current_url == article_url:\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 return driver.current_url\n",
    "\n",
    "#     finally:\n",
    "#         # Close the webdriver\n",
    "#         driver.quit()\n",
    "\n",
    "#     # If no PDF link is found, return None\n",
    "#     return None\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrapifurs",
   "language": "python",
   "name": "scrapifurs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
