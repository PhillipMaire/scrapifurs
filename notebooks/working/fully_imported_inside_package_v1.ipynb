{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cbde08f-a6dc-472a-b899-c5bc68ceeb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scrapifurs import utils, job_main_page, search_jobs_window\n",
    "import os\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631d2c5a-9f84-490c-af42-a5d9d1be8473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "165e7e87-86ca-4239-a628-355ad8b8b02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup basic variable as dict \n",
    "info_dict = {'init_url':'https://www.linkedin.com/',\n",
    "             'save_password_dir':'/Users/phil/Dropbox/GITHUB/DATA/scrapifurs/saved_cookies/',\n",
    "            }\n",
    "info_dict['full_cookies_save_path'] = info_dict['save_password_dir']+os.sep+\"linkedin_cookies.pkl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2876736-4393-4c60-aef1-0c02712d9066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directory if it dosnt exist \n",
    "base_path = os.path.dirname(info_dict['full_cookies_save_path'])\n",
    "Path(base_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0645ba3e-a292-4b73-b5a5-3664e3678c77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "302d48d6-78b8-4e84-a087-8ca19818a7c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "388bce71-997b-4d50-a59e-311e0b5f77b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define saving class, once this is set just leave it \n",
    "bd = '/Users/phil/Library/CloudStorage/Dropbox/GITHUB/scrapifurs/scrapifurs/data/data_by_date/'\n",
    "fn = bd+'all_searches_job_data.xlsx'\n",
    "data_class = search_jobs_window.DataFile(fn, include_date=True) \n",
    "\n",
    "# prevent repeat scraping by grabbing all data in the base directory\n",
    "master_dir = '/Users/phil/Library/CloudStorage/Dropbox/GITHUB/scrapifurs/scrapifurs/data/APPLIED_JOBS'\n",
    "existin_jobs_dir_list = [bd, master_dir]\n",
    "existing_job_ids = search_jobs_window.grab_all_job_ids_in_folder(bd)\n",
    "\n",
    "\n",
    "# set up all the links you want to scrape (note update the links every few days)\n",
    "links = []\n",
    "x = 'https://www.linkedin.com/jobs/search/?currentJobId=3725179320&distance=25&f_E=2%2C3%2C4&f_PP=102277331%2C106233382%2C102448103%2C103918656%2C102250832%2C103575230%2C100075706&f_T=25206%2C340%2C25190%2C25887%2C30209%2C288%2C2463%2C25584&f_WT=1&geoId=103644278&keywords=data%20scientist&origin=JOB_SEARCH_PAGE_JOB_FILTER'\n",
    "links.append(x)\n",
    "x = 'https://www.linkedin.com/jobs/search/?currentJobId=3095112138&f_E=2%2C3%2C4&f_WT=1%2C2%2C3&geoId=102095887&keywords=machine%20learning%20engineer&location=California%2C%20United%20States&origin=JOB_SEARCH_PAGE_JOB_FILTER&refresh=true'\n",
    "links.append(x)\n",
    "x = 'https://www.linkedin.com/jobs/search/?currentJobId=3665313562&f_WT=1%2C3%2C2&keywords=forecaster&origin=JOB_SEARCH_PAGE_JOB_FILTER'\n",
    "links.append(x)\n",
    "\n",
    "# to pass into scrape custom setting for each link\n",
    "scraper_settings_list = [\n",
    "    {\n",
    "        'start_url': links[0],\n",
    "        'n_pages_to_scrape': 8,\n",
    "        'wait_sec_each_page': 5,\n",
    "        'update_every_n_secs': 20,\n",
    "        'existing_job_ids': existing_job_ids\n",
    "    },\n",
    "\n",
    "    {\n",
    "        'start_url': links[1],\n",
    "        'n_pages_to_scrape': 8,\n",
    "        'wait_sec_each_page': 5,\n",
    "        'update_every_n_secs': 20,\n",
    "        'existing_job_ids': existing_job_ids\n",
    "    },\n",
    "\n",
    "    {\n",
    "        'start_url': links[2],\n",
    "        'n_pages_to_scrape': 8,\n",
    "        'wait_sec_each_page': 5,\n",
    "        'update_every_n_secs': 9*60,\n",
    "        'existing_job_ids': existing_job_ids\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "search_jobs_window.scrape_job_data(info_dict, scraper_settings_list, data_class, auto_update_link=True)\n",
    "# auto_update_link -- with auto click the 'search' button updating the search to include new jobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fada9a01-4798-441f-99bb-5214d703e64f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a78397-9587-4817-83b0-1ab23b9ca762",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095e56c4-f6fd-4340-a178-b82728d21afc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b02ae480-4003-4cbe-8f83-efd6942cb162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paste links of jobs you already applied to\n",
    "x = \"\"\"\n",
    "\n",
    "\n",
    "https://www.linkedin.com/jobs/view/3752864807/\n",
    "https://www.linkedin.com/jobs/view/3726834890/\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "url2scrape = job_main_page.split_and_clean(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cac1d2a1-280e-45a5-9a43-4fe72df7e17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOB ID: 3752864807\n",
      "Repeat entry, skipping...\n",
      "JOB ID: 3726834890\n",
      "Repeat entry, skipping...\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "# update links in your master jobs\n",
    "bd = '/Users/phil/Library/CloudStorage/Dropbox/GITHUB/scrapifurs/scrapifurs/data/APPLIED_JOBS'\n",
    "file_name = bd+os.sep+'master_jobs_applied_to.csv'\n",
    "\n",
    "driver = job_main_page.open_browser(info_dict)\n",
    "job_main_page.save_jobs_data(driver, url2scrape, file_name)\n",
    "\n",
    "print(\"DONE\")\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b62a07f-0b24-4bc5-ab16-60471be6bcba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dc2728f-6d0b-4a09-baba-49b7b1652dd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1963b220-d653-4ea8-98f6-05faf90a5bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "# Example usage:\n",
    "data_folder = '/Users/phil/Library/CloudStorage/Dropbox/GITHUB/scrapifurs/scrapifurs/data/data_by_date'\n",
    "fn_applied = '/Users/phil/Library/CloudStorage/Dropbox/GITHUB/scrapifurs/scrapifurs/data/APPLIED_JOBS/master_jobs_applied_to.xlsx'\n",
    "fn_skipped = '/Users/phil/Library/CloudStorage/Dropbox/GITHUB/scrapifurs/scrapifurs/data/APPLIED_JOBS/master_jobs_skipped.xlsx'\n",
    "utils.update_master_files(data_folder, fn_applied, fn_skipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a131e15-0f8c-4ac0-ba95-8da2824a34a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "# import numpy as np\n",
    "# def grab_all_job_ids_in_folder(bd):\n",
    "#     if isinstance(bd, list):\n",
    "#         files = []\n",
    "#         for d in bd:\n",
    "#             files+=glob.glob(f'{d}/*.xlsx')\n",
    "#     else:\n",
    "#         files = glob.glob(f'{bd}/*.xlsx')\n",
    "#     files = [file for file in files if not os.path.basename(file).startswith('~')]\n",
    "#     all_ids = []\n",
    "#     for fn in files:\n",
    "#         data = DataFile(fn, False)\n",
    "#         try:\n",
    "#             all_ids.append(data.df_main['job_ids'])\n",
    "#         except:\n",
    "#             all_ids.append(data.df_main['job_id'])\n",
    "#     if len(all_ids) == 0:\n",
    "#         return np.asarray([])\n",
    "#     return np.unique(np.concatenate(all_ids))\n",
    "\n",
    "\n",
    "# # def grab_all_job_ids_in_folder(bd):\n",
    "# #     if isinstance(bd, list):\n",
    "# #         files = []\n",
    "# #         for d in bd:\n",
    "# #             files += glob.glob(f'{d}/*.xlsx')\n",
    "# #     else:\n",
    "# #         files = glob.glob(f'{bd}/*.xlsx')\n",
    "\n",
    "# #     # Filter out temporary files\n",
    "# #     files = [file for file in files if not os.path.basename(file).startswith('~')]\n",
    "\n",
    "# #     # Check if the files list is empty and raise an exception if it is\n",
    "# #     if not files:\n",
    "# #         raise FileNotFoundError(\"No .xlsx files found in the specified directory or directories.\")\n",
    "\n",
    "# #     all_ids = []\n",
    "# #     for fn in files:\n",
    "# #         data = DataFile(fn, False)\n",
    "# #         try:\n",
    "# #             all_ids.append(data.df_main['job_ids'])\n",
    "# #         except KeyError:\n",
    "# #             all_ids.append(data.df_main['job_id'])\n",
    "\n",
    "# #     return np.unique(np.concatenate(all_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0274573-5984-4664-8614-459820386bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scrapifurs import utils, job_main_page, search_jobs_window\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# setup basic variable as dict \n",
    "info_dict = {'init_url':'https://www.linkedin.com/',\n",
    "             'save_password_dir':'/Users/phil/Dropbox/GITHUB/DATA/scrapifurs/saved_cookies/',\n",
    "            }\n",
    "info_dict['full_cookies_save_path'] = info_dict['save_password_dir']+os.sep+\"linkedin_cookies.pkl\"\n",
    "\n",
    "\n",
    "\n",
    "# create directory if it dosnt exist \n",
    "base_path = os.path.dirname(info_dict['full_cookies_save_path'])\n",
    "Path(base_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "# define saving class, once this is set just leave it \n",
    "bd = '/Users/phil/Library/CloudStorage/Dropbox/GITHUB/scrapifurs/scrapifurs/data/data_by_date/'\n",
    "fn = bd+'all_searches_job_data.xlsx'\n",
    "data_class = search_jobs_window.DataFile(fn, include_date=True) \n",
    "\n",
    "# prevent repeat scraping by grabbing all data in the base directory\n",
    "master_dir = '/Users/phil/Library/CloudStorage/Dropbox/GITHUB/scrapifurs/scrapifurs/data/APPLIED_JOBS'\n",
    "existin_jobs_dir_list = [bd, master_dir]\n",
    "existing_job_ids = search_jobs_window.grab_all_job_ids_in_folder(bd)\n",
    "\n",
    "\n",
    "# set up all the links you want to scrape (note update the links every few days)\n",
    "links = []\n",
    "x = 'https://www.linkedin.com/jobs/search/?currentJobId=3725179320&distance=25&f_E=2%2C3%2C4&f_PP=102277331%2C106233382%2C102448103%2C103918656%2C102250832%2C103575230%2C100075706&f_T=25206%2C340%2C25190%2C25887%2C30209%2C288%2C2463%2C25584&f_WT=1&geoId=103644278&keywords=data%20scientist&origin=JOB_SEARCH_PAGE_JOB_FILTER'\n",
    "links.append(x)\n",
    "x = 'https://www.linkedin.com/jobs/search/?currentJobId=3095112138&f_E=2%2C3%2C4&f_WT=1%2C2%2C3&geoId=102095887&keywords=machine%20learning%20engineer&location=California%2C%20United%20States&origin=JOB_SEARCH_PAGE_JOB_FILTER&refresh=true'\n",
    "links.append(x)\n",
    "x = 'https://www.linkedin.com/jobs/search/?currentJobId=3665313562&f_WT=1%2C3%2C2&keywords=forecaster&origin=JOB_SEARCH_PAGE_JOB_FILTER'\n",
    "links.append(x)\n",
    "\n",
    "# to pass into scrape custom setting for each link\n",
    "scraper_settings_list = [\n",
    "    {\n",
    "        'start_url': links[0],\n",
    "        'n_pages_to_scrape': 8,\n",
    "        'wait_sec_each_page': 5,\n",
    "        'update_every_n_secs': 20,\n",
    "        'existing_job_ids': existing_job_ids\n",
    "    },\n",
    "\n",
    "    {\n",
    "        'start_url': links[1],\n",
    "        'n_pages_to_scrape': 8,\n",
    "        'wait_sec_each_page': 5,\n",
    "        'update_every_n_secs': 20,\n",
    "        'existing_job_ids': existing_job_ids\n",
    "    },\n",
    "\n",
    "    {\n",
    "        'start_url': links[2],\n",
    "        'n_pages_to_scrape': 8,\n",
    "        'wait_sec_each_page': 5,\n",
    "        'update_every_n_secs': 9*60,\n",
    "        'existing_job_ids': existing_job_ids\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "search_jobs_window.scrape_job_data(info_dict, scraper_settings_list, data_class, auto_update_link=True)\n",
    "# auto_update_link -- with auto click the 'search' button updating the search to include new jobs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8038a80-ed00-432e-93ca-8c6a35c9e345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85c4b84-85bd-4da4-a8c8-46cc986ce149",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb5ee79-0aa9-4b4b-8dcb-1c07a4173449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec548cf-be16-4907-99cf-2024310003e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrapifurs",
   "language": "python",
   "name": "scrapifurs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
